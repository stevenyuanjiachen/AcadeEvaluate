以下是文件库中的文件：
第1个文件的内容为：
OneBit: Towards Extremely Low-bit
Large Language Models
Yuzhuang Xu1 Xu Han1 Zonghan Yang1 Shuo Wang1
Qingfu Zhu2 Zhiyuan Liu1 Weidong Liu1 Wanxiang Che2,B
1Department of Computer Science & Technology, Tsinghua University, Beijing, China
2Research Center for Social Computing and Information Retrieval,
Harbin Institute of Technology, Harbin, China
xyz21thu@gmail.com, car@ir.hit.edu.cn
Abstract
Model quantification uses low bit-width values to represent the weight matrices
of existing models to be quantized, which is a promising approach to reduce
both storage and computational overheads of deploying highly anticipated LLMs.
However, current quantization methods suffer severe performance degradation
when the bit-width is extremely reduced, and thus focus on utilizing 4-bit or 8-bit
values to quantize models. This paper boldly quantizes the weight matrices of
LLMs to 1-bit, paving the way for the extremely low bit-width deployment of
LLMs. For this target, we introduce a 1-bit model compressing framework named
OneBit, including a novel 1-bit parameter representation method to better quantize
LLMs as well as an effective parameter initialization method based on matrix
decomposition to improve the convergence speed of the quantization framework.
Sufficient experimental results indicate that OneBit achieves good performance
(at least 81% of the non-quantized performance on LLaMA models) with robust
training processes when only using 1-bit weight matrices. Code and checkpoints
are available at https://github.com/xuyuzhuang11/OneBit
1 Introduction
Transformer [36] has emerged as the pivotal architecture in large language models (LLMs), fun-
damentally reshaping the approach to natural language processing in deep learning era [6, 34, 4].
Despite their popularity, deploying transformer-based LLMs presents significant challenges due to
their computational intensity and considerable memory requirements as the parameters of LLMs
become more and more. For instance, even moderately-sized LLMs like LLaMA-13B [34] require
around 26GB of memory to load its all parameters in FP16 format. Such overheads make deploying
LLMs difficult beyond mid-to-high-end GPUs like the A100, let alone on mobile devices. The high
demand for resources not only drives up usage costs, but also restricts their wider application.
Numerous efforts [10, 14, 13] have been devoted to reducing the computational and memory over-
heads of LLMs, while still preserving most of their original model capabilities. Among these efforts,
quantization has gained widespread attention, particularly Post-Training Quantization (PTQ), benefit-
ted from its lower transferring costs. Seminal studies such as GPTQ [14], SpQR [12], and AWQ [20]
successfully compress the weight matrices of LLMs to 4-bit values while maintaining the main
abilities of LLMs. Efficient quantization represents significant advances in LLM optimization, by
achieving a balance between time and space efficiency as well as model performance.
Unfortunately, the efficacy of PTQ rapidly diminishes when the quantization bit-width is extremely
low, as shown in Figure 1. Existing PTQ methods managed to compress weight matrices down
Preprint. Under review.
ar
X
iv
:2
40
2.
11
29
5v
3
[
cs
.C
L
]
2
2
M
ay
2
02
4
to at least 3-bit [9]. Recent researches hope to leverage Quantization-Aware Training (QAT) to
overcome the bottlenecks faced by PTQ. LLM-QAT [21] introduces a few learnable parameters into
16 8 4 2 1
# weight bits
6
8
10
12
14
16
Pe
rp
le
xi
ty
Ours
GPTQ
LLM-QAT
OmniQuant
Figure 1: The perplexity (lower scores mean better
performance) of existing widely-used low-bit quan-
tization methods on LLaMA-7B, reported on Wiki-
text2 [23]. All the examined previous approaches suf-
fer from significant performance degradation when
quantizing models to 2-bit values. Our 1-bit quanti-
zation method can outperform these 2-bit baselines.
the quantization process, achieving notable re-
sults. OmniQuant [30], integrating learnable
equivalent transformation, presents promis-
ing results in 2-bit quantization. However,
existing methods decline when compressing
model weights to 1 bit, struggling to main-
tain effectiveness. This mainly stems from
the drastic precision loss at extremely low bit-
width representation in weight matrix W, sig-
nificantly increasing loss in linear projection
WX, which is the core operator within LLMs.
In this paper, we propose a novel Linear
layer and Sign-Value-Independent Decompo-
sition (SVID) for weight matrices to repre-
sent LLMs using approximately 1-bit values.
In our novel layer architecture, each original
high-bit weight matrix is represented as one
sign matrix (±1) and two value vectors. The
value vectors provide necessary floating-point
precision in linear projection at little cost and
help the model to be trained easily. The sign
matrix maintains the high rank of the original weight matrix with a small space cost, thereby pre-
serving high information capacity. SVID offers a better parameter initialization for 1-bit models
from the non-quantized model and we employ quantization-aware knowledge distillation to transfer
the capabilities of the original model to the proposed 1-bit counterpart. Experiments demonstrate
that our method performs well at the W1A16 (1-bit weight and 16-bit activation) quantization level.
Furthermore, our 1-bit model is more amenable to training and knowledge transfer than previous
works. In summary, our contributions are 3-fold:
• We propose a novel and efficient 1-bit model architecture for LLMs, which can improve
both the time and space efficiency during model inference. Moreover, our architecture is
more stable during quantizing LLMs.
• We propose SVID to decompose high-bit matrices into low-bit ones, which is essential for
the initialization of our 1-bit architecture. Experiments demonstrate that the SVID-based
initialization can improve the model performance and convergence speed.
• Extensive experiments demonstrate that our method works well in model sizes from 1.3B to
13B in OPT, LLaMA, and LLaMA2, showcasing its generalizability.
2 Related Work
2.1 Large Language Model Compression
Quantization, pruning, and knowledge distillation (KD) are the mainstream methods for model
compression. Quantization compresses model weights into low-bit values [14, 20, 11]. For data type
alignment in computation and reducing memory, it also involves quantizing activation [10, 39] and
key-value cache [30]. Pruning simplifies model complexity by removing unimportant weights or
modules, thereby sparsifying the original larger models [13, 31, 22]. KD trains a smaller student
model under the guidance of a larger teacher model [16, 1, 16], achieving the purpose of compressing
the larger one. Beyond these methods, low-rank factorization approximates the original weight
matrix W with the product of two lower-rank matrices [40] and also achieves promising results. Our
work belongs to quantization, using KD for knowledge transfer from the original LLM and uniquely
focusing on extremely low bit-width quantization. More details about model compression can refer
to existing survies [37, 43].
2
2.2 Large Language Model Quantization
Since this paper aims to obtain extremely low-bit LLMs, here we thus introduce more details about
LLM quantization. Quantization stands as a popular and crucial method for model compression,
capable of achieving a significant compression ratio with a relatively small loss. It can be classified
into Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) according to when
quantization is applied.
PTQ directly converts trained models into lower-bit counterparts using accurate solvers and limited
calibration data without additional training. Typically, GPTQ [14] row-wisely quantizes weight
matrices and adjusts remaining weights to compensate for the precision loss caused by quantization,
achieving nearly lossless 4-bit weight quantization. Moreover, numerous studies observed the effect
of “outliers” in quantization [10, 18, 20]. LLM.int8() [10] suggests mixed-precision decomposition
to ensure the accuracy of a few outliers in activations. SmoothQuant [39] reduces the difficulty of
quantization by smoothing the outliers of activation. SpQR [12] identifies sensitive weights to ensure
their precision, while quantizing other weights to lower bit-width.
QAT integrates quantization steps within the model, applying them during training or fine-tuning. It
allows the model to better adapt to the reduced precision induced by quantization, leading to improved
performance compared to PTQ. LLM-QAT [21] introduces a small number of learnable parameters
into quantization and employs KD using data generated by the original model itself. OmniQuant (30;
we classify it as QAT) further introduces learnable equivalent transformation, achieving acceptable
results in 2-bit weight quantization. Contemporary work QuIP# [35] combines randomized Hadamard
transform, vector quantization techniques, and fine-tuning to achieve better performance in 2-bit level.
PEQA [17] and QLoRA [11] focus on fine-tuning a limited number of extra parameters to mitigate
the precision loss caused by sub-4bit weight quantization. Our work is closely related to QAT, but due
to the unique challenges posed by 1-bit quantization, our representation and initialization methods of
quantized weights are distinct from any existing work.
3 Methodology
This section demonstrates our 1-bit architecture of the Linear layer to be quantized and discuss how
to initialize the quantized model to achieve better performance in knowledge distillation. We start
with a short review of classical weight quantization methods in Section 3.1 and then formulate our
OneBit from Section 3.2 to Section 3.4 in detail.
3.1 Background
The main idea of model quantization is to compress each weight matrix W within models in FP32 or
FP16 format to a low-bit counterpart. Specifically, we often quantize the weight matrices of Linear
layers in transformer to 8, 4, and even 2 bits.
The majority of quantization studies primarily employ the round-to-nearest (RTN) method, by which
the weight w is rounded to the nearest value in the quantization grid. It can be formulated as
ŵ = Clip
(⌊w
s
⌉
+ z, 0, 2N − 1
)
, (1)
where s denotes the quantization scale parameter, z denotes the zero point parameter, and N is the
quantization bit-width. Clip(·) truncates the result in the range of 0 to 2N − 1. With the bit-width
being lower and lower, the quantization grid also becomes sparser. When we quantize a LLM to
1-bit values, there are only 2 available numbers to be chosen in the quantized model. Existing study
[9] points out that quantization based on the RTN method may get their best performance at the
4-bit level. Further quantizing to 2-bit values following this paradigm would result in a substantial
degradation [30] as shown in Figure 1.
Furthermore, when N equals 1, quantization based on RTN method is essentially equivalent to
setting a threshold, with weight w on either side of it being converted to corresponding integer
value ŵ. In such a scenario, the parameters s and z in Eq. (1) effectively lose their practical
significance. Consequently, when quantizing weights to 1 bit, the element-wise RTN operation
drastically undermines the precision of the weight matrix W, leading to poor performance of the
quantized model.
3
LayerN
orm
(a) FP16 Linear Layer (b) Our Binary Quantized Linear Layer
Figure 2: The main idea of our method OneBit. The left is the original FP16 Linear Layer, in
which both the activation X and the weight matrix W are in FP16 format. The right is our proposed
architecture. Only value vectors g and h are in FP16 format, and the weight matrix consists of ±1
instead, which can be represented in INT1.
3.2 1-bit Linear Layer Architecture
Due to the severe precision loss of 1-bit weight quantization, converting weight matrices in Linear
layers directly from FP32/16 to 1-bit format based on RTN is challenging. Wang et al. [38] explore
this possibility by studying the capabilities of purely 1-bit weight matrices, training the 1-bit model
from scratch. In the W1A16 setting, their Linear layers are designed as
W±1 = Sign
[
W −Mean
(
W
)]
,
η = Mean
[
Abs
(
W −Mean
(
W
))]
,
Y = η · LayerNorm
(
X
)
WT
±1,
(2)
where W denotes the quantized weight matrix with the shape m × n and W±1 denotes the 1-bit
quantized matrix. X is the input of Linear layer and Y is the output. Sign(·), Mean(·) and Abs(·)
functions return the sign matrix, average and absolute value matrix. Unfortunately, this approach
reduces computational demands but also leads to a marked decrease in performance [38]. Moreover,
due to training difficulties, experiments show that this method is challenging to use for quantizing
existing models and can only be applied to training models from scratch.
Inspired by Wang et al. [38], we also quantize the weight matrix using the function Sign(·), and the
element of the quantized matrix is set to +1 or -1 as well. Moreover, we also notice that although
W±1 maintains a high rank of W, the missed floating-point precision still destroys the model
performance. Therefore, different from previous work, we introduce 2 value vectors with an FP16
format to compromise the precision loss in the quantization process. During training, our proposed
Linear layers are designed as
W±1 = Sign
(
W
)
,
Y =
[(
X⊙ g
)
WT
±1
]
⊙ h,
Z = LayerNorm
(
Y
)
,
(3)
where g and h are the two FP16 value vectors. During inference, W±1 is packed with an INT1
format, and Sign(·) will not be used, as shown in Figure 2. Note that we specify the calculation order
using brackets in Eq. (3) for minimizing the time and space cost. The main difference between Wang
et al. [38] and OneBit is the extra parameter g and h. Even if additional parameters are brought in,
the benefits far outweigh its small cost. For instance, when we quantize one weight matrix with the
shape 4096× 4096, the average bit-width of the quantized result is 1.0073. See A.7 for the details.
3.3 Sign-Value-Independent Decomposition
In our proposed 1-bit architecture, the weight matrix W is mathematically divided into two compo-
nents: one sign matrix W±1 in INT1 format and two value vector g/h in FP16 format. To initialize
the 1-bit model with the help of the fully trained weight, we introduce the Sign-Value-Independent
Decomposition (SVID) of the weight matrix W, which can be formulated as W = Wsign ⊙Wvalue.
Here we have Wvalue = |W| and Wsign = Sign(W). For Wvalue, we further approximately decom-
pose it into the outer product of two vectors a and b, which is also known as rank-1 approximation.
4
Hence, our proposed matrix decomposition method can be represented as
W ≈ Wsign ⊙
(
abT
)
. (4)
We can employ some widely used matrix decomposition methods to perform the rank-1 approximation,
such as SVD [2] and NMF [25].
Proposition 1 Given the weight matrix W and input X, the Linear layer can be reformulated as
the following according to SVID:
XWT ≈
[ (
X⊙ bT
)
WT
sign
]
⊙ aT. (5)
We prove this approximation in Appendix A.1. This bridges the gap between the architecture of
the quantized model and its original weights. It indicates that if we assign Wsign to W±1, aT to h
and bT to g, the quantized model is an approximate initialization of the original model. Moreover,
compared to restoring the original matrix W first (such as in Eq. (4)), the computational order in
Eq. (5) saves approximately one matrix W in FP16 format in memory as there is no need to restore
W in FP16 format.
The main objective of SVID is to involve the sign matrix Wsign in approximating matrix W, rather
than solely relying on value vectors in FP16 format. To substantiate the role of the sign matrix Wsign
in matrix approximation, we present the following proposition.
Proposition 2 Given matrices W and |W|, W = Wsign ⊙ |W|. We decompose these matrices in
the way W = abT +E1 and |W| = ãb̃T +E2, where Ei denotes the error matrices. In terms of
the Frobenius-norm, the SVID is closer to the original matrix W:∥∥∥W −Wsign ⊙ ãb̃T
∥∥∥2
F
≤
∥∥∥W − abT
∥∥∥2
F
. (6)
We also prove this proposition in Appendix A.1. It clearly demonstrates the practical role of the sign
matrix Wsign in matrix approximation.
Note that, given the predominantly low precision of most parameters, it is quite challenging to
approximate the weight matrix W accurately. SVID is not aimed to precisely replicate the original
model’s parameters, but to provide an effective starting point for further training, leveraging the
extensive training of the original model. Details on transferring knowledge from the original model
to the quantized counterpart are in Section 3.4.
3.4 Knowledge Transfer
We employ quantization-aware knowledge distillation to transfer knowledge from the original model
(i.e. teacher model) to the quantized one (i.e. student model). In the student model, the element
in matrix W and vectors g/h in Eq. (3) will be trained. We use cross-entropy based logits and
mean-square-error based hidden state of the full-precision teacher model to direct the quantized
student model [32]. Language modeling loss is not used. The cross-entropy is defined as
LCE = − 1
ns
ns∑
i=1
∑
c
P T
c (oi) logP
S
c (oi) , (7)
where c denotes the number of classes and ns denotes the number of training samples in the current
batch. T and S are the teacher model and student model, respectively. The error of hidden states is
defined as
LMSE =
ns∑
i=1
nl∑
j=1
∥∥∥∥∥ qT
i,j∥∥qT
i,j
∥∥
2
−
qS
i,j∥∥qS
i,j
∥∥
2
∥∥∥∥∥
2
2
, (8)
where nl denotes the number of layers and q denotes the hidden state. Hence the final objective
function can be formulated as
LKD = LCE + αLMSE, (9)
where α is the hyper-parameter that balances the importance of the cross-entropy loss and the features
in the intermediate layers. Please refer to A.6 for further discussions of this part.
5
4 Experiments
We experiment with 1-bit weight-only quantizaton and maintain 16-bit activation (W1A16) in this
work. We evaluate our approach by performing experiments on OPT-1.3B/2.7B models, LLaMA-
7B/13B models and LLaMA2-7B/13B models, and present results on various tasks.
4.1 Settings
Data For the training data of our quantization-aware knowledge distillation, we follow Liu et al.
[21] to synthesize corpus using next token generation from the original teacher model. It randomizes
the first token from vocabulary and generates the next token iteratively until reaching either the
<EOS> token or the maximum length. Specially, the top-1 predictions are selected deterministically
for the first 3 to 5 tokens, followed by stochastic sampling for the remaining tokens. We utilized
LLaMA-7B to generate a total of 132k data entries, each with a maximum length of 2,048.
Training Details Every KD experiment learns the training data over 50 epochs, from which 2048-
token segments are selected. We employ NMF in scikit-learn 1 to decompose the weight matrices in
SVID. The quantized student models are optimized by Adam [19] with β1 = 0.9, β2 = 0.98. The
learning rate for all experiments is scheduled by cosine strategy. We use NVIDIA A100 GPUs and
maintain FP16 precision while training quantized models. For additional details such as learning rate,
please refer to Table 1.
Table 1: Training details of knowledge distillation.
Models learning rate α # GPUs
OPT-1.3B 4e-4 1.0 1 × 8
OPT-2.7B 2e-4 1.0 1 × 8
LLaMA-7B 4e-4 1.0 1 × 8
LLaMA-13B 2e-4 1.0 2 × 8
LLaMA2-7B 1e-4 1.0 1 × 8
LLaMA2-13B 2e-4 1.0 2 × 8
Baselines To our knowledge, there is no previous work exploring the 1-bit quantization of LLMs
from a knowledge transfer perspective. To this end, we relax the quantization bit-width of baselines
to 2 bits (W2A16) while maintaining the W1A16 setting in our method. We compare our method with
GPTQ [14], LLM-QAT [21] and OmniQuant [30]. To ensure a fair comparison in terms of space
usage, baselines do not employ grouped quantization. Additionally, we included the results of vanilla
transformers with FP16 precision as a reference. While the recent work BitNet [38] also introduced
one 1-bit model architecture, it only worked for training models from scratch. We also analyze its
capability to transfer knowledge from the original models in Appendix A.5.
Evaluation Metrics Basically, we evaluate quantized models by testing the perplexity on the
validation set, specifically on WikiText2 [23] and C4 [28]. Lower perplexity indicates that the
compressed model is better at preserving the output distribution of the original model. Furthermore,
accuracies of zero-shot tasks including Winograde [29], HellaSwag [41], PIQA [4], BoolQ [7], and
ARC [8] are also reported. They evaluate if the capabilities of the original model on downstream
tasks are retained. We utilize the open-sourced toolkit “LM-Evaluation-Harness”2 to perform the
perplexity test and all zero-shot tasks.
4.2 Main Results
Table 2 compares our method with other typical strong baselines on different models. Due to space
limitations, results of LLaMA2-7B/13B are listed in Appendix A.3. In various model sizes, our 1-bit
weight quantization method obviously outperforms others under the W2A16 setting. Moreover, the
effectiveness of QAT based methods consistently improves as the model size increases, whereas the
result of the PTQ method, GPTQ, may degrade when model size increases (e.g., from 7B to 13B on
1https://scikit-learn.org/
2https://github.com/EleutherAI/lm-evaluation-harness
6
Table 2: Main results of evaluation experiment. We report the perplexity and zero-shot accuracy.
“FP16” is the transformer with FP16 parameters and we refer to it as the upper-bound of all the
methods. The best score is bolded.
Models Methods Perplexity(↓) Zero-shot Accuracy(↑)
Wiki2 C4 Wino. Hella. PIQA BoolQ ARC-e ARC-c Avg.
OPT-1.3B
FP16 14.63 14.72 59.67 53.73 72.42 57.68 50.80 29.69 54.00
GPTQ 9.5e3 3.8e3 49.33 25.57 52.07 39.60 26.68 23.63 36.15
LLM-QAT 4.9e3 2.1e3 49.72 25.72 50.05 37.83 25.76 25.09 35.70
OmniQuant 42.43 55.64 51.85 33.39 60.94 56.45 38.76 23.38 44.13
OneBit 25.42 22.95 51.14 34.26 62.57 59.45 41.25 24.06 45.46
OPT-2.7B
FP16 12.47 13.17 60.93 60.59 74.81 60.28 54.34 31.31 57.04
GPTQ 8.7e3 3.9e3 49.88 26.47 49.84 39.88 25.76 26.02 36.31
LLM-QAT 3.7e3 1.4e3 52.09 25.47 49.29 37.83 24.92 25.60 35.87
OmniQuant 30.25 41.31 51.62 38.21 62.19 54.25 40.82 24.74 45.31
OneBit 21.86 20.76 51.67 38.18 63.87 54.28 43.39 24.40 45.97
LLaMA-7B
FP16 5.68 7.08 66.85 72.99 77.37 73.21 52.53 41.38 64.06
GPTQ 1.9e3 7.8e2 49.41 25.63 49.95 43.79 25.84 27.47 37.02
LLM-QAT 7.1e2 3.0e2 51.78 24.76 50.87 37.83 26.26 25.51 36.17
OmniQuant 15.34 26.21 52.96 43.68 62.79 58.69 41.54 29.35 48.17
OneBit 10.19 11.40 58.48 51.54 68.01 57.28 42.47 30.20 51.33
LLaMA-13B
FP16 5.09 6.61 70.17 76.24 79.05 68.47 59.85 44.54 66.39
GPTQ 3.2e3 9.9e2 50.67 25.27 50.00 42.39 26.14 27.39 36.98
LLM-QAT 1.8e3 1.2e3 51.62 25.40 50.33 37.83 27.02 26.87 36.51
OmniQuant 13.43 19.33 53.83 54.16 68.99 62.20 45.50 30.38 52.51
OneBit 9.18 10.25 62.90 56.78 70.67 64.16 44.53 32.00 55.17
LLaMA). This demonstrates that QAT-based method can achieve stable results in extremely low-bit
quantization. Specifically, our method approaches the performance of FP16 more closely as the
model size increases. For instance, when scaling from LLaMA-7B to LLaMA-13B, the perplexity
(on C4) of the FP16 model decreases by only 0.47, whereas our method sees a reduction of 1.15.
For perplexity, only our method achieves comparable results to the strongest FP16 baseline. For
instance, our method achieves 9.18 in the Wiki2 dataset on LLaMA-13B model and the FP16
baseline is 5.09. The performance loss of other methods is significant, even though they use 2-bit
quantization, which is more than our 1 bit. For GPTQ and LLM-QAT, the performance degradation
after quantization is pretty severe. As for OmniQuant, even though it is the strongest baseline under
the W2A16 setting, it still suffers greater performance loss compared to our W1A16 setting.
For zero-shot accuracy, although all methods inevitably have some degradation, our method achieves
the closest performance to the FP16 baseline among most models. On the OPT-1.3B/2.7B model, our
method shows smaller performance loss on most tasks such as PIQA and ARC-e. Additionally, the
loss of other tasks is negligible compared with the second-best baseline, OmniQuant. On the LLaMA-
7B model, our method also notably outperforms OmniQuant in most tasks except BoolQ/ARC-e,
averaging about a 4% improvement overall.
4.3 Problem Solving Ability
We have demonstrated the superior performance of our method under the W1A16 setting, compared
to other representative baselines. Although all methods inevitably face performance degradation in
1-bit weight quantization, it remains of interest how our method fares in solving practical problems
among the various approaches to reducing model size. For instance, directly training smaller models
[42] or employing low-rank decomposition to reduce the number of parameters.
To this end, we consider two crucial abilities of LLMs: commonsense reasoning and world knowledge.
For commonsense reasoning, we use the 6 tasks (Hellaswag, etc.) and settings described in Section 4.2.
For world knowledge, we examine it using the Massive Multi-task Language Understanding (MMLU;
15), a benchmark that covers wide domains and knowledge. We compare the following 4 models:
7
(a) Common sense reasoning tasks (b) General world knowledge (MMLU)
Total Memory Average Bit-width1.0
1.2
1.4
1.6
1.8
2.0
2.2
M
em
or
y
(G
B)
2.0
2.2
1.3 1.3
Pythia-1.0B
TinyLLaMA-1.1B
LowRank Llama
OneBit-7B
0
2
4
6
8
10
12
14
16
18
Bi
t-w
id
th
(b
it)
16.0 16.0 16.0
2.88
(c) Memory footprint and bit-width
Figure 3: Comparison of model capabilities and compressive degree.
Pythia-1.0B [3] A well-trained model released by EleutherAI whose memory footprint is 1.54x that
of our OneBit-7B model.
TinyLLaMA-1.1B [42] A model with the same structure as the LLaMA models, which undergoes
continued training. To compare fairly, we use the checkpoint at 10k training steps, which is 2x that of
our OneBit-7B model.
LowRank LLaMA [24] Decompose every weight matrix in Linear layers to two low-rank matrices
and learn from the original LLaMA-7B model by KD in the same setting of OneBit-7B.
OneBit-7B The model that we use in Section 4.2, which is built with OneBit.
Figure 3a and 3b demonstrate common sense reasoning ability and general world knowledge of
different models. We can observe that, although other models have more parameters and are more
thoroughly trained than ours, our model still has advantages in common sense reasoning. This reflects
the benefits inherited from the larger 7B model. In terms of world knowledge, despite a significant
loss in social sciences, our model outperforms the fully trained Pythia-1B in other domains. These
results demonstrate the practical usability of OneBit.
5 Analysis and Discussion
5.1 Efficiency
It is evident that extremely low-bit quantization of weights can significantly reduce the memory
footprint of models. As shown in Table 3, the actual compression ratio increases as the model size
increases. This is particularly meaningful for larger models, making it possible to fit the model into
one GPU. While there is a performance loss, Figure 4 illustrates that our method achieves a good
8
0 6 12 18 24
Model Size (GB)
5
10
15
20
25
Pe
rp
le
xi
ty
o
n
W
ik
i2
OPT-1.3B
OPT-2.7B
LLaMA-7B LLaMA-13B
OPT-1.3B
OPT-2.7B
LLaMA-7B
LLaMA-13B
same size, 0.67 better PPL
same PPL, 0.22x size
Baseline (FP16)
OneBit (W1A16)
Figure 4: Tradeoff between size and PPL.
0 1000 2000 3000 4000 5000 6000
Training Steps
0
20
40
60
80
100
120
Tr
ai
ni
ng
L
os
s
Singular Value Decomposition (SVD)
Non-negative Matrix Factorization (NMF)
Only Copy from Original Weight
Figure 5: Training process of OneBit-7B.
Table 3: Compression ratio of LLaMA models.
Models FP16 (GB) OneBit (GB) Ratio (%)
LLaMA-7B 13.5 1.3 90.4
LLaMA-13B 26.0 2.2 91.5
LLaMA-30B 65.1 4.9 92.5
LLaMA-65B 130.6 9.2 93.4
trade-off between space occupancy and model performance. For example, we can achieve comparable
performance to FP16 with only 0.2x the model space. Furthermore, quantizing to ±1 also aids in
accelerating matrix multiplication on CPUs. It is because the floating-point multiplication of elements
in two matrices can be converted into much faster bit operations on these chips. Thus the substantial
reduction in memory overhead makes these low-bit LLMs meet the requirements for deployment on
PCs and smartphones.
5.2 Robustness
Existing work [38] has already noted the instability within QAT. Extremely low-bit quantization
makes the training process highly sensitive to the learning rate, making it difficult for the model
to converge when the rate is too small or too large. This is primarily due to the large magnitude
of gradients generated as the weight elements fluctuate between +1 and -1, leading to substantial
fluctuations in the output of Linear layers. Experiments demonstrate that OneBit shows more stable
training process and is not sensitive to learning rates. Please refer to Appendix A.5 for more details.
5.3 Effect of Different Components
The variable components in our method primarily include Post-LayerNorm, value vectors, and
parameter initialization.
Post-LayerNorm We discover that there might be floating-point overflow during the QAT process.
As depth increases, the activation can become progressively larger. We tackle it using Post-LayerNorm
instead of Pre-LayerNorm. In contrast, Pre-LayerNorm may occasionally be ineffective.
Value Vectors The main structural difference between OneBit and BitNet [38] is the two value
vectors, which are demonstrated to be effective in Section 4.2. They facilitate stable training and the
knowledge transfer process. Please refer to Appendix A.5 for more details of comparison.
Parameter Initialization In our proposed SVID, both NMF and SVD can be used to decompose
|W| and we recommend using the former. This is because we find that NMF may make the training
more faster to converge. Figure 5 shows that initializing by NMF facilitates better performance.
9
6 Conclusion
We propose a novel model structure for 1-bit weight quantization and a corresponding parameter
initialization method to address the difficulty in 1-bit quantization. Extensive experiments on LLMs
of various sizes and series demonstrate that OneBit has clear advantages over representative strong
baselines and achieves a good tradeoff between model size and performance. We further analyze the
capabilities of such extremely low-bit quantized models and provide guidance for future research.
Limitations
Although our proposed method significantly reduces the memory footprint of LLMs, bringing hope
for efficient deployment of them, there are still some limitations. Firstly, compared to the original
model, our extremely low-bit quantization inevitably incurs a performance loss. Additionally, we are
yet to understand the mathematical principles behind the optimal parameters of the 1-bit quantized
model, thus capability transfer can only be achieved through the relatively costly process of KD.
Fortunately, this cost is a one-time expense. Moreover, due to the unique nature of 1-bit quantization,
our method can not be naturally extended to higher bit-width. Lastly, we have not considered the
activation quantization and leave it as future work.
Ethics Statement
In this study, we employ models that are publicly available and open source. We affirm that the
use of these models aligns with their original intended purposes. These models have been utilized
strictly within the scope of academic and research-based activities, adhering to ethical guidelines and
ensuring compliance with open-source licenses.
References
[1] R. Agarwal, N. Vieillard, P. Stanczyk, S. Ramos, M. Geist, and O. Bachem. GKD: Generalized
knowledge distillation for auto-regressive sequence models. arXiv preprint arXiv:2306.13649,
2023.
[2] E. Beltrami. Sulle funzioni bilineari, giomale di mathematiche ad uso studenti delle uninersita.
11, 98–106.(an english translation by d boley is available as university of minnesota, department
of computer science). Technical report, Technical Report 90–37, 1990.
[3] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O’Brien, E. Hallahan, M. A. Khan,
S. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models
across training and scaling. In ICML, pages 2397–2430, 2023.
[4] Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al. PIQA: Reasoning about physical commonsense in
natural language. In Proceedings of the AAAI, volume 34, pages 7432–7439, 2020.
[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in NeurIPS, 33:
1877–1901, 2020.
[6] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee,
Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4.
arXiv preprint arXiv:2303.12712, 2023.
[7] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. BoolQ:
Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044,
2019.
[8] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think
you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint
arXiv:1803.05457, 2018.
10
[9] T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. In
ICML, pages 7750–7774, 2023.
[10] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. LLM.int8(): 8-bit matrix multiplication
for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.
[11] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. QLoRA: Efficient finetuning of
quantized LLMs. In Advances in NeurIPS, 2023.
[12] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar, S. Ashkboos,
A. Borzunov, T. Hoefler, and D. Alistarh. SpQR: A sparse-quantized representation for near-
lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023.
[13] E. Frantar and D. Alistarh. SparseGPT: Massive language models can be accurately pruned in
one-shot. In ICML, pages 10323–10337, 2023.
[14] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. GPTQ: Accurate post-training quantization
for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.
[15] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring
massive multitask language understanding. In ICLR, 2021.
[16] C.-Y. Hsieh, C.-L. Li, C.-k. Yeh, H. Nakhost, Y. Fujii, A. Ratner, R. Krishna, C.-Y. Lee, and
T. Pfister. Distilling step-by-step! outperforming larger language models with less training data
and smaller model sizes. In Findings of the ACL, pages 8003–8017, 2023.
[17] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, and D. Lee. Memory-efficient
fine-tuning of compressed large language models via sub-4-bit integer quantization. arXiv
preprint arXiv:2305.14152, 2023.
[18] S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W. Mahoney, and K. Keutzer.
SqueezeLLM: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.
[19] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[20] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han. AWQ: Activation-aware weight
quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.
[21] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Krishnamoorthi, and
V. Chandra. LLM-QAT: Data-free quantization aware training for large language models. arXiv
preprint arXiv:2305.17888, 2023.
[22] X. Ma, G. Fang, and X. Wang. LLM-Pruner: On the structural pruning of large language models.
In Advances in NeurIPS, 2023.
[23] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. arXiv preprint
arXiv:1609.07843, 2016.
[24] M. B. Noach and Y. Goldberg. Compressing pre-trained language models by matrix decomposi-
tion. In Proceedings of the AACL-IJCNLP, pages 884–889, 2020.
[25] P. Paatero and U. Tapper. Positive matrix factorization: A non-negative factor model with
optimal utilization of error estimates of data values. Environmetrics, 5(2):111–126, 1994.
[26] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint
arXiv:2304.03277, 2023.
[27] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are
unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[28] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.
Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of
Machine Learning Research, 21(1):5485–5551, 2020.
11
[29] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd
schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.
[30] W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Zhang, P. Gao, Y. Qiao, and P. Luo.
OmniQuant: Omnidirectionally calibrated quantization for large language models. arXiv
preprint arXiv:2308.13137, 2023.
[31] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter. A simple and effective pruning approach for large
language models. arXiv preprint arXiv:2306.11695, 2023.
[32] S. Sun, Y. Cheng, Z. Gan, and J. Liu. Patient knowledge distillation for BERT model compres-
sion. In Proceedings of the EMNLP-IJCNLP, pages 4323–4332, 2019.
[33] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto.
Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/
stanford_alpaca, 2023.
[34] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,
E. Hambro, F. Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971, 2023.
[35] A. Tseng, J. Chee, Q. Sun, V. Kuleshov, and C. De Sa. Quip#: Even better llm quantization
with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024.
[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I. Polosukhin. Attention is all you need. Advances in NeurIPS, 30, 2017.
[37] Z. Wan, X. Wang, C. Liu, S. Alam, Y. Zheng, Z. Qu, S. Yan, Y. Zhu, Q. Zhang, M. Chowdhury,
et al. Efficient large language models: A survey. arXiv preprint arXiv:2312.03863, 2023.
[38] H. Wang, S. Ma, L. Dong, S. Huang, H. Wang, L. Ma, F. Yang, R. Wang, Y. Wu, and F. Wei.
BitNet: Scaling 1-bit transformers for large language models. arXiv preprint arXiv:2310.11453,
2023.
[39] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. SmoothQuant: Accurate and
efficient post-training quantization for large language models. In ICML, pages 38087–38099,
2023.
[40] M. Xu, Y. L. Xu, and D. P. Mandic. TensorGPT: Efficient compression of the embedding layer
in llms based on the tensor-train decomposition. arXiv preprint arXiv:2307.00526, 2023.
[41] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine really
finish your sentence? arXiv preprint arXiv:1905.07830, 2019.
[42] P. Zhang, G. Zeng, T. Wang, and W. Lu. TinyLlama: An open-source small language model.
arXiv preprint arXiv:2401.02385, 2024.
[43] X. Zhu, J. Li, Y. Liu, C. Ma, and W. Wang. A survey on model compression for large language
models. arXiv preprint arXiv:2308.07633, 2023.
A Appendix
A.1 Proofs of Propositions
In this section, we provide the necessary and detailed proofs for the propositions presented in this
paper. All symbols have the same definition as in the main text.
Proposition 1 Given the weight matrix W and input X, the Linear layer can be reformulated as
the following according to SVID:
XWT ≈
[ (
X⊙ bT
)
WT
sign
]
⊙ aT.
12
Proof From Eq. (4), we have wij ≈ sij · aibj , where sij is the element of Wsign. Hence we have(
XWT
)
ij
≈
∑
k
xikw
T
kj =
∑
k
xikwjk
=
∑
k
xiksjkajbk
=
∑
k
xikbksjkaj
=
∑
k
(
X⊙ bT
)
ik
sTkjaj
=
[ (
X⊙ bT
)
WT
sign
]
ij
aj
=
{[ (
X⊙ bT
)
WT
sign
]
⊙ aT
}
ij
.
This proposition is proved.
Lemma 1 Let σi (W) denote the i-th biggest singular value of matrix W. The following inequality
holds:
σ1 (|W|) ≥ σ1 (W) .
Proof According to the definition of induced norm, there are
σ1 (W) = ∥W∥2 = max
x,∥x∥2=1
∥Wx∥2,
σ1 (|W|) = ∥|W|∥2 = max
y,∥y∥2=1
∥|W|y∥2.
Note that for ∀x, ∥x∥2 = 1 and we have
∥|W||x|∥22 =
∑
i
(∑
j
|wij ||xj |
)2
≥
∑
i
(
|
∑
j
wijxj |
)2
=
∑
i
(∑
j
wijxj
)2
= ∥Wx∥22.
Therefore
max
y,∥y∥2=1
∥|W|y∥2 ≥ max
x,∥x∥2=1
∥Wx∥2.
This lemma is proved.
Proposition 2 Given matrices W and |W|, W = Wsign ⊙ |W|. We decompose these matrices in
the way W = abT +E1 and |W| = ãb̃T +E2, where Ei denotes the error matrices. In terms of
the Frobenius-norm, the SVID is closer to the original matrix W:∥∥∥W −Wsign ⊙ ãb̃T
∥∥∥2
F
≤
∥∥∥W − abT
∥∥∥2
F
.
Proof Here we consider SVD to prove it. For SVD, the norm of the error matrix E in the rank-1
approximation is the sum of the squares of all singular values except for the largest one. We have
∥E1∥2F =
n∑
i=2
σ2
i (W) ,
∥E2∥2F =
n∑
i=2
σ2
i (|W|) .
13
Based on ∥W∥2F = ∥|W|∥2F , we have
n∑
i=1
σ2
i (W) =
n∑
i=1
σ2
i (|W|) .
According to Lemma 1, we can conclude
∥E2∥2F ≤ ∥E1∥2F .
From the equation in this proposition, we can formulate
Wsign ⊙ |W| = Wsign ⊙ ãb̃T +Wsign ⊙E2.
Hence we have
W −Wsign ⊙ ãb̃T = Wsign ⊙E2.
Therefore
∥Wsign ⊙E2∥2F =
∑
i,j
s2ije
2
ij =
∑
i,j
e2ij
= ∥E2∥2F ≤ ∥E1∥2F ,
where sij = ±1 is the element of Wsign. Hence the inequation in this proposition is proved.
A.2 Details on Baselines
In this subsection, we provide the essential details of the baselines in this work:
• GPTQ [14]: We employ the open-source code released by the author. Both OPT models
and LLaMA models take 128 2048-token samples from the C4 dataset to calibrate the
quantized model. For LLaMA models, we apply the activation order heuristic according to
the recommendation from the code.
• LLM-QAT [21]: We reimplement this method to adapt the W2A16 setting, as LLM-QAT is
not designed for 2-bit weight quantization. We also do not quantize the KV Cache. When
quantizing the weight matrix in Linear layer, we use symmetric MinMax quantization in
which the zero-point is set to 0. The training hyper-parameters are the same as ours. Please
refer to the training details in Section 4.1.
• OmniQuant [30]: We employ the open-source code released by the author. Both OPT
models and LLaMA models take 128 2048-token samples from the WikiText2 dataset to
calibrate the quantized model. The learning rate for learnable weight clipping and equivalent
transformation is set to 5e-3 and 1e-2, respectively. We use a batch size of 1 and train 40
epochs for each model. For OPT models, both learnable weight clipping and equivalent
transformation are leveraged. For LLaMA models, only learnable weight clipping is used.
A.3 Results of LLaMA2
Table 4 compares the results on LLaMA2-7B/13B. Obviously, our method has advantages in both
perplexity and zero-shot accuracy. It also reflects that the advantages of our method are more
pronounced in larger models. For instance, when scaling from LLaMA2-7B to LLaMA2-13B, the
perplexity of the FP16 model decreases by around only 0.5, whereas our method reduces it by around
1.0 on both Wiki2 and C4 datasets.
A.4 Instrution Following Ability
Instruction following is an important ability of LLMs [27, 5, 26]. Beyond the discussion on model
abilities and efficiency before, we also focus on the instruction following ability of extremely low-bit
models, which is closely related to their practical usability. In this subsection, we empirically study
this capability of our quantized model. We fine-tune the model for 3 epochs using the alpaca_en_52k
dataset and alpaca templates [33], then observe the generation in both zero-shot and few-shot settings
14
Table 4: Results of LLaMA2. We bold the best scores.
Models Methods Perplexity(↓) Zero-shot Accuracy(↑)
Wiki2 C4 Wino. Hella. PIQA BoolQ ARC-e ARC-c Avg.
LLaMA2-7B
FP16 5.47 6.97 67.09 72.94 76.88 71.10 53.58 40.61 63.70
GPTQ 7.7e3 NAN 50.28 26.19 49.46 42.97 26.77 28.58 37.38
LLM-QAT 1.1e3 6.6e2 49.08 25.10 50.12 37.83 26.26 26.96 35.89
OmniQuant 31.21 64.34 51.22 33.87 56.53 59.14 33.63 24.32 43.12
OneBit 9.73 11.11 58.41 52.58 68.12 63.06 41.58 29.61 52.23
LLaMA2-13B
FP16 4.88 6.47 69.77 76.62 79.05 68.99 57.95 44.20 66.10
GPTQ 2.1e3 3.2e2 51.85 25.67 51.74 40.61 25.46 27.30 37.11
LLM-QAT 5.1e2 1.1e3 51.38 24.37 49.08 39.85 27.15 24.32 36.03
OmniQuant 16.88 27.02 53.20 50.34 62.24 62.05 40.66 29.61 49.68
OneBit 8.76 10.15 61.72 56.43 70.13 65.20 43.10 33.62 55.03
before and after fine-tuning. During training, the learning rate is set to 1e-7 and the batch size to 32.
Other parameters are consistent with Section 4.1.
Table 5 demonstrates the content generation and instruction following abilities of our 7B model.
Under the zero-shot setting, the model without SFT produced verbose, repetitive, and low-quality
text. However, once experienced to SFT, our model is able to smoothly output high-quality content,
exhibiting excellent instruction following ability. For the few-shot setting, our model exhibits
instruction following ability both before and after SFT.
A.5 Comparison with BitNet
Recently, BitNet [38] introduces a 1-bit model architecture and applies the architecture to train
models from scratch, demonstrating the feasibility and application value of the 1-bit model structure.
In this paper, we attempt to combine 1-bit quantization with knowledge distillation to quantize the
LLaMA-7B model. Unfortunately, despite following the suggestion to use larger learning rates, the
behavior remains unstable during training.
Figure 6 shows that the training process of BitNet may suffer from instability during knowledge
distillation. We conjecture that it is because the gradient is pretty large when the weight elements
fluctuate between +1 and -1, further aggravating the output of the Linear layer.
As a more effective measure, the value vectors we propose for quantization not only supplement the
necessary floating-point numerical precision but also limit the fluctuation range of the matrix multi-
plication results after quantization. This can be understood from forward and backward computation,
respectively.
Forward stability. Quantized matrix multiplication is more prone to overflow than FP16 coun-
terparts in response to minor perturbations of input activations. This is because the magnitude of
elements in quantized matrices, particularly the value ±1, is far greater than the parameters of most
FP16 matrices. By multiplying by value vectors of a magnitude similar to that of the FP16 model, the
range of variation in model output activations can be restored to the level of FP16. Furthermore, we
also avoid the increasingly large “drift phenomenon” of activations through Post-LayerNorm.
Backward stability. Since Sign(·) function is not differentiable, when the elements of the matrix
change, their gradient may become infinite. Similar to forward stability, by multiplying two numer-
ically smaller value vectors, we avoid layer-by-layer accumulation and explosion during gradient
back-propagation. Moreover, we implement the derivative function of Sign(·) using the derivative of
the hyperbolic tangent function, thereby avoiding the problem of gradient explosion at the zero point
of every weight.
15
Table 5: Instruction following cases in zero-shot and few-shot settings before and after Supervised
Fine-Tuning (SFT). The few-shot examples are from MMLU benchmark.
Zero-shot w/o SFT
Prompt:
List several Windows operating systems developed by Microsoft corporation.
Generation:
Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal
computers and tablets. It was first released to the public on 29 April 2014.
Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal
computers and tablets. It was first released to the public on 29 April 2014. Windows 10 is the
successor to Windows 8.1.
Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal
computers and tablets. It was first released to the public on 29 April 2014. Windows 10 is the
successor to Windows 8.1. Windows 10 is the successor to Windows 8.1.
Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal
computers and tablets.
Zero-shot w/ SFT
Prompt:
Below is an instruction that describes a task. Write a response that appropriately completes
the request.
### Instruction:
List several Windows operating systems developed by Microsoft corporation.
### Response:
Generation:
Microsoft has developed several Windows operating systems, including Windows 10, Windows
8, Windows 7, Windows XP, Windows 2000, Windows NT, Windows ME, Windows 95, and
Windows 1. Windows 10 is the latest version of the Windows operating system. It was released
in 2019. Windows 8 was released in 2011. Windows 7 was released in 2009. Windows XP
was released in 2001. Windows NT was released in 1997. Windows ME was released in 1999.
Windows 95 was released in 1995. Windows 1 was released in...
Few-shot w/o SFT
Prompt:
The following are multiple choice questions (with answers) about college biology.
Which of the following contain DNA sequences required for the segregation of chromosomes
in mitosis and meiosis? A. Telomeres B. Centromeres C. Nucleosomes D. Spliceosomes
Answer: B. The Barr body found within female mammalian cells’ nuclei is an example of: A.
euchromatin B. heterochromatin C. a cistron D. pseudogenes Answer: B. Cleavage symmetry
arising from early cell divisions that occur at an oblique angle to the polar axis is known
as: A. meroblastic cleavage B. radial cleavage C. spiral cleavage D. superficial cleavage
Answer: C. Which of the following is NOT a source of atmospheric carbon? A. Respiration
B. Photosynthesis C. Bacterial decomposition D. Combustion of fossil fuels Answer: B. The
concept of punctuated equilibrium refers to: A. oscillating ecological successional stages
B. ecological succession arrested by sudden environmental changes, e.g., fire C. persistent
predator-prey relationships in relatively stable environments D. bursts of speciation followed
by relatively unchanging lineages Answer: D. Which of the following is a second messenger
that stimulates release of calcium ions into the cytoplasm? A. Prostaglandins B. Calmodulin
C. Cyclic AMP D. Inositol triphosphate Answer:
Generation:
B
Few-shot w/ SFT
Prompt:
Below is an instruction that describes a task. Write a response that appropriately completes
the request.
### Instruction:
The following are multiple choice questions (with answers) about college biology.
Which of the following contain DNA sequences required for the segregation of chromosomes
in mitosis and meiosis? A. Telomeres B. Centromeres C. Nucleosomes D. Spliceosomes
Answer: B. The Barr body found within female mammalian cells’ nuclei is an example of: A.
euchromatin B. heterochromatin C. a cistron D. pseudogenes Answer: B. Cleavage symmetry
arising from early cell divisions that occur at an oblique angle to the polar axis is known
as: A. meroblastic cleavage B. radial cleavage C. spiral cleavage D. superficial cleavage
Answer: C. Which of the following is NOT a source of atmospheric carbon? A. Respiration
B. Photosynthesis C. Bacterial decomposition D. Combustion of fossil fuels Answer: B. The
concept of punctuated equilibrium refers to: A. oscillating ecological successional stages
B. ecological succession arrested by sudden environmental changes, e.g., fire C. persistent
predator-prey relationships in relatively stable environments D. bursts of speciation followed
by relatively unchanging lineages Answer: D. Which of the following is a second messenger
that stimulates release of calcium ions into the cytoplasm? A. Prostaglandins B. Calmodulin
C. Cyclic AMP D. Inositol triphosphate Answer:
### Response:
Generation:
D
16
0 50 100 150 200 250 300 350
Training Steps
0
50
100
150
200
250
300
350
Tr
ai
ni
ng
L
os
s
(a) learning rate 4e-4
0 100 200 300 400 500 600
Training Steps
0
50
100
150
200
250
300
350
Tr
ai
ni
ng
L
os
s
(b) learning rate 6e-4
0 200 400 600 800
Training Steps
0
50
100
150
200
250
300
350
Tr
ai
ni
ng
L
os
s
(c) learning rate 8e-4
0 250 500 750 1000 1250 1500 1750
Training Steps
0
50
100
150
200
250
300
350
Tr
ai
ni
ng
L
os
s
(d) learning rate 10e-4
0 200 400 600 800 1000 1200
Training Steps
0
50
100
150
200
250
300
350
Tr
ai
ni
ng
L
os
s
(e) learning rate 12e-4
0 100 200 300 400 500 600 700 800
Training Steps
0
50
100
150
200
250
300
350
Tr
ai
ni
ng
L
os
s
(f) learning rate 15e-4
Figure 6: Training comparisons among different learning rates when BitNet performs knowledge
distillation from LLaMA-7B. Here we choose the same W1A16 setting as ours. The weight matrices
in BitNet are directly copied from the original LLaMA-7B model.
A.6 Discussion on Knowledge Distillation
Although knowledge distillation is not the main contribution of this paper, we nevertheless provide
the rationale behind certain settings used in our experiments to explain the necessity of these
configurations.
We firstly explain the role of different loss functions in guiding the process of knowledge transfer.
Fundamentally, distillation loss alone can achieve a satisfactory transfer process (comparing to other
baselines). Additionally, as shown in the Table 6, aligning the hidden states between layers can
result in a quantized model with better perplexity. However, further incorporating attention score
alignment on this basis leads to the model failing to converge. LLM-QAT [21] has conducted similar
experiments on quantization-aware knowledge distillation loss and concluded that using only the
distillation loss yields the best results. The difference in conclusions may stem from two factors. On
one hand, due to our adoption of a novel model architecture, which differs from theirs, the optimal
17
Table 6: Ablation study of different loss on LLaMA-7B. “ATTN” means attention score alignment.
Loss Setting Perplexity(↓) Zero-shot Accuracy(↑)
Wiki2 C4 Wino. Hella. PIQA BoolQ ARC-e ARC-c Avg.
LKD 13.48 14.57 50.83 35.14 62.89 60.46 37.33 26.37 45.50
LKD + LMSE (α = 1) 10.19 11.40 58.48 51.54 68.01 57.28 42.47 30.20 51.33
LKD + LMSE (α = 10) 10.38 11.56 60.30 50.73 67.46 62.51 41.71 29.61 52.05
LKD + LMSE + LATTN NAN NAN - - - - - - -
usage of loss functions may be different as well. On the other hand, as we focus on extremely low
bit-width compression, each layer of the model suffers significant information loss compared to the
teacher model. The regularization of hidden states between layers may help reduce the variance in
the learning process, thus demonstrating stronger generalization.
Furthermore, we also discuss the cost of our quantization method. Using LLaMA-7B as an example,
quantizing the model with our method requires approximately 7 days on 8 A100-80GB GPUs. In
comparison, training the LLaMA-7B model from scratch consumes 82,432 GPU hours [34]. The
quantization time, being less than 2% of the pretraining time, is still an acceptable cost.
A.7 Average Bit-width of Linear Layer
This subsection formulates the calculation of the average bit-width of Linear layers. Assume there is
a weight matrix with a shape of 4096× 4096 in such a layer, the number of bits in every component
is
1× 4096× 4096,
16× 1× 4096× 2,
where the first is for the 1-bit quantized weight matrix and the second is for the two FP16 value
vectors. Hence the overall number of bits is 16, 908, 288. Moreover, the number of parameters is
4096× 4096 + 2× 4096× 1 = 16, 785, 408. Therefore, the average bit-width of this Linear layer
is 16, 908, 288÷ 16, 785, 408 ≈ 1.0073.
18
第2个文件的内容为：
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
Wei Huang 1 Yangdong Liu 2 Haotong QinB 3 Ying Li 2 Shiming Zhang 1
Xianglong Liu 2 Michele Magno 3 Xiaojuan Qi 1
Abstract
Pretrained large language models (LLMs) exhibit
exceptional general language processing capabili-
ties but come with significant demands on mem-
ory and computational resources. As a power-
ful compression technology, binarization can ex-
tremely reduce model weights to a mere 1 bit,
lowering the expensive computation and mem-
ory requirements. However, existing quantiza-
tion techniques fall short of maintaining LLM
performance under ultra-low bit-widths. In re-
sponse to this challenge, we present BiLLM, a
groundbreaking 1-bit post-training quantization
scheme tailored for pretrained LLMs. Based on
the weight distribution of LLMs, BiLLM first
identifies and structurally selects salient weights,
and minimizes the compression loss through an
effective binary residual approximation strategy.
Moreover, considering the bell-shaped distribu-
tion of the non-salient weights, we propose an op-
timal splitting search to group and binarize them
accurately. BiLLM, for the first time, achieves
high-accuracy inference (e.g. 8.41 perplexity on
LLaMA2-70B) with only 1.08-bit weights across
various LLM families and evaluation metrics, out-
performs SOTA quantization methods of LLM by
significant margins. Moreover, BiLLM enables
the binarization process of a 7-billion LLM within
0.5 hours on a single GPU, demonstrating satis-
factory time efficiency. Our code is available at
https://github.com/Aaronhuang-778/BiLLM.
1. Introduction
Recently, large language models (LLMs) based on trans-
formers (Vaswani et al., 2017) have garnered significant
attention in natural language processing. Pre-trained LLMs
1The University of Hong Kong 2Beihang University
3ETH Zürich. Correspondence to: Haotong Qin <qinhao-
tong@buaa.edu.cn>.
Proceedings of the 41 st International Conference on Machine
Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by
the author(s).
5.68 5.68
6.28
25.53
106767.33
168388
5.68 5.68
6.19
11.1
152.31
267001.71
1
10
100
1000
10000
100000
1000000
16 8 4 3 2 1
RTN
GPTQ
PB-LLM (INT 8, 10%)
BiLLM (Ours)
15.14 (Ours)
838.13
Quantization Bit-width
Pe
rp
le
xi
ty
in
퐥
�

ퟏ
s
ca
le
LLaMA-13B
Figure 1: Perplexity of LLaMA-13B on WikiText2 under
different bit-widths. Round-to-nearest (RTN), GPTQ, and
PB-LLM (10% weight of INT8) suffer accuracy loss at ultra-
low bits, facing the sharply increasing perplexity (↓). BiLLM
demonstrates exceptional performance under binarization.
such as OPT (Zhang et al., 2022) and LLaMA (Touvron
et al., 2023a), have demonstrated excellent performance
across a range of evaluation benchmarks. However, LLMs
pose substantial challenges in deployment on memory-
constrained devices due to their immense parameter size
and computation requirements. For instance, the widely-
used LLaMA2-70B (Touvron et al., 2023b) model, with its
70 billion parameters, requires 150 GB of storage in half-
precision (FP16) format. This necessitates at least two A100
GPUs, each with 80 GB of storage space, for inference.
Model quantization has emerged as a highly effective tech-
nology for compressing neural networks, thereby reducing
the model size of LLMs and substantially saving GPU mem-
ory consumption (Dettmers et al., 2022). Current quan-
tization techniques primarily fall into Quantization-Aware
Training (QAT) and Post-Training Quantization (PTQ). QAT
involves fine-tuning and retraining during the quantization
process, while PTQ significantly streamlines the compu-
tation by eliminating back-propagation, enabling a faster
quantization process and promoting the practicality of quan-
tization (Frantar et al., 2022; Shang et al., 2023; Lin et al.,
2023). Given the deep structures and numerous parameters
of LLMs, PTQ stands out for its ability to rapidly perform
1
ar
X
iv
:2
40
2.
04
29
1v
2
[
cs
.L
G
]
1
5
M
ay
2
02
4
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
the quantization process, especially on time and resource-
constrained scenarios (Zhu et al., 2023).
Despite the success of previous PTQ methods in 8-bit and
4-bit quantization (Dettmers et al., 2022; 2023b; Frantar
et al., 2022; Xiao et al., 2023; Frantar & Alistarh, 2022), the
expanding size of LLMs demands more aggressive quan-
tization approaches (Shang et al., 2023). Neural network
binarization, which reduces the weight bit-width to only 1
bit, is a promising approach (Helwegen et al., 2019; Qin
et al., 2020; 2023). However, as depicted in Figure 1, current
advanced PTQ methods for LLMs exhibit a performance
collapse under ultra-low bit (⩽3 bits) quantization. This
phenomenon can be attributed to the significant difference
between quantized and original weights. Even the recent bi-
nary PTQ method for LLMs, PB-LLM (Shang et al., 2023),
only maintains a perplexity metric of around 800 with an
average weight of 1.7 bits. This observation underscores
the challenges existing PTQ methods face in promoting the
weight binarization of LLMs.
In pursuit of this goal, we conducted an empirical study to
analyze the distribution of pre-trained weights in LLMs. The
findings derived from this study are presented in Appendix
G, revealing two key observations:
• The second-order Hessian matrix of weights demon-
strates an exceptionally long-tail distribution and is
often used to measure the importance of weight ele-
ments in neural networks (LeCun et al., 1989; Dong
et al., 2019). As depicted in Figure 2, a small fraction
of weights elements possesses significantly high Hes-
sian values, substantially influencing the layer output.
In contrast, most Hessian values cluster around 0.
• The density distribution of weight magnitudes in LLMs
follows a bell-shaped pattern. This bell-shaped dis-
tribution exhibits a significant resemblance to both the
Gaussian or Laplace distribution in terms of its char-
acteristics (Blundell et al., 2015). Figure 2 illustrates
that most weight values cluster around zero with a
non-uniform bell-shaped distribution.
The above implies: a) A minority of weights play an impor-
tant role in LLMs, whereas the majority of weights exhibit
characteristics of redundancy (Shang et al., 2023; Dettmers
et al., 2023b); b) With the most aggressive bit-width, bina-
rization incurs most severe error among quantization under
bell-shaped distributions in LLMs (Jacob et al., 2018).
Motivated by the above observation, we propose a novel
1-bit PTQ framework for LLMs, namely BiLLM, incorpo-
rating two core designs to achieve highly accurate weight
binarization. First, guided by the Hessian-based metric, we
select the salient weights structurally (Figure 3 upper-right)
to achieve a trade-off between accuracy and storage sav-
Density Distribution of Weight
��
��+�
...
long-tail distribution
fre
qu
en
cy Hessian
00
Se
ns
itiv
ity
Value
Magnitude
bell-shaped distribution
Figure 2: The Hessian metrics (sensitivity) and magnitude
(value) of weights in LLMs. The weights of different lay-
ers in LLMs are characterized by bell-shaped distribution,
accompanied by a few salient values.
ings and develop a residual approximation to maximize the
restoration of salient weights with highly dynamic range.
Second, for the remaining non-salient weights (Figure 3
lower-right), we design an optimal splitting binarization
strategy, where a meticulous search process is applied to de-
termine an optimal break-point for weight distribution and
binarization of the segments is then processed separately to
minimize binarization errors. Moreover, BiLLM incorpo-
rates error compensation on a block-wise basis by default
following existing common practices (Frantar et al., 2022;
Shang et al., 2023), which further reduces quantization error.
Extensive experiments demonstrate that BiLLM achieve the
state-of-the-art (SOTA) performance for LLMs across mul-
tiple LLM families on various evaluation metrics, and first
achieves extremely compact 1.07∼1.11 bit-width in aver-
age for the PTQ binarization. For example, on the Wiki-
text2(Merity et al., 2016) metric, BiLLM achieved perplexi-
ties of 8.49 and 8.41 with only 1.08-bit weights on LLaMA-
65B (Touvron et al., 2023a)and LLaMA2-70B (Touvron
et al., 2023b), respectively, even surpassing the 9.34 perfor-
mance of the FP16 OPT-66B (Zhang et al., 2022).
2. Related Works
2.1. Large Language Model Quantization
Quantization maps high-precision parameters to a discrete
range. This method, which compresses parameters without
altering the model structure, effectively reduces the storage
and computational overhead of deep neural networks. Re-
cent work has successfully applied QAT and PTQ to LLMs.
QAT, through a quantization-aware retraining strategy, bet-
ter preserves the performance of quantized models. LLM-
QAT (Liu et al., 2023) addressed data barrier issues in QAT
training through data-free distillation. However, for LLMs
with extremely large parameter sizes, the cost of retraining
is prohibitively high and inefficient. Therefore, techniques
such as QLoRA (Dettmers et al., 2023a) focus on parameter-
efficient fine-tuning (PEFT) methods for quantizing LLMs,
enhancing the efficiency of QAT. Nevertheless, even these
2
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
V
al
ue
Binarized
FC for Q
Binarized
FC for K
Binarized
FC for V
MatMul
MatMul
Activation
Activation
Binarized
FC1
Activation
Binarized
FC2
Activation
Multi-Head
Self-Attrntion
Feed-Fordward Block
Ab
so
lu
te
V
al
ue
BiLLM Framework
Binarized-FC Projection
Binary Weight
Hessian Matrix
Float Weight
V
al
ue
Residual
Binarization
�∗
Bell-shaped Splitting for Non-salient Weight
Splitting
Binarization
Binary Residual Approximation for Salient Weight
V
al
ue
V
al
ue V
al
ue
V
al
ue
Figure 3: Schematic of the PTQ binarization framework for LLMs. The left side shows the structure of the Transformer
block after binarization. The right side shows the binarization process of BiLLM, which consists of two parts, Residual
Approximation for salient weights and Bell-shaped Splitting for non-salient weights.
efficient fine-tuning quantization strategies require over 24
hours of GPU time.
Therefore, the PTQ strategy has become a significant
option for quantizing LLMs efficiently. Works like
BRECQ (Li et al., 2021), ZerqQuant (Yao et al.) and
LLM.int8() (Dettmers et al., 2022) enhance quantization
accuracy by adding additional grouping labels for custom
quantization blocks. Other studies adopt a feature segmen-
tation strategy, such as PB-LLM (Shang et al., 2023) and
SpQR (Dettmers et al., 2023b). They preserve the bit-width
of outlier features or those with higher quantization errors
to FP16 or INT8, mitigating the precision loss due to quanti-
zation. GPTQ (Frantar et al., 2022) employs a more precise
quantization framework, reducing the block quantization
errors of LLMs through Hessian-based second-order er-
ror compensation (Frantar & Alistarh, 2022), achieving
commendable performance in low-bits (4 bits) quantization.
Smoothquant (Xiao et al., 2023) introduces a strategy of
scaling weight and activation outliers to simplify quantiza-
tion. Subsequently, AWQ (Lin et al., 2023) and OWQ (Lee
et al., 2023) also proposed scale transformations of more
crucial weight channels for activation features, preserving
their information representation capacity.
2.2. Network Binarization
Binarized compression can quantize parameters to only 1 bit,
expressed as ±1. In forward propagation, the sign function
is used to binarize the original parameter tensor:
Wb = α · sign(Wf ), (1)
sign(x) =
{
1 if x ≥ 0,
−1 others.
(2)
where Wf ∈ Rn×m is the full precision weight and Wb ∈
Rn×m is the binarized output. n and m represent the size of
the weight matrix. α denotes the scaling factor (Courbariaux
et al., 2016). Binarization usually uses the channel-wise
scale (Rastegari et al., 2016; Qin et al., 2023), so α ∈ Rn.
Most previous binarization works adopt a framework based
on QAT for quantization (Qin et al., 2023). Straight through
estimator (STE) (Bengio et al., 2013) is deployed to ad-
dress the issue of gradient vanishing caused by the sign(·)
function. Binary Weight Network (BWN) (Rastegari et al.,
2016) was initially proposed for executing neural network
computations by binarizing weights and using full-precision
activations, while XNOR-Net (Rastegari et al., 2016) ex-
tends this approach by binarizing both weights and activa-
tions. Both methods minimize quantization errors through
dynamic searching of α. DoReFa-Net (Zhou et al., 2016)
further expands upon XNOR-Net, employing quantized gra-
dients to accelerate network training. Group segmentation
is also applied in binarization tasks, with Syq (Faraone et al.,
2018) utilizing network weight to the small size of groups
for minimizing binarization errors.
Based on the successful application of binarization in Trans-
formers (Wang et al., 2023) and Bert (Qin et al., 2022), we
believe that the binarization of LLMs is filled with poten-
tial. PB-LLM (Shang et al., 2023) investigates the impact
of binarized QAT and PTQ strategies on LLMs, but it is
necessary to retain a significant proportion (over 30%) of
the weights at 8 bits to enable LLMs to produce reasonable
answers. Due to the presence of a large amount of INT8,
LLMs still have a relatively high average bit-width. To ad-
dress this issue, we proposed BiLLM, which aims to push
the limit of PTQ binarization for LLMs.
3. Method
To achieve accurate binarization of LLMs, our approach is
designing distinct binarization strategies for salient and non-
3
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
salient weights. We first introduce the selection rules for
salient weights and their binarization strategies in Section
3.1. Then, we elaborate on the distribution-based binariza-
tion strategy for non-salient weights in Section 3.2.
3.1. Salient Weight Binarization for LLMs
In deep neural networks, not all parameters carry equal sig-
nificance. Utilizing solely the magnitude of the weights
can not fully capture the impact of each element on the
model’s performance. The Hessian metric serves as a com-
mon benchmark for detecting parameter sensitivity (Dong
et al., 2019; Dettmers et al., 2023b; 2022). We thus leverage
the Hessian matrix to assess the salience of parameters in
each under-binarized layer. We implement an optimized
computation process to derive weight sensitivity, which
allows us to obtain the importance metric of parameters
without compromising efficiency:
si =
w2
i
[H−1]2ii
, (3)
where H represents the Hessian matrix of each layer, and
wi represents the original value of each element. In the
following section, si serves as a criterion for assessing the
significance of weight elements and is used as a feature
indicator for structured selection.
Structural Searching Selection. Utilizing an unstructured
selection enables the coverage of all salient elements. How-
ever, it requires the implementation of an additional 1-bit
bitmap index (Chan & Ioannidis, 1998), leading to increased
average bit-width. This balance is inefficient, especially for
Hessian outlier weights that constitute a mere 1-5% of the
total (Yao et al., 2023). In our analysis of sensitivity distri-
bution within LLMs, we discovered that the majority of the
weights’ sensitive Hessian values are predominantly con-
centrated in specific columns or rows (Appendix G). This
pattern is attributed to the convergence effects inherent in
the multi-head self-attention mechanism of these models
and further motivates us to implement a structured approach
for selecting salient weights, for reducing the additional
bitmap. Given that BiLLM employs a per-channel (or per-
row) type of binarization, we determine salience through a
per-column segmentation on the whole weight matrix.
We organize the column salience in descending order and
introduce an optimized search algorithm aimed at minimiz-
ing quantization error, which in turn determines the number
of columns within the salient group. To elaborate on this
methodology, we initially define the objective of binariza-
tion quantization, grounded on Equation (1):
argmin
α,B
||W − αB||2, (4)
where B ∈ {−1,+1}n×k, k is the number of selected
columns. The problem (Rastegari et al., 2016) of optimal
��
�퐬퐚퐥�  猀
Resdual
original
binarization
residual
binarization
H
�퐬퐚퐥�  猀
�
-
��
Figure 4: Illustration of salient weight binarization. The B1
binarized from salient weight is made into a residual with
the original value and then binarized again to obtain B2.
α and B can simply be solved as α = ||W||ℓ1
n×k and B =
sign(W). Then, the optimization function for selecting
salient columns is defined as:
argmin
Wuns
||W−(αsal sign(Wsal)∪αuns sign(Wuns))||2, (5)
where Wsal denotes the column-wise combination of orig-
inal weight and Wuns is the left non-salient part. We can
easily get that W = Wsal ∪ Wuns, so the only variable
parameter is the number of rows in Wsal.
Binary Residual Approximation. Salient weights are lim-
ited in quantity, yet exhibit significant variance when ag-
gregated. Direct preservation of these weights in INT8 or
FP16 formats leads to an increase in the average weight bits,
undermining the compressive benefits of binarization. Tra-
ditional binarization methods for salient weights, however,
result in substantial quantization errors. To that end, we
develop a residual approximation approach for binarizing
salient weights. Contrary to the comprehensive high-order
quantization (Li et al., 2017) applied to the entire weight
matrix, our technique minimizes binarization error through
a second-order approximation of merely a select subset of
salient weights. This method guarantees the precision of
salient weights while simultaneously decreasing bit-width
overhead. As illustrated in Figure 4, this approach incor-
porates a recursive computation strategy for weight bina-
rization compensation, applying a subsequent binarization
process to the residuals remaining after the initial binary pro-
cess. Building upon Equation (4), we propose a redesigned
residual approximation optimization specifically for salient
weights, which is defined as follows:
α∗
o,B
∗
o = argmin
αo,Bo
||W − αoBo||2),
α∗
r ,B
∗
r = argmin
αr,Br
||(W − α∗
oB
∗
o)− αrBr||2),
(6)
where Bo represents the original binary tensor, while Br
denotes the residual binarized matrix with the same size as
Bo. We efficiently solve for the two binarized optimization
objectives using the same solution method as in Equation (4).
4
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
Non-salient Weight Distribution
Optimal Breakpoint Search
Sparse AreaSparse Area
Sensitivity Matrix
Salient WeightNon-salient
Weight
Concentrated Area
�∗
Figure 5: Distribution and splitting schematic of the 4th
projection layer in LLaMA2-7B. The top 5% of the Hessian
elements are orange, and the optimal break-point divides
the non-salient weights into sparse and concentrated areas.
Ultimately, we arrive at the following approximation:
W ≈ α∗
oB
∗
o + α∗
rB
∗
r . (7)
It can be easily proven that the residual approach of Equa-
tion (7) has a lower quantization error than the direct one of
Equation (4). We define the residual binarization error E :
Erb = ||W − α∗
oB
∗
o − α∗
rB
∗
r ||2. (8)
The original binarized quantization error is calculatde as
||W − α∗
oB
∗
o||2 by Equation (4), and from the second
sub-equation of Equation (6) we can get that loss Erb ≤
||W − α∗
oB
∗
o||2. Therefore, through the method of resid-
ual approximation, we are able to further reduce the binary
quantization error of salient weights with ultra-low bit-width
storage compared to retaining salient weights at 8 or 16 bits.
3.2. Bell-shaped Distribution Splitting for Binarization
Following the removal of salient weights, the remaining
weights maintain a bell-shaped distribution, which becomes
closer to symmetric with the exclusion of salient weights’
impact, as depicted in Figure 5. Binary quantization, rep-
resenting an extreme form of uniform quantization, en-
counters more loss in the presence of non-uniform distribu-
tions. A practical approach involves the group-wise quan-
tization (Park et al., 2018; Fang et al., 2020; Jain et al.,
2019) of weights according to their distribution. Balancing
between quantization accuracy and compression efficiency,
we identify a single break-point within the distribution. As
shown in Figure 5, this partition divides the non-salient bell-
shaped distribution into two categories: the sparse area and
the concentrated area.
The segmentation process identifies a break-point that cat-
egorizes non-salient weights into two groups: Ac[−p, p]
for concentrated weights and As[−m,−p] ∪ [p,m] for
sparse weights, where signifies the maximum extent of
non-salient weights. We then apply binarization to both
Ac (concentrated) and As (sparse). To determine the opti-
mal break-point p∗, we assume that the non-salient weights
possess a symmetrical probability density function (PDF)-
g(x) over the bounded domain [−m,m], with the properties
g(x) = g(−x). Then the mean squared quantization error
of binarization is defined as:
θ2q =
∫ 0
−m
(−α− x)2g(x)dx+
∫ m
0
(α− x)2g(x)dx. (9)
Since g(x) is a symmetric function, the above formula is
simplified to:
θ2q = 2
∫ m
0
(α− x)2g(x)dx. (10)
Then, the break-point p divides the non-salient weights into
two parts. According to the Equation (10), under the discon-
tinuous weight distribution, we get a new binary quantiza-
tion error:
θ2q,p = ||Ws − αsBs||2 + ||Wc − αcBc||2, (11)
where Ws and Wc denote the weights of the sparse and
concentrated area, respectively. Bs and Bc were calculated
from Equation (2), αs and αc are the binarization scales,
determined by Equation (4):
αs =
1
ns
||Ws||ℓ1, αc =
1
nc
||Wc||ℓ1, (12)
where n represents the number of weight elements in each
area. Therefore, the problem function is only related to p,
and our target to find the optimal p∗ can be defined as:
p∗ = argmin
p
(θ2q,p). (13)
When the remaining weights follow an ideal Gaussian
distribution, Equation (11) is demonstrated to be a con-
vex function with a global minimum, as evidenced in
prior studies (Fang et al., 2020; You, 2010). Nonetheless,
the actual distribution of non-salient weights, while bell-
shaped, diverges from the ideal Gaussian model. Simultane-
ously, we retain the block-wise compensation strategies of
GPTQ (Frantar et al., 2022) and OBC (Frantar & Alistarh,
2022) to offset quantization errors, which could change the
distribution of weights. In response, we employ a percentile
search method to identify the optimal break-point based
on the objective function outlined in Equation (13). This
percentile search strategy is efficient and straightforward,
completing the binarization process for a 7B LLM within
merely 30 minutes. Furthermore, our findings indicate that
despite the deviation of non-salient weights from the ideal
Gaussian distribution, the error curve associated with the
search process still exhibits convex properties (as detailed
in Appendix C), confirming the feasibility of pinpointing
the optimal break-point.
5
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
1
1.005
1.01
1.015
1.02
1.025
1.03
1.035
1.04
1.045
1.05
32 64 128 256 512 1024
A
ve
ra
ge
b
it-
w
id
th
block size
storing
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Av
er
ag
e
bi
t-w
id
th
salient ratio
weights
Figure 6: Weights and hardware overhead changes on
Llama-7B. The left shows the calculation parameters as
a function of the significant weight ratio; the right shows
the hardware overhead as a function of the block.
Table 1: Average bit results from structural searching and
residual binarization of OPT, LLaMA, and LLaMA2.
Model 7B 13B 30B 66B/65B/70B*
OPT 1.10 1.12 1.12 1.13
LLaMA 1.09 1.09 1.10 1.10
LLaMA2 1.07 1.08 N/A 1.09
*: OPT-66B, LLaMA-65B and LLaMA2-70B.
3.3. Pipeline of BiLLM
As depicted in Figure 3 left, BiLLM primarily performs
binary quantization on all Linear weights within the Trans-
former blocks. This section introduces the detailed pipeline
of BiLLM.
Binarization Workflow. We first deploy the structural
search of salient columns and a residual approximation
binarization for salient columns. The process of salient
columns incurs additional weight bits due to the search
proportion and residual mechanism. Table 1 presents the
extra bits generated in some LLMs (Zhang et al., 2022; Tou-
vron et al., 2023a;b). It can be observed that the searching
and residuals bring only about 0.1 additional weight bits.
Then, for these non-uniformly distributed weights, we use
a split binarization strategy searching optimal p∗. The con-
centrated area and the sparse area are binarized separately.
This part incurs the cost of an additional 1 bit for hardware
group identification, but the computing parameters are still
compressed to 1 bit. By retaining only block-wise com-
pensation(Frantar et al., 2022; Frantar & Alistarh, 2022)
and eliminating column-wise quantization error compensa-
tion, we further enhance the efficiency of PTQ and ensure
the effectiveness of distribution exploration. Algorithm 1
illustrates the complete process of BiLLM, and detailed im-
plementation of BiLLM is shown in Appendix A.
Extra Storing Bits. The extra bits is acceptable under the bi-
nary weight quantization of BiLLM. The weight parameters
and additional hardware overhead are as follows: Nparam = 2× rsalient + 1× (1− rsalient),
Nstoring = 1 +
1
bsize
,
(14)
where rsalient signifies the proportion of salient weights and
bsize denotes the block size in OBC compensation, with 1
bit allocated for marking the division of non-salient weights.
1
bsize
represents the identifier for the structured column of
salient weights. For example, a 10% structural selection
along with an OBC compensation of size 128 was employed.
This results in a weight parameter bit-width of 1.1 bits and a
hardware flag bit-width of 1.008 bits. Figure 6 illustrates the
weight overhead for different proportions and block sizes.
It is important to note that flag weights do not participate
in the computation; actual calculations are executed solely
with parameter weights. Therefore, additional hardware
identification bits do not affect the acceleration effect of
binary quantization.
Algorithm 1 Main Framework of BiLLM: Inner details of
each function are shown in Algorithm 2
func BinaryLLM(W, X, β, λ)
Input: W ∈ Rn×m - weight matrix
X ∈ Rr×d - calibration data
β - block size
λ - hessian regularizer
Output: B - binarized weights
1: H := 2XX⊤ // ℓ2 error hessian matrix
2: Hc := Cholesky((H+ λI)
−1
)
3: B := 0n×m
4: for b = 0, β, 2β, ..., N do
5: Wb := W:,b:b+β
6: rows{·} := salient(W:,b:b+β ,H
c)
7: B̃1 := res approximation(Wb
:,j∈{rows})
8: p∗ := seg search(Wb
i,j /∈{rows})
9: B̃2 := binary(Wb
|wi,j |≤p∗,j /∈{rows})
10: B̃3 := binary(Wb
|wi,j |>p∗,j /∈{rows})
11: B:,b:b+β := B̃1 + B̃2 + B̃3
12: E := (W:,b:b+β −B:,b:b+β)/H
c
bb:b+βb+β
13: W:,b+β: := W:,b+β:−E·Hc
b:b+β,b+β: // block-wise
OBC
14: end for
15: return B
4. Experiments
4.1. Setup
We deploy BiLLM within the Pytorch (Paszke et al., 2019)-
Huggingface libraries (Wolf et al., 2019). All the binariza-
tion processes and experiments are conducted on a single 80
6
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
Table 2: Perplexity of RTN, GPTQ, PB-LLM, and BiLLM on OPT Family. The columns represent the perplexity results on
Wikitext2 datasets with different model sizes.
Method Block
Size
Weight
Bits 1.3B 2.7B 6.7B 13B 30B 66B
Full Precision - 16.00 14.62 12.47 10.86 10.13 9.56 9.34
RTN - 3.00 13337.38 15594.72 5797.32 3357.01 1566.00 6126.09
GPTQ 128 3.00 20.97 16.88 14.86 11.61 10.27 10.51
RTN - 2.00 11272.65 9505.76 28363.14 194086.78 169616.47 1165864.25
GPTQ 128 2.00 115.17 61.59 50.19 21.36 15.71 82.10
RTN - 1.00 17165.72 36516.69 11550.91 6986.35 6485.99 184796.30
GPTQ 128 1.00 14884.73 14144.58 10622.81 15196.96 12478.37 13106.45
PB-LLM † 128 1.70 265.52 124.35 105.16 81.92 25.14 29.09
BiLLM ‡ 128 1.11 69.97 49.55 35.36 18.82 12.71 12.06
-: Vanilla RTN conducts layer-wise quantization. †: PB-LLM selects 10% elements in the original tensor as salient weights based on
Hessian. ‡: BiLLM uses structural searching for salient weights. The table gives the average bit-width of the OPT family.
GB NVIDIA A100. Given that BiLLM is an efficient PTQ
framework, it eliminates the need for any fine-tuning, allow-
ing for completion through a single quantization process.
Models and Datasets. We facilitate our method on the
OPT (Zhang et al., 2022) and LLaMA (Touvron et al.,
2023a;b) families. Additionally, considering the custom-
ary need for instruction-based fine-tuning of LLMs to adapt
to varying contexts, we also conducted experiments on Vi-
cuna (Chiang et al., 2023). In terms of evaluation metrics,
we mainly focused on the perplexity of LLMs’ outputs,
which is widely acknowledged in prior studies as a challeng-
ing yet stable indicator of LLM capabilities, particularly
apt for network compression (Yao et al.; Frantar et al.,
2022; Frantar & Alistarh, 2023; Xiao et al., 2023). We con-
sider the test of WikiText2 (Merity et al., 2016), PTB (Mar-
cus et al., 1994), as well as a part of the C4 (Raffel et al.,
2020) data. Then, we further conduct the experiments on
seven zero-shot evaluation tasks (PIQA (Bisk et al., 2020),
BoolQ (Clark et al., 2019), OBQA (Mihaylov et al., 2018),
Winogrande (Sakaguchi et al., 2021), ARC-e (Clark et al.,
2018), ARC-c (Clark et al., 2018) Hellaswag (Zellers et al.,
2019)) in the Appendix D, further verifying the robustness
of our proposed BiLLM to the binarization of LLMs.
Baseline. Our primary baseline is PB-LLM (Shang et al.,
2023), the most recent PTQ approach on binary LLMs.
GPTQ (Frantar et al., 2022) and vanilla RTN are also se-
lected. GPTQ is currently the advanced technology in PTQ,
and many works(Lin et al., 2023; Dettmers et al., 2023b;
Shang et al., 2023) choose it as the baseline. Other methods
oriented towards 8-bit and 4-bit quantization are deemed
unsuitable for binarization and were thus not considered.
4.2. Results
Comparison results. We conduct a meticulous compar-
ison of the binary performance of different LLMs across
various model sizes. We deploy the BiLLM on the OPT
models (Zhang et al., 2022) under the condition of a block
size equal to 128. As seen in Table 2, the model outputs
under the RTN and GPTQ methods have already collapsed
at 1-bit weights, whereas BiLLM still maintains reasonable
linguistic output capabilities with an average weight of 1.1
bits. In comparison with PB-LLM at 1.7 bits, our method
achieves a 35% reduction in weight bit-width while enhanc-
ing the performance of different sizes of the OPT model by
49.4% to 77.0%. It is noteworthy that when the parameter
size exceeds 30B, BiLLM can achieve performance nearly
equivalent to that of GPTQ with 3-bit quantization.
Due to the exceptional performance of the LLaMA (Touvron
et al., 2023a;b) series, they have become the foundation for
many open-source models (Chiang et al., 2023). Then, in
Table 3, we evaluate the perplexity of outputs from the
LLaMA series models using different methods. It can be
observed that, even at ultra-low weight bit-width, BiLLM
consistently outperforms the 2-bit RTN and GPTQ methods.
And 1.08 bits BiLLM for LLaMA-65B and LLaMA2-70B
even surpasses the output of the full-precision OPT-66B
model, which demonstrates the further binary potential of
the LLaMA family. We extend perplexity evaluation to the
PTB and C4 datasets. Figure 7 illustrates the performance
of the 7B parameter LLaMA series as well as the 6.7B
OPT models. BiLLM continues to achieve a leading edge in
performance compared to other methods (more additional
comparisons are discussed in Appendix D).
Experiments of instruction-tuned models. Instruction
fine-tuning can significantly improve the application capa-
bilities of the model and has become a necessary process for
LLMs deployment in different scenarios (Wei et al., 2021;
Sanh et al., 2021; Chiang et al., 2023). We also deployed
BiLLM on the recently popular fine-tuning instruction model
Vicuna for benchmark testing. As shown in Table 4, the
7
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
Table 3: Perplexity of RTN, GPTQ, PB-LLM, BiLLM on LLaMA Family. The columns represent the perplexity results on
Wikitext2 datasets with different model sizes.
Model Method Block
Size
Weight
Bits 7B 13B 30B 65B/70B*
Full Precision - 16.00 5.68 5.09 4.10 3.53
RTN - 2.00 106767.34 57409.93 26704.36 19832.87
GPTQ 128 2.00 152.31 20.44 13.01 8.78
LLaMA RTN - 1.00 168388.00 1412020.25 14681.76 65253.24
GPTQ 128 1.00 267001.72 113894.12 67093.73 25082.88
PB-LLM † 128 1.70 102.36 36.60 33.67 12.53
BiLLM ‡ 128 1.09 35.04 15.14 10.52 8.49
Full Precision - 16.00 5.47 4.88 N/A 3.32
RTN - 2.00 17788.93 51145.61 N/A 26066.13
GPTQ 128 2.00 60.45 19.70 N/A 9.12
LLaMA2 RTN - 1.00 157058.34 47902.32 N/A 160389.91
GPTQ 128 1.00 115905.67 9387.80 N/A 74395.42
PB-LLM † 128 1.70 69.20 151.09 N/A 28.37
BiLLM ‡ 128 1.08 32.48 16.77 N/A 8.41
The table gives the average bit-width of the LLaMA family. N/A: LLaMA2 do not have 30B version. *: LLaMA has 65B version and
LLaMA2 has 70B version.
9564.53
43.24
5361.30 80.15
3877.38
40.52
ptb c4
LLaMA-2-7B GPTQ-2bits
PB-LLM-1.7bits
BiLLM-1.1bits
80.43
40.47
193.95
89.8573.63
43.16
ptb c4
OPT-6.7B GPTQ-2bits
PB-LLM-1.7bits
BiLLM-1.1bits2020.51
101.3
891.15
100.38
421.27
39.59
ptb c4
LLaMA-7B GPTQ-2bits
PB-LLM-1.7bits
BiLLM-1.1bits
Figure 7: GTPQ, PB-LLM, BiLLM performed on the PTB and c4 datasets, mainly on LLaMA-7B, LLaMA2-7B, and
OPT-6.7B, and we found that BiLLM performed relatively well.
Table 4: Perplexity of BiLLM on Vicuna-7B and Vicuna-
13B. The columns of different models represent the perplex-
ity results on Wikitext2, PTB, and C4 datasets. The block
size is set to 128.
Model Method Weight
Bits
Wiki
-text2 ↓ PTB ↓ C4 ↓
GPTQ 2.00 109.56 6227.73 64.28
Vicuna-7B PB-LLM 1.70 68.01 477.52 67.23
BiLLM 1.08 33.00 332.17 36.24
GPTQ 2.00 41.75 465.94 40.57
Vicuna-13B PB-LLM 1.70 362.17 772.44 346.16
BiLLM 1.08 36.57 300.31 28.76
perplexity performance of GPTQ and PB-LLM are com-
pared on Vicuna-7B and Vicuna-13B with three evaluations.
BiLLM can achieve better performance at an average weight
bit of 1.08, which further proves that BiLLM’s universal
LLMs binarization potential. We also provide dialogue
examples of binary models in Appeandix F.
1
10
100
1000
10000
100000
1000000
wikitext2 ptb c4
Pe
rp
le
xi
ty

LLaMA-7B
RTN Salient-only
Splitting-only Both-BiLLM
1
10
100
1000
10000
100000
wikitext2 ptb c4
Pe
rp
le
xi
ty

OPT-6.7B
RTN Salient-only
Splitting-only Both-BiLLM
Figure 8: Ablation results of salient-only and splitting-only
methods on OPT and LLaMA.
Zero-Shot results. To conduct a more comprehensive eval-
uation of binary LLMs, we extend our experiments to 7
zero-shot datasets. Appendix D provides detailed results of
our approach compared to previous methods in ultra-low bit
quantization, further showing the outlier of BiLLM.
Ablation results. BiLLM enhances binarization precision
through two primary methods: structured salient binariza-
tion via residual approximation, and non-salient weight bina-
rization via optimal splitting. To examine the effects of these
8
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
Table 5: Model size comparison of LLaMA family.
Method LLaMA-7B LLaMA2-7B LLaMA-13B LLaMA2-13B LLaMA-30B LLaMA-65B LLaMA-70B
FP16 13.5GB 13.5 GB 24.2 GB 25.0 GB 60.5 GB 121.0 GB 129.3 GB
BiLLM 1.5 GB 1.6 GB 2.7 GB 2.8 GB 6.1 GB 14.8 GB 15.4 GB
Table 6: The memory occupancy rate compared with FP16
and the corresponding accuracy on OPT-30B.
Configuration BiLLM PB-LLM (10%) GPTQ
Average bit-width 1.11 1.7 2
Memory Occupancy∗ 9.70% 16.50% 13.30%
PPL on WikiText-2 12.71 25.14 15.71
*: Equation of memory occupancy [R6]:
memory occupancy compared w FP16 = (bi-
nary unsalint weight size + residual binary salint weight size
+ CSR compressed bitmap size + scaling factor size) / float-
ing point weight size.
Table 7: Memory occupancy rate compared with FP16 OPT.
Model BiLLM GPTQ-2bit
OPT-1.3B 9.40% 13.30%
OPT-2.7B 9.50% 13.30%
OPT-6.7B 9.30% 13.30%
OPT-13B 9.70% 13.30%
OPT-30B 9.70% 13.30%
strategies, we conducted decomposition experiments. As
shown in Figure 8, both approaches significantly improve
binary performance. Notably, we found that OPT-6.7B
exhibits greater sensitivity to the splitting of non-salient
weights (the blue line is lower than the green line), whereas
LLaMA-7B is more responsive to salient weights’ residual
approximation (the green line is lower than the blue line).
This further indicates that different LLMs exhibit varying
responses to distinct binarization optimization strategies,
showing that the two binarization strategies proposed by
BiLLM are efficient to various LLMs. We further discuss
details on the block-size ablation results in Appendix E.
Model size. In Table 5, we present the FP16 of models
ranging from LLaMA-7B to 65B and LLaMA2-7B to 70B,
as well as the sizes of binarized models after compression
by BiLLM. Notably, BiLLM achieved close to a tenfold
compression of weights across LLMs of different sizes.
GPU memory. The motivation of our BiLLM is to push
the bit-width compression limit of LLM weights under post-
training conditions, which reduces both the storage and
GPU memory footprint of LLMs and retains their accuracy
to the greatest extent for being practical. Although binarized
GEMM is hard to implement directly due to fine-grained
grouping, the extreme bit-width compression of our BiLLM
brings significant savings in GPU memory requirements
(size and bandwidth), which is considered to be one of
the most significant efficiency bottlenecks of LLM infer-
ence (Gholami et al., 2024; Dettmers et al.; 2023a; Xiao
et al., 2023; Chee et al., 2024; Shang et al., 2023). Here, we
provide detailed memory and performance comparisons to
demonstrate the advantages of BiLLM (as shown in Table
A.1): for the OPT-30B model, BiLLM (1.1-bit) achieves
a 41.57% and 27.07% memory compression improvement
compared to PB-LLM (1.7-bit) and GPTQ (2-bit), respec-
tively, while enhancing accuracy by 49.44% and 19.10%.
We further provide a detailed comparison of memory usage
with the 2-bit GPTQ method under different sizes of LLM
in Table 7. The memory occupancy of our BiLLM is only
about 69.9% of 2-bit quantization, which shows the great
memory-saving benefit of our BiLLM from the extreme
bit-width reduction, and we also achieve higher accuracy
with the significantly saved memory.
5. Conclusions
This work proposed a novel post-training binary quantiza-
tion method named BiLLM, specifically tailored for com-
pressing pre-trained LLMs. Inspired by the characteristics
of weight’s value and Hessian distributions, we adopted a bi-
nary residual approximation for structurally salient weights
to preserve their capabilities at ultra-low bits. For non-
salient weights, we employed optimal segmentation for
grouped binarization. Our results demonstrate that LLMs
can undergo a one-time weight quantization at ultra-low bits
without substantial loss of precision. BiLLM has pioneered
the achievement of LLM performance guarantees at an av-
erage bit rate close to 1 bit. We validated the binarization
performance of BiLLM across multiple open-source LLM
families and conducted generalization tests on a fine-tuned
instruction model. BiLLM advances the bit-width quantiza-
tion frontier of LLMs, promising to facilitate the deployment
of LLMs in edge scenarios and resource-constrained devices,
and encourages further exploration in LLMs compression.
Acknowledgement. This work was supported by
the National Science and Technology Major Project
(2021ZD0110503), the Swiss National Science Foundation
(SNSF) project 200021E 219943 Neuromorphic Attention
Models for Event Data (NAMED), the Baidu Scholarship,
and the National Natural Science Foundation of China (No.
62306025, No. 92367204).
9
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
Impact Statement
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.
References
Bengio, Y., Léonard, N., and Courville, A. Estimating or prop-
agating gradients through stochastic neurons for conditional
computation. arXiv preprint arXiv:1308.3432, 2013.
Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning about
physical commonsense in natural language. In Proceedings of
the AAAI conference on artificial intelligence, volume 34, pp.
7432–7439, 2020.
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D.
Weight uncertainty in neural network. In International confer-
ence on machine learning, pp. 1613–1622. PMLR, 2015.
Chan, C.-Y. and Ioannidis, Y. E. Bitmap index design and evalua-
tion. In Proceedings of the 1998 ACM SIGMOD international
conference on Management of data, pp. 355–366, 1998.
Chee, J., Cai, Y., Kuleshov, V., and De Sa, C. M. Quip: 2-bit quan-
tization of large language models with guarantees. Advances in
Neural Information Processing Systems, 36, 2024.
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng,
L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., et al. Vicuna:
An open-source chatbot impressing gpt-4 with 90%* chatgpt
quality. See https://vicuna. lmsys. org (accessed 14 April 2023),
2023.
Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M.,
and Toutanova, K. Boolq: Exploring the surprising difficulty
of natural yes/no questions. arXiv preprint arXiv:1905.10044,
2019.
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,
Schoenick, C., and Tafjord, O. Think you have solved question
answering? try arc, the ai2 reasoning challenge. arXiv preprint
arXiv:1803.05457, 2018.
Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio,
Y. Binarized neural networks: Training deep neural networks
with weights and activations constrained to+ 1 or-1. arXiv
preprint arXiv:1602.02830, 2016.
Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Llm.
int8 (): 8-bit matrix multiplication for transformers at scale,
2022. CoRR abs/2208.07339.
Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Llm.
int8 (): 8-bit matrix multiplication for transformers at scale.
arXiv preprint arXiv:2208.07339, 2022.
Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L.
Qlora: Efficient finetuning of quantized llms. arXiv preprint
arXiv:2305.14314, 2023a.
Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev,
D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T.,
and Alistarh, D. Spqr: A sparse-quantized representation
for near-lossless llm weight compression. arXiv preprint
arXiv:2306.03078, 2023b.
Dong, Z., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer,
K. Hawq: Hessian aware quantization of neural networks with
mixed-precision. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 293–302, 2019.
Fang, J., Shafiee, A., Abdel-Aziz, H., Thorsley, D., Georgiadis, G.,
and Hassoun, J. H. Post-training piecewise linear quantization
for deep neural networks. In Computer Vision–ECCV 2020:
16th European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part II 16, pp. 69–86. Springer, 2020.
Faraone, J., Fraser, N., Blott, M., and Leong, P. H. Syq: Learning
symmetric quantization for efficient deep neural networks. In
Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 4300–4309, 2018.
Frantar, E. and Alistarh, D. Optimal brain compression: A frame-
work for accurate post-training quantization and pruning. Ad-
vances in Neural Information Processing Systems, 35:4475–
4488, 2022.
Frantar, E. and Alistarh, D. Sparsegpt: Massive language models
can be accurately pruned in one-shot. In International Confer-
ence on Machine Learning, pp. 10323–10337. PMLR, 2023.
Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq:
Accurate post-training quantization for generative pre-trained
transformers. arXiv preprint arXiv:2210.17323, 2022.
Gholami, A., Yao, Z., Kim, S., Hooper, C., Mahoney, M. W., and
Keutzer, K. Ai and memory wall. IEEE Micro, 2024.
Helwegen, K., Widdicombe, J., Geiger, L., Liu, Z., Cheng, K.-T.,
and Nusselder, R. Latent weights do not exist: Rethinking
binarized neural network optimization. Advances in neural
information processing systems, 32, 2019.
Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A.,
Adam, H., and Kalenichenko, D. Quantization and training of
neural networks for efficient integer-arithmetic-only inference.
In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 2704–2713, 2018.
Jain, S., Venkataramani, S., Srinivasan, V., Choi, J., Gopalakrish-
nan, K., and Chang, L. Biscaled-dnn: Quantizing long-tailed
datastructures with two scale factors for deep neural networks.
In Proceedings of the 56th Annual Design Automation Confer-
ence 2019, pp. 1–6, 2019.
LeCun, Y., Denker, J., and Solla, S. Optimal brain damage. Ad-
vances in neural information processing systems, 2, 1989.
Lee, C., Jin, J., Kim, T., Kim, H., and Park, E. Owq: Lessons
learned from activation outliers for weight quantization in large
language models. arXiv preprint arXiv:2306.02272, 2023.
Li, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., Yu,
F., Wang, W., and Gu, S. Brecq: Pushing the limit of post-
training quantization by block reconstruction. arXiv preprint
arXiv:2102.05426, 2021.
Li, Z., Ni, B., Zhang, W., Yang, X., and Gao, W. Performance
guaranteed network acceleration via high-order residual quanti-
zation. In Proceedings of the IEEE international conference on
computer vision, pp. 2584–2592, 2017.
Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han, S. Awq:
Activation-aware weight quantization for llm compression and
acceleration. arXiv preprint arXiv:2306.00978, 2023.
10
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi,
Y., Krishnamoorthi, R., and Chandra, V. Llm-qat: Data-free
quantization aware training for large language models. arXiv
preprint arXiv:2305.17888, 2023.
Marcus, M., Kim, G., Marcinkiewicz, M. A., MacIntyre, R., Bies,
A., Ferguson, M., Katz, K., and Schasberger, B. The penn
treebank: Annotating predicate argument structure. In Hu-
man Language Technology: Proceedings of a Workshop held at
Plainsboro, New Jersey, March 8-11, 1994, 1994.
Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel
mixture models. arXiv preprint arXiv:1609.07843, 2016.
Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit of
armor conduct electricity? a new dataset for open book question
answering. arXiv preprint arXiv:1809.02789, 2018.
Park, E., Yoo, S., and Vajda, P. Value-aware quantization for
training and inference of neural networks. In Proceedings of
the European Conference on Computer Vision (ECCV), pp. 580–
595, 2018.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan,
G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Py-
torch: An imperative style, high-performance deep learning
library. Advances in neural information processing systems, 32,
2019.
Qin, H., Gong, R., Liu, X., Shen, M., Wei, Z., Yu, F., and Song, J.
Forward and backward information retention for accurate binary
neural networks. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pp. 2250–2259,
2020.
Qin, H., Ding, Y., Zhang, M., Yan, Q., Liu, A., Dang, Q., Liu,
Z., and Liu, X. Bibert: Accurate fully binarized bert. arXiv
preprint arXiv:2203.06390, 2022.
Qin, H., Zhang, M., Ding, Y., Li, A., Cai, Z., Liu, Z., Yu, F.,
and Liu, X. Bibench: Benchmarking and analyzing network
binarization. arXiv preprint arXiv:2301.11233, 2023.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,
M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of trans-
fer learning with a unified text-to-text transformer. The Journal
of Machine Learning Research, 21(1):5485–5551, 2020.
Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A. Xnor-
net: Imagenet classification using binary convolutional neural
networks. In European conference on computer vision, pp.
525–542. Springer, 2016.
Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Wino-
grande: An adversarial winograd schema challenge at scale.
Communications of the ACM, 64(9):99–106, 2021.
Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L.,
Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A.,
et al. Multitask prompted training enables zero-shot task gener-
alization. arXiv preprint arXiv:2110.08207, 2021.
Shang, Y., Yuan, Z., Wu, Q., and Dong, Z. Pb-llm: Partially bina-
rized large language models. arXiv preprint arXiv:2310.00034,
2023.
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A.,
Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.
Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971, 2023a.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A.,
Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S.,
et al. Llama 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288, 2023b.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,
Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all
you need. Advances in neural information processing systems,
30, 2017.
Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L., Yang, F.,
Wang, R., Wu, Y., and Wei, F. Bitnet: Scaling 1-bit transformers
for large language models. arXiv preprint arXiv:2310.11453,
2023.
Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B.,
Du, N., Dai, A. M., and Le, Q. V. Finetuned language models
are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi,
A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Hug-
gingface’s transformers: State-of-the-art natural language pro-
cessing. arXiv preprint arXiv:1910.03771, 2019.
Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S.
Smoothquant: Accurate and efficient post-training quantization
for large language models. In International Conference on
Machine Learning, pp. 38087–38099. PMLR, 2023.
Yao, Z., Aminabadi, R., Zhang, M., Wu, X., Li, C., and
He, Y. Z. Efficient and affordable post-training quantiza-
tion for large-scale transformers, 2022. URL https://arxiv.
org/abs/2206.01861.
Yao, Z., Li, C., Wu, X., Youn, S., and He, Y. A comprehensive
study on post-training quantization for large language models.
arXiv preprint arXiv:2303.08302, 2023.
You, Y. Audio coding: theory and applications. Springer Science
& Business Media, 2010.
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.
Hellaswag: Can a machine really finish your sentence? arXiv
preprint arXiv:1905.07830, 2019.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen,
S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt:
Open pre-trained transformer language models. arXiv preprint
arXiv:2205.01068, 2022.
Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., and Zou, Y. Dorefa-
net: Training low bitwidth convolutional neural networks with
low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.
Zhu, X., Li, J., Liu, Y., Ma, C., and Wang, W. A survey on
model compression for large language models. arXiv preprint
arXiv:2308.07633, 2023.
11
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
A. BiLLM Implementation
Algorithm 2 BiLLM: Detailed functions process
func salient (W,Hc)
1: S := W2/[Hc
b:b+βb:b+β ]
2 // salient matrix
2: rows{·} := topk(sum(abs(S)).(dim = 0))
3: e = inf // searching error
4: n∗ = 0 // optimal number of salient columns
5: for i = 1, 2, ..., len(rows) do
6: B1 := binary(W:,j, j∈rows[:i])
7: B2 := binary(W:,j, j /∈rows[:i])
8: if ||W − (B1 ∪B2)||2 < e then
9: e := ||W − (B1 ∪B2)||2
10: n∗ := i
11: end if
12: end for
13: return rows{: n∗}
func binary (W)
1: α :=
||W||ℓ1
m
2: B := α · sign(W)
3: return B
func res approximation (W)
1: B1 := binary(W)
2: R := W −B1
3: B2 := binary(R)
4: B := B1 +B2
5: return B
func seg search (W)
1: e = inf // searching error
2: p∗ = 0 // optimal break-point
3: for i = 0.1, 0.2, 0.3, ..., 9 do
4: p := i ·max(abs(W))
5: B1 := binary(W|wi,j |≤p)
6: B2 := binary(W|wi,j |>p)
7: if ||W − (B1 +B2)||2 < e then
8: e := ||W − (B1 +B2)||2
9: p∗ := p
10: end if
11: end for
12: return p∗
BiLLM necessitates the structured selection of salient rows and their subsequent quantization through residual approximation
binarization. This is followed by dividing the non-salient weights, which exhibit a bell-shaped distribution, into a sparse area
and a concentrated area. The division requires the optimization of the segmentation point p∗ by minimizing quantization
loss. Ultimately, the two regions of non-salient weights are binarized separately to derive the final binary weights for LLMs.
The implementation details of the aforementioned function are enumerated in Algorithm 2.
B. Quantization Error
Quantization error definition for weight distribution The numerical range covered by the uniform quantizer spans from
[Xmin, Xmax]. The number of intervals post-quantization, denoted as M , typically equals 2b, where b represents the target
bit-width of quantization. So the quantization step size is:
∆ =
Xmax −Xmin
M
(15)
The boundaries can be calculated as:
bq = Xmin +∆ · l (16)
where l ∈ 0, 1, ...,M , and we have bq ∈ {−α, 0, α} under binarization. Then we give the mean of each interval:
xq = Xmin +∆ · l − 0.5∆ (17)
where l ∈ 1, ...,M . In this quantization scheme, we can get the MSQE from (You, 2010):
θ2 =
M∑
l=1
∫ Xmin+∆·l
Xmin+∆·(l−1)
(Xmin +∆ · l − 0.5∆− x)2g(x)dx (18)
then we let the y to replace the Xmin +∆ · l − 0.5∆− x part, so the Equation (18) becomes:
θ2 =
M∑
l=1
∫ 0.5∆
−0.5∆
y2f [Xmin +∆ · l − (y + 0.5∆)]2dx (19)
12
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
consider the Equation (16) and Equation (17), the above equation becomes:
θ2 =
M∑
l=1
∫ 0.5∆
−0.5∆
x2f(xp − x)dx (20)
The aforementioned reasoning indicates that the MSQE of a uniform quantizer depends on the PDF and the quantization
bit-width. Due to previous observations of the weights in pretrained LLMs, we have eliminated the salient weights. The
remaining distribution of non-salient weights’ g(x), is not uniform and resembles a Gaussian distribution. In binarization,
therefore, we substitute α into Equation (18), resulting in:
θ2 =
M∑
l=1
∫ (l−0.5M)∆
(l−1−0.5M)∆
[(l − 0.5− 0.5M)∆− x]2g(x)dx
=
∫ 0
Xmin
(−α− x)2g(x)dx+
∫ Xmax
0
(α− x)2g(x)dx (21)
C. Searching Curve of Salient Column and Non-salient Distribution
Q K V Out FC1 FC2
Figure 9: Block-wise searching curve of salient columns in OPT-6.7B. The majority of the curves indicate that the minimal
quantization error can be achieved at the block level by considering only a few columns as salient. The Out Projection layer
has a larger number of salient columns, hence varying coverage for each block. The distribution in the FC layer is more
dispersed. After optimal searching, the overall average weight bit is merely 1.1 bits.
We implemented a column-level segmentation and formulated a minimal-error column number search, as delineated in
Equation (5). The identification of the optimal count of salient column groups commences with the column exhibiting the
highest salience. To mitigate the increase in bit-width resulting from residual approximation, we confined the search range
to between 3 to 30 columns. Figure 9 illustrates the search curve pertinent to the inaugural Transformer block within the
OPT6.7B model. It includes six layers of operators (Q, K, V, Out Projection, FC1, and FC2), with each layer showing
the search curves for the first five blocks. Figure 15 elucidates the clustering of salient weights, suggesting that a majority
of the layers and blocks are capable of attaining minimal quantization errors with a limited number of salient columns.
13
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
The block-wise changes in weight distribution brought about by OBC (Frantar & Alistarh, 2022) introduce fluctuations
in the search curve; however, the structured selection still manages to encompass the majority of salient weights. In the
Feedforward layer, where salient weight distribution is more scattered, the search curve leans towards employing residual
approximation across an increased number of columns. Nonetheless, Table 1, displaying the average weight bit numbers
across various LLMs, confirms that this search strategy effectively maintains weight compression at approximately 1.1 bits.
Figure 10 shows the unstructured search curve for the non-salient weights in the OPT6.7B model, with the same composition
as that in Figure 9. The horizontal axis represents the ratio between p and the maximum weight value. Despite searching
on a block-wise basis, the search curve still exhibits convex properties, indicating the presence of an optimal p∗. This
phenomenon demonstrates that the non-salient weights exhibit characteristics closely resembling an ideal Gaussian or
Laplacian distribution (You, 2010; Fang et al., 2020).
Q K V Out FC1 FC2
Figure 10: Block-wise splitting curve of bell-shaped distribution in OPT6.7B. The overall presentation exhibits the
characteristics of a convex function, fundamentally aligning with the theoretical optimal point in terms of theoretical basis.
D. Multi-evaluation Comparisons
Perplexity results on PTB and C4.
We use tables in the main text to show the perplexity of the three methods GPTQ, PB-LLM, and BiLLM on the Wikitext2
dataset, and bar charts to show the perplexity results for LLaMA-7B, LLaMA2-7B, and OPT-6.7B on the PTB and C4
datasets. In the appendix, we show the quantitative comparison results for models of other sizes on the PTB and C4 datasets
with more images.
In Figure 11, we find that although different models have different perplexity results, they still roughly follow the law that
the larger the model, the lower the perplexity. BiLLM is generally still relatively better than the GPTQ and PB-LLM results
in terms of perplexity with a lower bit-width configuration, while PB-LLM and GPTQ are higher or lower than each other,
with slightly inferior results at very low bits.
Zero-shot results
For completeness of testing, we have also tested and compared metrics such as the accuracy of GPTQ, PB-LLM, and BiLLM
on datasets such as PIQA and BoolQ, all using Zero Shot’s experimental setup. From Table 8, We find that despite the loss
14
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
83.23
15.38
141.09
35.8547.54
12.26
ptb c4
LLaMA-30B GPTQ-2bits
PB-LLM-1.7bits
BiLLM-1.1bits
20.86
15.13
35.05
25.29
21.24
16.15
ptb c4
OPT-30B GPTQ-2bits
PB-LLM-1.7bits
BiLLM-1.1bits
163.72
69.07
278.52
180.05
103.82
66.51
ptb c4
OPT-1.3B GPTQ-2bits
PB-LLM-1.7bits
BiLLM-1.1bits
87.22
57.75
143.93
96.13
76.99
45.82
ptb c4
OPT-2.7B GPTQ-2bits
PB-LLM-1.7bits
BiLLM-1.1bits
63.9
11.56
66.8
16.92
46.99
11.12
ptb c4
LLaMA-65B GPTQ-2bits
PB-LLM-1.7bits
BiLLM-1.1bits 109.49
61.01
45.24 34.98
18.57 14.13
ptb c4
OPT-66B GPTQ-2bits
PB-LLM-1.7bits
BiLLM-1.1bits
470.34
29.47
719.25
144.59
332.09
27.54
ptb c4
LLaMA-2-13B GPTQ-2bits
PB-LLM-1.7bits
BiLLM-1.1bits
27.16 18.83
110.47
53.26
26.98 19.89
ptb c4
OPT-13B GPTQ-2bits
PB-LLM-1.7bits
BiLLM-1.1bits196.7
21.2
213.17
39.51
85.32
16.93
ptb c4
LLaMA-13B GPTQ-2bits
PB-LLM-1.7bits
BiLLM-1.1bits
Figure 11: GPTQ, PB-LLM, BiLLM performed on the PTB and C4 datasets, mainly on LLaMA-13B, LLaMA2-13B,
OPT-13B, and so on. The results showed that BiLLM performed relatively well.
in quantification, a side-by-side comparison between the three methods still shows BiLLM to be superior overall, testing one
level higher on some datasets, while the effect of some random perturbations, although present, does not pull down BiLLM’s
performance across the board. This suggests that BiLLM’s quantization results have significantly improved performance at
very low bits, and further validates the conclusions.
Table 8: Accuracy on 7 data sets, from binarization LLaMA, LLaMA2, and OPT, and we also compare the results among
GPTQ, PB-LLM, and BiLLM to validate the quantization effect.
Model Method Weight
Bits
Block
Size PIQA ↑ BoolQ ↑ OBQA ↑ Winogrande ↑ ARC-e ↑ ARC-c ↑ Hellaswag ↑
GPTQ 2.00 128 52.8 50.0 28.2 49.3 26.6 29.5 26.3
LLaMA-7B PB-LLM 1.70 128 54.6 59.7 30.4 50.6 28.2 24.6 28.7
BiLLM 1.09 128 61.2 62.7 31.8 51.1 36.0 25.7 36.8
GPTQ 2.00 128 51.1 43.9 29.0 50.8 26.6 28.5 26.3
LLaMA2-7B PB-LLM 1.70 128 53.8 62.3 30.2 49.3 28.0 25.0 27.7
BiLLM 1.08 128 60.6 61.8 33.2 52.4 36.2 24.4 34.8
GPTQ 2.00 128 56.6 51.1 25.6 51.2 31.3 22.9 30.4
OPT-6.7B PB-LLM 1.70 128 57.6 55.5 24.2 47.7 33.2 21.0 31.0
BiLLM 1.11 128 58.6 62.2 29.0 51.5 34.1 23.9 31.9
E. Ablation of BiLLM with different block size
To explore the effect of different chunk sizes on the quantization effect of BiLLM, we set up block size settings including 32
columns and 64 columns up to 512 columns and performed quantization experiments on them. The results show that the
overall perplexity is lower as the chunk granularity becomes finer and the number of bits used becomes relatively smaller.
15
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
We believe this is because the smaller the chunks, the finer the data representation, and the more scale is used, but increasing
the diversity of quantization results also increases the weighting overhead. A block size of 128 can better balance the
bit-width and quantization effect.
Table 9: Perplexity on Wikitext2, PTB, and C4 with different block size settings on BiLLM.
Model Block Size Wikitext2 PTB C4
512 74.14 1078.90 81.76
256 48.91 574.34 57.60
LLaMA-7B 128 35.04 421.27 39.59
64 27.23 399.81 27.74
32 17.56 263.39 19.85
512 52.90 267.82 43.86
256 43.69 232.34 43.21
LLaMA2-7B 128 32.48 3877.38 40.52
64 20.12 830.36 24.46
32 13.58 440.40 17.34
512 151.81 257.22 101.96
256 84.42 116.44 77.25
OPT-6.7B 128 35.36 73.63 43.16
64 33.36 48.16 31.94
32 20.48 31.02 21.47
F. Dialog Examples
In this section, we show some dialogue examples of binarized LLaMA-13B and Vicuna-13B.
G. Magnitude and Hessian Distribution of LLMs
Figure 2 displays the distribution characteristics of weights and Hessian in LLMs. In this section, we provide additional
examples to illustrate the bell-shaped distribution of weight values and the long-tailed distribution of Hessian weights.
Figure 13 depicts the distributions of four linear layers in the first Transformer block of the OPT-1.3B model, while Figure 14
shows the distributions of seven linear layers in the sixth block of the LLaMA-7B model. The selection of these specific
block positions is intended to demonstrate the universality of these distribution characteristics in LLMs.
Figure 15 displays the distribution of sensitive weights across 5 Transformer blocks within the OPT-1.3B model. We present
the Hessian distribution results for both the attention and feedforward blocks, with the red portion indicating the top 10% of
the most significant weight distribution. We observed that the salient weights of Q, K, and V in the OPT family tend to
concentrate in some columns or rows. Moreover, we noticed that salient weights in the Out Projection layer of multi-head
self-attention blocks are distinctly concentrated in specific columns, supporting our structured selection approach discussed
in the main text. In contrast, the distribution of salient weights in the feedforward layers is more dispersed. Based on these
observations, we adopt a sensitivity-based structured search method to identify salient columns.
16
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
Alaska is a state of ___Prompt:
mind & body aided with other person.PB-LLM-1.7bit:
the US, it is situated in North America.BiLLM-1.1bit:
Yangtze and Yellow River locate in ___Prompt:
the lower land.PB-LLM-1.7bit:
the country which is called as the country of the China.BiLLM-1.1bit:
After climbing thirty flights of stairs, he was drenched in
sweat and didn't feel much like climbing. I advised him ___
Prompt:
he is really not very health.PB-LLM-1.7bit:
to get used now and the next day. After 2 days, he'd do a lot
better and his sweat had lower.
BiLLM-1.1bit:
LLaMA-13B
What are the capitals of China, US and Russia?Prompt:
A’s, U’sa and U's were. 2, U’s with, A’s, U’s, U's: W, S’A
and U’ with, U’s, or US's, or U's: U's S – with
PB-LLM-1.7bit:
The capital of China is Beijing, the capital of US is
Washington, DC, and the capital of Russia is Moscow.
BiLLM-1.1bit:
I have a friend who lives in New York and I would like to
visit him. How would I go there from Paris?
Prompt:
I would like it to go to Paris.PB-LLM-1.7bit:
You can fly from Paris to New York, and there are many
airlines that can take you there. You can also take a flight
to the New York, but you will have to check the schedule, as
it will depend.
BiLLM-1.1bit:
What should I do after failed in a math exam?Prompt:
I failed to fail.PB-LLM-1.7bit:
Retake math exams if you don’t pass this year. Take more
time to study and work on your weak areas. Or you
need more support for the final, You can also seek outside
help in the same subject.
BiLLM-1.1bit:
Vicuna-13B
Figure 12: Some examples of conversations. LLaMA-13B and Vicuna-13B are chosen to show the case of language
supplementary and Q&A ability. And PB-LLM (int 8, 10%) is selected as the comparison. We color the text to show the
reasonable or inappropriate responses.
17
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
Figure 13: Different layers weight density distribution (blue) and hessian density distribution (orange) of the 1st Transformer
block of the OPT-1.3B model
18
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
Figure 14: Different layers weight density distribution (blue) and hessian density distribution (orange) of the 6th Transformer
block of the LLaMA-7B model
19
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
Figure 15: Distribution of top 10% salient elements in Hessian matrix. The distribution of 1st − 5th Transformer blocks in
OPT-1.3B
20
第3个文件的内容为：
BiMLP: Compact Binary Architectures for Vision
Multi-Layer Perceptrons
Yixing Xu, Xinghao Chen, Yunhe Wang
Huawei Noah’s Ark Lab
{yixing.xu, xinghao.chen, yunhe.wang}@huawei.com
Abstract
This paper studies the problem of designing compact binary architectures for
vision multi-layer perceptrons (MLPs). We provide extensive analysis on the
difficulty of binarizing vision MLPs and find that previous binarization methods
perform poorly due to limited capacity of binary MLPs. In contrast with the
traditional CNNs that utilizing convolutional operations with large kernel size,
fully-connected (FC) layers in MLPs can be treated as convolutional layers with
kernel size 1× 1. Thus, the representation ability of the FC layers will be limited
when being binarized, and places restrictions on the capability of spatial mixing and
channel mixing on the intermediate features. To this end, we propose to improve
the performance of binary MLP (BiMLP) model by enriching the representation
ability of binary FC layers. We design a novel binary block that contains multiple
branches to merge a series of outputs from the same stage, and also a universal
shortcut connection that encourages the information flow from the previous stage.
The downsampling layers are also carefully designed to reduce the computational
complexity while maintaining the classification performance. Experimental results
on benchmark dataset ImageNet-1k demonstrate the effectiveness of the proposed
BiMLP models, which achieve state-of-the-art accuracy compared to prior binary
CNNs. The MindSpore code is available at https://gitee.com/mindspore/
models/tree/master/research/cv/BiMLP.
1 Introduction
Recent years have witness the boosting of convolutional neural networks (CNNs) on several computer
vision (CV) applications, e.g., image recognition [19, 38, 14, 9, 11], object detection [35, 34],
semantic segmentation [32] and low-level vision [21]. However, the success of CNN models highly
depends on their huge computational cost and massive parameters, which are not suitable to be
directly applied to edge devices that have limited computational resources such as mobile phones,
smart watches.
There are several model compression and acceleration methods to reduce the number of parameters
and FLOPs of the original CNNs and derive a portable model. For instance, knowledge distillation [16,
48, 49] aims to train a small student network with the help of a cumbersome teacher network. Filter
pruning [29, 15, 42, 41] methods sort the weights of the network based on their importance and throw
away those who have negligible influence on the final performance. Model quantization [4, 54, 31]
methods reduce the original 32-bit floating point weights and activations into lower bits and tensor
decomposition [1, 46] methods express a large weight tensor as a sequence of elementary operations
on a series of simpler tensors. Among them, binary CNN [53, 33, 12, 45] is an extreme case of model
quantization that uses only 1-bit for weights and activations. Compared to the original convolutional
operation, binarized one has 64× less FLOPs and 32× less memory consumption.
Preprint. Under review.
ar
X
iv
:2
21
2.
14
15
8v
1
[
cs
.C
V
]
2
9
D
ec
2
02
2
Note that all of the existing binarization methods are applied on the convolutional operations in CNN
models. However, multi-layer perceptron (MLP) model also shows its potential on CV tasks [43, 3, 10]
and matrix multiplication is computational friendly to GPUs and has advantage on inference time
in reality. Compared to traditional CNNs that utilize convolutional operation with various kernel
sizes, FC layers in MLP models can be treated as convolutions with kernel size 1× 1, which suffer
more from limited representation ability compared to the convolutional operations with larger kernel
size when being binarized. In fact, directly binarizing MLP models with the existing methods
show more severe performance degradation compared to the CNN models. For example, the Top-1
classification accuracy on ImageNet will drop by more than 23% when binarizing CycleMLP [3] and
Wave-MLP [40] with the method proposed in Dorefa-Net [53] while the performance drop is only
17% for ResNet-18 [14] using 3 × 3 kernel size for most of the convolutional layers and 13% for
AlexNet [19] with larger kernel size (11× 11 and 5× 5).
100 200 300 400 500 600
OPs (M)
55.0
57.5
60.0
62.5
65.0
67.5
70.0
72.5
75.0
To
p-
1
Ac
cu
ra
cy
(%
) MeliusNet
ResNet-34
ResNet-18
ReActNet
BiMLP (Ours)
RBNN
IR-Net
BiMLP (Ours)
MeliusNet
RBNN
IR-Net
BiRealNet
ReAct-ResNet
ReActNet
Real2Bin
Figure 1: Accuracy vs. OPs of different binary models
on ImageNet-1k dataset.
Thus, in order to alleviate the problem men-
tioned above, we introduce a novel binary
block for MLP model that contains mul-
tiple branches to merge a series of out-
puts from the same stage of the network.
Besides, a universal shortcut connection
(Uni-shortcut) is designed to better trans-
fer the knowledge from previous layers to
the current layer. Compared to the origi-
nal shortcut connection, the proposed Uni-
shortcut can merge two features with dif-
ferent shapes. By fusing the outputs of
multiple layers derived from the same or
different stages of the network, we show
that the representation power of the binary
MLP is drastically increased. The down-
sampling layers are also modified to reduce
the computational complexity while main-
taining the classification performance. Ex-
perimental results on ImageNet-1k dataset demonstrate that by using the proposed method, we
achieve state-of-the-art performance compared to other binary CNN models, as shown in Fig. 1.
We summarize our contributions for learning 1-bit vision MLPs as follows:
• This paper points out the main difficulty of binarizing MLP models is that the representation
ability of FC layers is worse than the convolutional layers in CNN models with kernel size
larger than 1× 1.
• We introduce a mulit-branch binary MLP block (MBB block) with Uni-shortcut operation
to unlock the representation power of binary MLP models and also modify the architecture
of downsampling layer to reduce the computational complexity.
• The experimental results on ImageNet-1k dataset show that the proposed BiMLP improves
the top-1 accuracy by 1.3% with 12.1% less OPs compared to the state-of-the-art ReActNet,
which indicates the effectiveness of the proposed BiMLP models.
2 Preliminaries
2.1 Binarization in CNN models
Given the weights W ∈ Rco×ci×k×k where ci and co are the number of input and output channels
and k is the kernel size, and also the activation A ∈ Rci×h×w where h and w are the height and
width of the input feature, a common way of binarizing the weights and activations in CNN models is
to apply sign function on the 32-bit floating point inputs, i.e.,
Wb = sign(W ), Ab = sign(A), (1)
in which sign(·) is an element-wise operation that outputs +1 when the input is positive and -1
otherwise. Given the binarized weights and activations, the convolutional operation can be realized
2
with binary operations only, i.e.:
Y =Wb Ab, (2)
in which  represents binary convolutional operation that can be implemented with XNOR and
POPCOUNT.
Note that the gradient of aforementioned sign function is zero almost everywhere and backward
propagation is unable to be applied during training. Thus, the back-propagation of sign function
follows the straight through estimator (STE) rule:
∂L
∂W
= Clip(
∂L
∂Wb
,−1, 1), (3)
where L is the loss function corresponding to the specific task, e.g., cross-entropy loss for image
classification. The Clip(·,−1, 1) function clips the elements of the gradient into range [−1, 1].
Most of the existing binary CNN methods focus on improving the forward sign function and backward
STE rule. For example, XNOR-Net [33] introduces channel-wise scale factors on the pure sign
function and improves the performance of binary CNN. Dorefa-Net [53] uses only a single scale
factor to achieve similar performance. ReActNet [26] proposes RSign and RPreLU function for
binary CNNs. DSQ [8] replaces the STE rule and uses the gradient of tanh function to replace
the gradient of sign function while BNN+ [6] introduces SignSwish function. Methods mentioned
above are all applied on CNN models and directly transfer them to MLP models results in severe
performance degradation.
2.2 Vision MLP Models
Vision MLP models take patches of image (tokens) as input, and stack a sequence of channel-FC
layers and spatial-FC layers for image recognition. Specifically, given the input X ∈ Rn×d in which
n is the number of tokens and d is the number of channels in each token, the channel-FC layer fuses
the channels in a single token and generate d′ channels, i.e.,
channel-FC(X,Wc) = X ·Wc, (4)
where Wc ∈ Rd×d′ is the parameter matrix of the channel-FC layer, and · represents the matrix
multiplication operation. Channel-FC layer only aggregates information from different input channels,
while lacking the communications between tokens. Thus, the spatial-FC layer is also introduced in
MLP models [43], i.e.,
spatial-FC(X,Ws) =W>s ·X, (5)
where Ws ∈ Rn×n′
is the parameter of spatial-FC layer. In general, MLP models usually set d = d′
and n = n′ to pursuit efficiency on both feature representation and computation of the entire network.
The spatial-FC layer mentioned above cannot deal with images with diverse shapes, thus is unable to
be applied on downstream tasks such as image detection and image segmentation. In order to solve the
problem, Cycle-MLP [3] retains the position of each token and introduces Cycle-FC layer to enlarge
the receptive field of MLP to cope with downstream tasks while maintaining the computational
complexity. Specifically, given the input feature Z ∈ RH×W×Cin , the output of Cycle-FC layer is:
Cycle-FC(Z)i,j,: =
Cin∑
c=0
Zi+δi(c),j+δj(c),c ·W
cycle
c,: ; (6)
in which W cycle ∈ RCin×Cout is the parameter matrix of the Cycle-FC layer, and δi(c) and δj(c) are
defined as:
δi(c) = (c mod SH)− 1, δj(c) = (b c
SH
c mod SW )− 1, (7)
where SH and SW are predefined receptive fields. Besides Cycle-MLP, Wave-MLP [40] represents
token as a wave function with amplitude and phase and proposes Wave-FC layer to solve the above
problem. AS-MLP [22] avoids the restriction of fixed input size by axially shifting channels of the
feature map, and is able to obtain information flow from different axial directions.
Compared to the convolutional operation in CNN and self-attention operation in Transformer, matrix
multiplication in MLP is computational friendly to GPUs and has specific advantage on inference
time in reality. Benefits from its simple architecture, MLP models can be generalized to various
hardware. However, the cumbersome MLP model with a large number of parameters and FLOPs
limits its ability to apply to portable devices, which means it is essential to derive a compact MLP
model, e.g. binary vision MLPs.
3
3 Binary Vision MLPs
In this section, we first analyze the difficulty of binarizing vision MLP through comparing the
representation ability of binary features in FC layers and convolutional layers. We then propose a
novel binary architecture for vision MLP, which fuses the outputs from the same layer with multi-
branch MLP block and those from different layers with a universal shortcut connection to strengthen
the representation ability of MLP.
3.1 Difficulty of Binarizing Vision MLP
Given the input feature F ∈ RCin×H×W and the convolutional kernel K ∈ RCout×Cin×Kh×Kw
in which Kh and Kw are the height and width of the kernel, respectively. The computational
complexity of the convolutional operation is:
O(Cin × Cout ×Kh ×Kw ×H ×W ). (8)
After binarizing the weights and activations, the elements of the output feature map Y is derived from
the following equation:
Y (o, i, j) =
Cin∑
c=1
bKw
2 c∑
dx=−bKw
2 c
bKh
2 c∑
dy=−b
Kh
2 c
F b(c, i+ dy, j + dx)×Kb(o, c, dy +
Kh
2
, dx +
Kw
2
), (9)
in which Kb and F b are binarized kernel and input feature using Eq. 1 whose elements are selected
from {+1,−1}. According to Eq. 9, we can easily conclude that each element in Y is chosen from
N+1 different values from the set Sb = {−N,−N+2, ..., N−2, N}, in whichN = Cin ·Kh ·Kw.
Specifically, we denote the number N as the representation ability of the binary FC layer.
In traditional CNN models such as ResNet [14] and VGGNet [38], convolutional operation with
kernel size 3× 3 occupies the network. Efficient-Net [39] uses convolution with kernel size 3× 3
and 5× 5, while the recently proposed RepLKNet [7] expands the kernel size to 31× 31. Different
from the CNN models mentioned above, FC layers in MLP model can be treated as convolutional
operation with kernel size 1× 1.
Therefore, compared to the convolutional layer with kernel size k×k, FC layer with the same number
of input channels and output channels has 1/k2 computational complexity and 1/k2 representation
ability after being binarized. Note that assuming CNN models and MLP models have the same
number of input channels and output channels is reasonable. For example, Wave-MLP-S [40] with
30M parameters and 4.5G FLOPs has four stages with base channel 64-128-320-512, while ResNet-
50 with 25.5M parameters and 4.1G FLOPs has four stages with base channel 64-128-256-512
which are roughly the same.
In order to achieve the same representation ability as convolutional layer, the number of input channels
of FC layer must be multiplied by k2. Similarly, the number of output channels should also be scaled
up in order to maintain the representation ability of the next FC layer. Hence, the computational
complexity is k2 times compared to the convolutional layer with kernel size k × k, which drastically
reduce the advantage of binary neural network. To this end, we design a novel binary architecture
for vision models (BiMLP) to enhance the representation ability while maintaining compact model
complexity. A series of specific design are proposed to achieve this goal, which will be elaborated in
the following sections.
3.2 BiMLP Architecture
To deal with the above problem, we introduce a mulit-branch binary MLP block (MBB block) with
Uni-shortcut operation to enhance the representation capacity of binary MLP models. We also design
a novel architecture of downsampling layer for binary MLPs to further reduce the computational
complexity while maintaining the accuracy.
Multi-branch binary MLP block. To enhance the representation ability of binary vision MLPs
with compact complexity, we propose a new binary MLP block containing multiple branches. Note
that in MLP-Mixer [43], ResMLP [44] and gMLP [25], an MLP block mainly consists of two parts
4
Channel Proj
Spatial Proj
FC (dim, 4 × dim)
GeLU
FC (dim, 4×dim)
1
x1
F
C
1
x7
P
A
TM
7
x1
P
A
TM
LayerNorm
LayerNorm
Patch Embeddings
Outputs
BatchNorm
MBB Block 1
BatchNorm
Patch Embeddings
Outputs
C
h
an
n
el
B
i-
FC
Sp
at
ia
l B
i-
M
LP
Sp
at
ia
l B
i-
M
LP
MBB Block 2
C
h
an
n
el
B
i-
M
LP
Sp
at
ia
l B
i-
FC
Sp
at
ia
l B
i-
FC
Spatial Bi-MLP
……
Spatial Bi-FC
Local Bi-FC
BatchNorm
RPReLU
Uni-Shortcut
Channel Bi-FC
Channel Bi-MLP
Channel Bi-FC
Channel Bi-FC
(a) Vision MLP (b) BiMLP (Ours)
e.g., Wave-MLP Global Bi-FC
BatchNorm
RPReLU
PATM (Wave-MLP)
Uni-Shortcut
CycleFC (CycleMLP)
Figure 2: (a) Details of vision MLP blocks, e.g., Wave-MLP [40]. (b) The proposed Multi-branch
binary MLP (MBB) blocks for BiMLP.
including the spatial projection part and the channel projection part. For CycleMLP [3] and Wave-
MLP [40], the spatial projection is achieved by introducing some local FC operations (e.g. Cycle-FC,
Wave-FC) which are applied on the adjacent tokens to cope with input images with different shapes.
Meanwhile, the global FC layer (ordinary FC layer) is used for channel mixing. Different from them,
we simultaneously introduce spatial projection and channel projection into a single part, and form
two different multi-branch binary MLP blocks (MBB blocks) as shown in Fig. 2.
There are four different elements in the MBB blocks, i.e., the Spatial Binary FC, the Spatial Binary
MLP, the Channel Binary FC and the Channel Binary MLP. Given the binarized input feature Xb, the
output of Spatial Binary FC is:
YSB_FC = RPReLU(LFC(BN(Xb)) + U(Xb)), (10)
in which RPReLU(·) is the activation function introduced in ReActNet [26]. LFC(·) is the local FC
that fuses the spatial information which is realized with different form in previous works. BN(·) is the
ordinary batch normalization, and U(·) is the proposed Uni-shortcut which will be introduced below
in Eq. 12. For Spatial Binary MLP, we utilize the PATM architecture introduced in Wave-MLP [40].
Other forms such as CycleMLP architecture in [3], and axially shifting architecture in AS-MLP [22]
can be used as a replacement.
The Channel Binary FC is defined as:
YCB_FC = RPReLU(GFC(BN(Xb)) + U(Xb)), (11)
in which GFC(·) is the original global FC layer that mixes the information between different channels.
Finally, the Channel Binary MLP is the stack of multiple Channel Binary FCs to strengthen the ability
of channel mixing.
Given the four different elements introduced above, the first MBB block uses two Spatial Binary
MLPs focusing on the height and width of the spatial dimension, and a Channel Binary FC. The
second MBB block uses two Spatial Binary FC and a Channel Binary MLP. In this way, the channel
projection and spatial projection are used multiple times, while at the same time the first block has a
stronger ability for spatial mixing and the second block is good at mixing the channel information.
Finally, all the layer normalizations are replaced with batch normalizations, as shown in the ablation
study in Tab. 4.
With the above architectures, the representation ability can be recovered with less computational
complexity by using multiple branches compared with directly expanding the input and output chan-
nels. Generally, k2 branches are needed to get the same representation ability, and the computational
complexity is the same compared to the convolutional layer with kernel size k × k. Note that as
the number of branches increases, both the representation ability and the computational complexity
5
3 × 3 conv
stride=2
𝐻 ×𝑊 × 𝐶𝑖𝑛
𝐻
2
×
𝑊
2
× 𝐶𝑜𝑢𝑡
1 × 1 conv
stride=1
2 × 2maxpool
stride=2
3 × 3maxpool
stride=2
5 × 5maxpool
stride=2
7 × 7maxpool
stride=2
𝐻 ×𝑊 × 𝐶𝑖𝑛
𝐻
2
×
𝑊
2
× 𝐶𝑜𝑢𝑡
𝐻 ×𝑊 × 𝐶𝑜𝑢𝑡
Figure 3: Left: The original downsampling layer used in 32-bit floating point MLP models. Right:
The proposed downsampling block for BiMLP.
will increase. Thus, we seek for a balance between the effectiveness and the efficiency. In the
following experiments, we use only 3 branches as mentioned above and show that it is enough to
obtain competitive results in Tab. 3.
Uni-shortcut. Recall that the proposed MBB blocks fuse the information from the same layer. The
previous works of binary CNNs show that combining the outputs from different layers is important
for the information flow. However, the number of output channels and input channels are usually
different in MLP models and the original shortcut connection cannot be directly applied. Since
the input channels cin are usually multiple times of the output channels cout (or vise versa), i.e.
cin = ncout (or cout = ncin), n ∈ N+. Therefore, we propose a universal shortcut (Uni-shortcut) to
cope with the following two different situations.
Given the input featureXb ∈ RCin×H×W and a FC layer with cin and cout input and output channels,
we have:
U(Xb) =

1
n
n∑
i=1
Xb(:, :,
i− 1
n
cin :
i
n
cin), cin = ncout,
concat(
n∑
i=1
Xb, dim = 2), cout = ncin,
(12)
where the input is averaged based on the channel dimension when cin = ncout, and the input is
repeated n times and concatenated together to fit the output dimension when cout = ncin.
Downsampling block. The downsampling layers are not binarized during training and inference for
MLP models, otherwise the performance will drastically decrease, as also discussed in prior binary
CNNs [27, 26, 33]. This brings the problem that the FLOPs of downsampling layers occupy the
whole network since the 3× 3 convolution with stride 2 is a common way of simultaneously reducing
the spatial size and changing the number of channels of the feature maps. Thus, similar to [13],
we separately handle the spatial and channel dimension by using 1 × 1 convolution with stride 1
(FC layer), and maxpooling with diverse kernel size to replace the original convolution, as shown in
Fig. 3. By this way, the computational complexity is significantly reduced while the classification
performance is maintained.
4 Experiments
In this section, we first evaluate the effectiveness of the proposed method on ImageNet dataset. The
experimental settings include the dataset statistics, details of the network architectures and the training
strategy is introduced in Sec. 4.1. In Sec. 4.2, we compare our method with a series of state-of-the-art
binary CNN models in terms of accuracy and FLOPs. The ablation study on each part of the proposed
method is conducted in Sec. 4.3.
4.1 Experimental Settings
ImageNet dataset. The benchmark dataset ImageNet-1k [36] contains over 1.2M training images
and 50k validation images from 1,000 different categories. This is the most commonly used image
classification dataset in other binary CNN methods.
6
Table 1: Details of different architectures of BiMLP. The parameter ‘dim’ indicates the dimension of
features, and ‘ratio’ is the expand ratio of the mid layer of MLP. FLOPs is calculated on the layers
that are not binarized, BOPs is calculated on binarized layer and OPs = BOPs / 64 + FLOPs. The
computational complexity is calculated based on the input shape 224× 224.
Output size BiMLP-S BiMLP-M
Stage 1 H
4
× W
4
[
dim = 64
ratio = 4
]
× 2
[
dim = 64
ratio = 4
]
× 2
Stage 2 H
8
× W
8
[
dim = 128
ratio = 4
]
× 2
[
dim = 128
ratio = 4
]
× 3
Stage 3 H
16
× W
16
[
dim = 320
ratio = 4
]
× 4
[
dim = 320
ratio = 4
]
× 10
Stage 4 H
32
× W
32
[
dim = 512
ratio = 4
]
× 2
[
dim = 512
ratio = 4
]
× 3
FLOPs (×108) 1.21 1.21
BOPs (×109) 2.25 4.32
OPs (×108) 1.56 1.88
Network architecture. We utilize the architecture proposed in [40] and replace the MLP block with
the proposed MBB block with Uni-shortcut (Fig. 2(b)) as well as the replacement of downsampling
blocks (Fig. 3). Details of the architectures of BiMLP models are specified in Tab. 1.
Training details. Following prior methods [27, 26, 30], we use a two step training strategy. In the
first training step, we use the full precision MLP model as the teacher model and the network with
the same architecture but binary activations as the student model. A knowledge distillation loss is
used to facilitate the training of student network:
L = α
n∑
i=1
Lkl(ys,yt) + (1− α)
n∑
i=1
Lce(ys,ygt), (13)
in which Lkl(·) and Lce(·) are the KL divergence and cross-entropy loss, ys and yt are the output
probability of the student and teacher model, ygt is the one-hot ground-truth probability and α is the
trade-off hyper-parameter (α = 0.9 in the following experiments). For the second training step, the
full precision ResNet-34 is used as the teacher network. The binarized MLP model (both weights and
activations) is used as the student model and the corresponding weights are initialized by the training
results from the first step.
In both steps, the student models are trained for 300 epochs using AdamW [28] optimizer with
momentum of 0.9 and weight decay of 0.05. We start with the learning rate of 1× 10−3 and a cosine
learning rate decay scheduler is used during training. We use NVIDIA V100 GPUs with a total
batchsize of 1024 to train the model with Mindspore [17]. The commonly used data-augmentation
strategies such as Cut-Mix [51], Mixup [52] and Rand-Augment [5] are used. The first layer, the last
layer and the downsampling layers are not binarized during training and inference.
4.2 Experimental Results on ImageNet
In this section, we compare the proposed model with the binary CNN models derived from other
state-of-the-art methods on the ImageNet-1k dataset, including Dorefa-Net [53], XNOR-Net [33],
ABCNet [24], Bireal-Net [27], Real2bin [30], MeliusNet [2], ReActNet [26], etc. As shown in Tab. 2,
the proposed BiMLP models achieve competitive performance compared to the state-of-the-art binary
CNN models. We can see that BiMLP-S achieves 70.0% top-1 accuracy and 89.6% top-5 accuracy
with 0.156G OPs, which surpasses the most recent methods such as MeliusNet-42 by 0.8% and 1.3%
with less than a half OPs, and is comparable to ReActNet-B. Meanwhile, BiMLP-M achieves 72.7%
top-1 accuracy and 91.1% top-5 accuracy with only 0.188G OPs, which has 12.1% less OPs than
ReActNet-C and is 1.3% better on top-1 accuracy. The above results show the superiority of the
proposed BiMLP.
7
Table 2: Experimental results on ImageNet-1k using different binary CNN models and the proposed
BiMLP. Bit-width (W/A) indicates the bit length of weights and activations. FLOPs is calculated on
the full-precision layers, BOPs is calculated on binarized layers and OPs = BOPs / 64 + FLOPs.
Methods Bit-width FLOPs BOPs OPs Top-1 Acc Top-5 Acc
(W/A) (×108) (×109) (×108) (%) (%)
XNOR-Net [33] 1/1 1.41 1.70 1.67 51.2 73.2
Dorefa-Net [53] 1/1 - - - 52.5 67.7
ABCNet [24] 1/1 - - - 42.7 67.6
Bireal-Net-18 [27] 1/1 1.39 1.68 1.63 56.4 79.5
Bireal-Net-34 [27] 1/1 1.39 3.53 1.93 62.2 83.9
UAD-BNN-18 [18] 1/1 - - - 57.2 80.2
UAD-BNN-34 [18] 1/1 - - - 62.8 84.5
RBNN-18 [23] 1/1 - - - 59.9 81.9
RBNN-34 [23] 1/1 - - - 63.1 84.4
Real2bin [30] 1/1 1.56 1.68 1.83 65.4 86.2
ReCU [50] 1/1 - - - 66.4 86.5
FDA-BNN [47] 1/1 - - - 66.0 86.4
LCR-BNN-18 [37] 1/1 - - - 59.6 81.6
LCR-BNN-34 [37] 1/1 - - - 63.5 84.6
LCR-BNN-ReAct [37] 1/1 - - - 69.8 85.7
Bi-half-18 [20] 1/1 - - - 60.4 82.9
Bi-half-34 [20] 1/1 - - - 64.2 85.4
AdaBin-18 [45] 1/1 1.41 1.69 1.67 66.4 86.5
AdaBin-ReAct [45] 1/1 - - 0.88 70.4 -
AdaBin-Melius59 [45] 1/1 - - 5.27 71.6 -
MeliusNet-22 [2] 1/1 1.35 4.62 2.08 63.6 84.7
MeliusNet-29 [2] 1/1 1.29 5.47 2.14 65.8 86.2
MeliusNet-42 [2] 1/1 1.74 9.69 3.25 69.2 88.3
MeliusNet-59 [2] 1/1 2.45 18.3 5.25 71.0 89.7
ReActNet-B [26] 1/1 0.44 4.69 1.63 70.1 -
ReActNet-C [26] 1/1 1.40 4.69 2.14 71.4 -
BiMLP-S (Ours) 1/1 1.21 2.25 1.56 70.0 89.6
BiMLP-M (Ours) 1/1 1.21 4.32 1.88 72.7 91.1
Table 3: Experimental results of using different branches in MBB blocks. The experiments are
conducted using BiMLP-S model on ImageNet-1k dataset, and setting 3 is used as the baseline
architecture which is the same as Fig. 2(b).
Setting MBB Block 1 MBB Block 2 OPs Top-1 Acc
#S1 #C1 #S2 #C2 (×108) (%)
1 4 0 0 2 1.60 69.2
2 0 2 4 0 1.53 68.9
3 2 1 2 1 1.56 70.0
4 4 1 2 2 1.86 70.8
5 2 2 4 1 1.78 70.5
4.3 Ablation Studies
In this section, we demonstrate the effectiveness of each part of the proposed method by conducting a
series of ablation studies.
Different branches in MBB blocks. Firstly, we show the experimental results of using different
branches in Tab. 3. The #Si indicates the number of Spatial Binary FC (MLP) in the MBB block i and
#Ci is the number of Channel Binary FC (MLP) in the corresponding block. Note that we fuse the
information along height and width of the feature map with different Spatial Binary FC (MLP), thus
the #Si should be even numbers. During the ablation study we keep (#S1 + #S2)/(#C1 + #C2) = 2
so that the ability of fusing information of different dimensions is roughly the same.
The experiments are conducted on BiMLP-S model. As shown in Tab. 3, when the MBB blocks
focus on only one dimension in setting 1 and 2, the top-1 accuracy decreases from 70.0% to 69.2%
8
Table 4: Different forms of normalization layer and activation function. The experiments are
conducted using BiMLP-S model on ImageNet-1k dataset.
Normalization LN LN LN LN BN BN BN BN
Activation GeLU ReLU PReLU RPReLU GeLU ReLU PReLU RPReLU
Top-1 Acc (%) 68.2 67.8 68.4 69.2 68.5 68.1 69.4 70.0
Table 5: Experimental results of using differ-
ent form of residual connections. The exper-
iments are conducted on ImageNet-1k dataset
with BiMLP-S model.
Top-1 Acc (%)
w/o shortcut 68.3
w/ Uni-shortcut 70.0
w/ shortcut 69.1
Table 6: Results of using different downsampling
layers. ‘Original’ indicates the 3×3 convolution
with stride 2, and ‘proposed’ means the proposed
downsampling block as shown in Fig. 3 right.
Setting OPs Top-1 Acc
(×108) (%)
Original 2.65 70.3
Proposed 1.56 70.0
and 68.9%, which shows that mixing the information from different dimensions in a single block is
important. In setting 4 and 5, more branches are used in each block compared to setting 3 (baseline),
and only improves the accuracy by 0.8% and 0.5%, while increases 19.2% and 14.1% OPs. Thus, we
use setting 3 for all the experiments considering the balance between the effectiveness and efficiency.
Normalization and activation. Different forms of normalization layer and activation function are
compared. The candidates of normalization layer include Layer normalization (LN) and batch
normalization (BN), and the candidates of activation functions are GeLU, ReLU, PReLU and
RPReLU. As the results shown in Tab. 4, the combination of BN and RPReLU achieves the best
accuracy, and is applied to all other experiments.
Shortcut connection. We also study the effectiveness of the proposed Uni-shortcut. Three different
setting are used in this ablation study. The first is using the MBB block without any residual
connection. The second is using the proposed Uni-shortcut in MBB blocks. The third is using the
traditional shortcut in MBB blocks. In this circumstances, only those input-output pairs who have the
same shape will be connected.
As shown in Tab. 5, compared to the MBB block without using any residual connection and using the
traditional shortcut, Uni-shortcut improves the top-1 accuracy by 1.7% and 0.9% which shows the
priority of the proposed method.
Downsampling layer. We compare the proposed downsampling block using the FC layer together
with multi-branch maxpooling to the original downsampling layer using 3× 3 convolution with stride
2 in Tab. 6. The result shows that we can significantly reduce the computational complexity from
0.265G to 0.156G (41.1% decrease) with only 0.3% drop of top-1 classification accuracy.
Hyper-parameters. As shown in Eq. 13, α is used as a hyper-parameter to balance the loss function
between learning from the output probability of teacher model and learning from the ground-truth
Table 7: Results of using different binarization
methods on ImageNet-1k dataset. The ‘FP32’
model indicates the Wave-MLP model, and ’1-
bit’ model uses sign function and STE rule to
binarize the FP32 model.
Model Top-1 Acc(%)
FP32 80.1
1-bit 55.3
+ Dorefa-Net [53] 58.4
+ XNOR-Net [33] 59.1
+ ReActNet [26] 63.2
+ Ours 70.0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
α
68.6
68.8
69.0
69.2
69.4
69.6
69.8
70.0
A
cc
u
ra
cy
(
%
)
Figure 4: Top-1 acc vs. α on ImageNet val set.
9
one-hot label. We report the accuracy on ImageNet-1k validation set in Fig. 4 and show that α = 0.9
yields the best result.
Comparisons of different binarization methods. We compare the proposed method to other
binarization methods proposed for CNN models in Tab. 7. The 1-bit model is to directly binarize the
FP32 Wave-MLP model using the sign function and the STE rule. The experimental results show that
our method can significantly outperform other binarization methods that are directly applied to MLP
model.
Table 8: Comparisons of FP32 and bi-
nary variants for original and our pro-
posed architectures on ImageNet-1k.
Model Wave-MLP BiMLP
FP32 80.1 79.9
1-bit 63.2 70.0
Overall benefit of modifying the architecture. We con-
duct an ablation study to show the overall benefit of mod-
ifying the architecture. The accuracy gaps between FP32
and the binary version of the original MLP and the modi-
fied MLP are reported in Tab. 8. We can see that the FP32
variant of the modified model achieves roughly the same
performance with original FP32 model. Meanwhile, our
proposed BiMLP architecture outperforms the 1bit Wave-
MLP with a large margin, which justifies the effectiveness
of the proposed binary vision MLP architecture.
5 Conclusion
In this paper, we propose a new paradigm with specific architecture design for binarizing the vision
MLP models. We point out that compared to binarizing the convolutional operation with large kernel
size in CNN models, binarizing the FC layers yields a MLP model with poor representation ability.
Simply increase the number of channels will fix this problem, but the computational complexity is
quadratically increased. Thus, we propose a novel multi-branch binary MLP block (MBB block) with
universal shortcut (Uni-shortcut) that can recover the representation ability while maintaining the
computational complexity. We also redesign the downsampling layer that can significantly reduce the
OPs of the binary MLP model while at the same time keep the performance roughly unchanged. The
experimental results on ImageNet-1k dataset demonstrate the effectiveness of the proposed method,
and we achieve a comparable performance with the state-of-the-art binary CNN models.
Acknowledgments
We gratefully acknowledge the support of MindSpore, CANN (Compute Architecture for Neural
Networks) and Ascend AI Processor used for this research.
References
[1] Davide Bacciu and Danilo P Mandic. Tensor decompositions in deep learning. arXiv preprint
arXiv:2002.11835, 2020.
[2] Joseph Bethge, Christian Bartz, Haojin Yang, Ying Chen, and Christoph Meinel. MeliusNet: Can binary
neural networks achieve mobilenet-level accuracy? In WACV, 2021.
[3] Shoufa Chen, Enze Xie, Chongjian Ge, Ding Liang, and Ping Luo. CycleMLP: A mlp-like architecture for
dense prediction. In ICLR, 2022.
[4] Brian Chmiel, Ron Banner, Gil Shomron, Yury Nahshan, Alex Bronstein, Uri Weiser, et al. Robust
quantization: One model to rule them all. NeurIPS, 33:5308–5317, 2020.
[5] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data
augmentation with a reduced search space. In CVPR Workshops, pages 702–703, 2020.
[6] Sajad Darabi, Mouloud Belbahri, Matthieu Courbariaux, and Vahid Partovi Nia. Bnn+: Improved binary
network training. 2018.
[7] Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong Han, Guiguang Ding, and Jian Sun. Scaling up
your kernels to 31x31: Revisiting large kernel design in cnns. In CVPR, 2022.
[8] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie
Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. In ICCV, 2019.
[9] Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, and Chang Xu. Cmt:
Convolutional neural networks meet vision transformers. In CVPR, pages 12175–12185, 2022.
10
[10] Jianyuan Guo, Yehui Tang, Kai Han, Xinghao Chen, Han Wu, Chao Xu, Chang Xu, and Yunhe Wang.
Hire-mlp: Vision mlp via hierarchical rearrangement. In CVPR, pages 826–836, 2022.
[11] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. Ghostnet: More features
from cheap operations. In CVPR, pages 1580–1589, 2020.
[12] Kai Han, Yunhe Wang, Yixing Xu, Chunjing Xu, Enhua Wu, and Chang Xu. Training binary neural
networks through learning with noisy supervision. In ICML, pages 4017–4026. PMLR, 2020.
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep convolutional
networks for visual recognition. IEEE T-PAMI, 37(9):1904–1916, 2015.
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In CVPR, pages 770–778, 2016.
[15] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating deep
convolutional neural networks. In IJCAI, 2018.
[16] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2(7), 2015.
[17] Huawei. Mindspore. https://www.mindspore.cn/, 2020.
[18] Hyungjun Kim, Jihoon Park, Changhun Lee, and Jae-Joon Kim. Improving accuracy of binary neural
networks using unbalanced activation distribution. In CVPR, pages 7862–7871, 2021.
[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional
neural networks. NeurIPS, 25, 2012.
[20] Yunqiang Li, Silvia-Laura Pintea, and Jan C van Gemert. Equal bits: Enforcing equally distributed binary
network weights. In AAAI, volume 36, pages 1491–1499, 2022.
[21] Zhen Li, Jinglei Yang, Zheng Liu, Xiaomin Yang, Gwanggil Jeon, and Wei Wu. Feedback network for
image super-resolution. In CVPR, pages 3867–3876, 2019.
[22] Dongze Lian, Zehao Yu, Xing Sun, and Shenghua Gao. AS-MLP: An axial shifted mlp architecture for
vision. In ICLR, 2022.
[23] Mingbao Lin, Rongrong Ji, Zihan Xu, Baochang Zhang, Yan Wang, Yongjian Wu, Feiyue Huang, and
Chia-Wen Lin. Rotated binary neural network. NeurIPS, 33:7474–7485, 2020.
[24] Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. NeurIPS,
30, 2017.
[25] Hanxiao Liu, Zihang Dai, David So, and Quoc V Le. Pay attention to mlps. In NeurIPS, volume 34, pages
9204–9215, 2021.
[26] Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards precise binary
neural network with generalized activation functions. In ECCV, pages 143–159. Springer, 2020.
[27] Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real net:
Enhancing the performance of 1-bit cnns with improved representational capability and advanced training
algorithm. In ECCV, pages 722–737, 2018.
[28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101, 2017.
[29] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network
compression. In ICCV, pages 5058–5066, 2017.
[30] Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos. Training binary neural networks
with real-to-binary convolutions. arXiv preprint arXiv:2003.11535, 2020.
[31] Ying Nie, Kai Han, and Yunhe Wang. Multi-bit adaptive distillation for binary neural networks. In BMVC,
2021.
[32] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic
segmentation. In ICCV, pages 1520–1528, 2015.
[33] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classifica-
tion using binary convolutional neural networks. In ECCV, pages 525–542. Springer, 2016.
[34] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time
object detection. In CVPR, pages 779–788, 2016.
[35] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection
with region proposal networks. NeurIPS, 28, 2015.
[36] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.
IJCV, 115(3):211–252, 2015.
11
[37] Yuzhang Shang, Dan Xu, Bin Duan, Ziliang Zong, Liqiang Nie, and Yan Yan. Lipschitz continuity retained
binary neural network. In ECCV, 2022.
[38] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. arXiv preprint arXiv:1409.1556, 2014.
[39] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In
ICML, pages 6105–6114. PMLR, 2019.
[40] Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Yanxi Li, Chao Xu, and Yunhe Wang. An image patch is
a wave: Phase-aware vision mlp. In CVPR, 2022.
[41] Yehui Tang, Yunhe Wang, Yixing Xu, Yiping Deng, Chao Xu, Dacheng Tao, and Chang Xu. Manifold
regularized dynamic network pruning. In CVPR, pages 5018–5028, 2021.
[42] Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing Xu, Chao Xu, and Chang Xu. Scop:
Scientific control for reliable neural network pruning. NeurIPS, 33:10936–10947, 2020.
[43] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner,
Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture
for vision. NeurIPS, 34, 2021.
[44] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave,
Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, et al. Resmlp: Feedforward networks
for image classification with data-efficient training. arXiv preprint arXiv:2105.03404, 2021.
[45] Zhijun Tu, Xinghao Chen, Pengju Ren, and Yunhe Wang. AdaBin: Improving binary neural networks with
adaptive binary sets. In ECCV, 2022.
[46] Yinan Wang, Weihong “Grace” Guo, and Xiaowei Yue. Tensor decomposition to compress convolutional
layers in deep learning. IISE Transactions, 54(5):481–495, 2022.
[47] Yixing Xu, Kai Han, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe Wang. Learning frequency domain
approximation for binary neural networks. NeurIPS, 34:25553–25565, 2021.
[48] Yixing Xu, Yunhe Wang, Hanting Chen, Kai Han, Chunjing Xu, Dacheng Tao, and Chang Xu. Positive-
unlabeled compression on the cloud. NeurIPS, 32, 2019.
[49] Yixing Xu, Chang Xu, Xinghao Chen, Wei Zhang, Chunjing Xu, and Yunhe Wang. Kernel based
progressive distillation for adder neural networks. NeurIPS, 33:12322–12333, 2020.
[50] Zihan Xu, Mingbao Lin, Jianzhuang Liu, Jie Chen, Ling Shao, Yue Gao, Yonghong Tian, and Rongrong Ji.
Recu: Reviving the dead weights in binary neural networks. In ICCV, pages 5198–5208, 2021.
[51] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019.
[52] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk
minimization. In ICLR, 2018.
[53] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low
bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160,
2016.
[54] Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, and Pascal Frossard. Adaptive quanti-
zation for deep neural network. In AAAI, volume 32, 2018.
Checklist
1. For all authors...
(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes]
(c) Did you discuss any potential negative societal impacts of your work? [N/A]
(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results...
(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments...
12
(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes]
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes]
(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [No]
(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [N/A]
(b) Did you mention the license of the assets? [N/A]
(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
(d) Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identifiable
information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects...
(a) Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]
13
第4个文件的内容为：
Mixture of Scales: Memory-Efficient Token-Adaptive
Binarization for Large Language Models
Dongwon Jo 1 Taesu Kim 2 Yulhwa Kim 3
∗
Jae-Joon Kim 1
∗
1 Seoul National University 2 SqueezeBits Inc. 3 Sungkyunkwan University
{dongwonjo, kimjaejoon}@snu.ac.kr
{taesu.kim}@squeezebits.com
{yulhwakim}@skku.edu
Abstract
Binarization, which converts weight parameters to binary values, has emerged as
an effective strategy to reduce the size of large language models (LLMs). However,
typical binarization techniques significantly diminish linguistic effectiveness of
LLMs. To address this issue, we introduce a novel binarization technique called
Mixture of Scales (BinaryMoS). Unlike conventional methods, BinaryMoS em-
ploys multiple scaling experts for binary weights, dynamically merging these
experts for each token to adaptively generate scaling factors. This token-adaptive
approach boosts the representational power of binarized LLMs by enabling contex-
tual adjustments to the values of binary weights. Moreover, because this adaptive
process only involves the scaling factors rather than the entire weight matrix, Bina-
ryMoS maintains compression efficiency similar to traditional static binarization
methods. Our experimental results reveal that BinaryMoS surpasses conventional
binarization techniques in various natural language processing tasks and even out-
performs 2-bit quantization methods, all while maintaining similar model size to
static binarization techniques.
1 Introduction
Though large language models (LLMs) have delivered impressive results in a variety of natural
language processing (NLP) tasks, their massive size often complicates deployment. One common
method to compress LLMs is through the quantization of weight parameters, which reduces model
sizes by lowering the precision of weight values [1, 8, 2, 30, 31, 32, 33, 3, 4]. Existing quantization
approaches such as GPTQ [2], AWQ [3], and OWQ [4] have successfully managed to reduce model
sizes by converting 16-bit floating point weights to 4-bit representations, achieving a fourfold decrease
in size. Binarization pushes this concept even further by reducing weight values to 1-bit, resulting in
a 16-fold size reduction.
However, such aggressive compression through binarization drastically limits the representational
capacity of weights, leading to a significant degradation in the linguistic capabilities of LLMs. To
address this limitation and improve the accuracy of binarized LLMs, recent research has actively
explored binarization techniques tailored for LLMs [29, 5, 6, 7]. Nonetheless, previous efforts often
compromise the inherent advantages of binarization by introducing high memory overhead, and they
continue to struggle to achieve sufficient accuracy with binarized LLMs.
In this paper, we propose a novel binarization technique named as Mixture of Scales (BinaryMoS).
Typical binarization methods use scaling factors to control the effective values of binarized weights.
Although these scaling factors occupy a tiny fraction of the overall model size, they are crucial
∗Corresponding Author
Preprint. Under review.
ar
X
iv
:2
40
6.
12
31
1v
1
[
cs
.L
G
]
1
8
Ju
n
20
24
Figure 1: A brief overview of various LLM binarization methods. PB-LLM involves both a binary
weight matrix and a high-precision, sparse weight matrix, and BiLLM stores four types of binary
weight matrices. OneBit simplifies the layer structure by introducing scaling factors for input and
output dimensions respectively. BinaryMoS introduces multiple scaling experts to enhance the
capacity of binarized models.
in reducing binarization error. BinaryMoS advances the functionality of these scaling factors by
incorporating token-adaptive scaling factors. Inspired by the Mixture of Experts (MoE) approach [26,
27, 34], which empolys multiple expert layers to enhance the model capacity, BinaryMoS adopts
multiple scaling factors as experts to improve the representational capacity of binarized LLMs in a
memory-efficient way. During inference, BinaryMoS linearly combines these scaling experts based
on the context to generate token-adaptive scaling factors, thus dynamically adjusting the represented
values of binarized weights to maximize the expressive power of the model. As a result, BinaryMoS
can improve the linguistic performance of binarized LLMs with minimal memory overhead.
2 Background
2.1 Binarization of LLMs
Binarization stands out as an extreme yet effective method for reducing model sizes in deep learning.
This method achieves size reduction by transforming high-precision weight parameters into 1-bit
values. The binarization process is typically governed by the following equation:
WB = α · Sign(WFP −WFP ) (1)
Here, WFP ∈ Rn×m is the full-precision weight matrix of a linear layer where n and m represent
the size of output and input dimension, respectively, and WB ∈ Rn×m denotes its binarized version.
α ∈ Rn represents scaling factors that are responsible for adjusting the binary weight values along
the output dimension. In general, the scaling factors are analytically derived as the absolute mean of
FP weight values to minimize the L2 error between full-precision and binarized weights, and these
scaling factors play a vital role in bridging the gap between the original full-precision weights and
their binarized counterparts.
While binarization has been effectively applied in traditional deep learning models like Convolutional
Neural Networks (CNNs) for image classification without losing accuracy [9, 11, 10, 12], LLMs tend
to be more sensitive to such extreme quantization, often experiencing significant accuracy degradation
with standard binarization techniques. Therefore, various binarziation techniques tailored for LLMs
2
have been developed, as shown in Figure 1. PB-LLM [5] partially binarizes weight parameters while
maintaining salient weight parameters as high-precision values (e.g., Float16 or INT8). However, this
method results in considerable memory overhead. For instance, quantizing 10% of weight parameters
as INT8 while binarizing the remaining 90% results in an average bit-width of 1.7 bits for the weight
parameters, which is closer to 2 bits than 1 bit.
Furthermore, despite this partial binarization strategy of PB-LLM, the significant information loss
inherent in binarization still causes considerable accuracy degradation. To reduce the binarization
error and enhance accuracy, BiLLM [6] adopts a more refined approach to assigning scaling factors.
Assuming that weight parameters follow a bell-shaped distribution, BiLLM categorizes weight
parameters based on their proximity to the mean value: concentrated weights, close to the mean, and
sparse weights, distant from the mean. Distinct scaling factors are then assigned to each group to
minimize binarization errors. Then, to reduce the memory overhead associated with maintaining
information of salient weights, BiLLM preserves this information by binarizing the difference
between the binarized values and their full-precision counterparts. Consequently, each salient
weight is represented by two 1-bit values, effectively amounting to a 2-bit representation. Despite
significantly reducing binarization error, BiLLM complicates the structure of binarized LLMs, adding
complexity to the inference process. This complexity arises from the need to manage additional sparse
and salient weights alongside regular concentrated weights, requiring extra matrix multiplication
during inference.
Meanwhile, unlike conventional binarization methods that typically employ scaling factors only for the
output dimension of weights, OneBit [7] enhances the binarization process by incorporating scaling
factors for both the input and output dimensions. This dual-dimension scaling approach addresses
binarization errors across both dimensions, potentially enhancing model accuracy. Additionally,
the size of each scaling vector is substantially smaller compared to the weight matrix, making this
approach memory efficient. For instance, in linear layers with a hidden dimension of h, the weight
matrix size is h×h, while each scaling vector is only h×1. Therefore, doubling these scaling factors
adds a negligible memory overhead to the network. Moreover, as this approach of dual-dimensional
scaling efficiently preserves enough information to significantly reduce binarization errors, OneBit
eliminates the need to store separate information for salient weights, thereby simplifying the model
structure. The result of matrix multiplication Y of a linear layer using the OneBit approach can be
defined as follows:
Y = X[ST
in ⊙ Sign(WT
FP )⊙ Sout] = [(X ⊙ Sin)Sign(WT
FP )]⊙ Sout (2)
Here, X ∈ Rk×m is the matrix of input activation where k represents batch size, while Sin ∈ R1×m
and Sout ∈ R1×n denote the scaling factors for input and output dimensions, respectively. As outlined
in Equation 2, processing scaling factors for both input and output dimension can be simplified to
scaling input and output of the linear layer before and after matrix multiplication, respectively.
Despite advances in binarization techniques for LLMs, a notable accuracy gap still exists between full-
precision models and their binarized counterparts. Therefore, bridging this gap without sacrificing the
fundamental benefits of binarization, particularly low memory usage, remains an important challenge
in the field of LLM compression.
2.2 Mixture of Experts
The MoE approach is a widely adopted strategy to boost the capabilites of deep learning models by
integrating multiple specialized experts into a single framework [26, 27, 34]. Typically, the MoE
approach for LLMs involves duplication of layers and selecting the appropriate layers among these
duplicates for a specific task during inference. In the MoE setup, the router is a key to selecting
the appropriate expert. It generally consists of a linear layer followed by a softmax function, which
calculates and assigns scores to each expert. During the inference, only the experts with the highest
score are selected and processed.
While integrating the MoE approach with binarized LLMs offers potential for improving model
accuracy, it presents a substantial memory trade-off. The duplication of layers inherent in MoE
increases the model size proportionally with the number of experts, thus diminishing the memory
efficiency benefits gained from binarization. To address these challenges, we propose BinaryMoS,
a novel binarization technique that aims to enhance model capacity while maintaining memory
efficiency. This approach leverages scaling factors as experts, improving accuracy of binarized
3
Figure 2: Illustration of the proposed BinaryMoS scheme. The proposed BinaryMoS introduce
mixture of scale approach to generate token-adaptive scaling factors.
LLMs without the extensive memory overhead associated with traditional MoE configurations. In
the following section, we will delve deeper into how BinaryMoS operates and its benefits over
conventional techniques.
3 Proposed BinaryMoS
3.1 Binarization with Mixture of Scale
An overview of the proposed BinaryMoS is presented in Figure 2. Unlike previous binarization
techniques that utilize a single scaling vector per input or output dimension, BinaryMoS integrates
the concept of experts from the MoE framework into the scaling factors and utilizes multiple scaling
experts for each dimension. As discussed in Section 2.1, although the size of scaling factors is
relatively small, they play a crucial role in preserving the accuracy of binarized models. Therefore,
introducing multiple scaling experts incurs minimal memory overhead while effectively leveraging
the advantages of the MoE strategy to enhance the capabilities of binarized models.
In the MoE framework, the number of experts selected corresponds directly to the number of layers
processed. As a result, the typical MoE framework selects only one or two experts per inference
stage to manage the increased processing burden associated with more experts being selected. On the
other hand, the scaling factors of binarized LLMs are solely involved in linear operations with matrix
multiplication, as detailed in Equation 2. This linearity allows for the efficient management of multiple
scaling experts by linearly combining them before executing the matrix multiplication. Hence, instead
of selecting only a few experts, as done in the conventional MoE framework, BinaryMoS dynamically
generates instructions on how to combine these scaling experts based on the context. This approach
overcomes the limitations of fixed expert choices in typical MoE setups by enabling the creation
of effectively infinite token-adaptive scaling factors through linear combinations. Consequently, by
optimally utilizing the representational power of multiple scaling experts, BinaryMoS maximizes the
potential of binarized models while maintaining memory efficiency.
3.2 Router Design
In order to generate the token-adaptive scaling factors, the proposed BinaryMoS designs the router
for processing the following operations:
G = Softmax(XWR) (3)
Ŝin = GSin, Ŝout = GSout (4)
Here, WR ∈ Rm×e represents the weight parameters of router’s linear layer, where e denotes
the number of experts. Sin ∈ Re×m and Sout ∈ Re×n denote the scaling experts for input and
output dimension, respectively. Initially, the router computes the gating score G, which represents
4
Table 1: Comparison of memory requirements for deploying Float16 and binarized models, with the
number in parentheses denoting the compression ratio of binarized models over Float16 models.
Model Float16 PB-LLM BiLLM OneBit BinaryMoS
LLaMA-1/2-7B 13.51 GB 2.78 GB (4.86×) 2.28 GB (5.93×) 1.37 GB ( 9.86×) 1.40 GB ( 9.65×)
LLaMA-1/2-13B 26.20 GB 5.02 GB (5.22×) 4.06 GB (6.45×) 2.29 GB (11.44×) 2.33 GB (11.24×)
the significance of each scaling expert, using input activations and router weights, as outlined in
Equation 3. Notably, as the gating scores are generated with the softmax function, the sum of gating
scores for the scaling experts equals 1. These scores are used to linearly combine the scaling experts,
resulting in the creation of token-adaptive scaling factors Ŝin and Ŝout, as shown in Equation 4. Then,
by replacing the static scaling factors Sin and Sout from Equation 2 with token-adaptive scaling
factors, the result of matrix multiplication Ŷ in a linear layer using the BinaryMoS approach can be
revised as follows:
Ŷ = [(X ⊙ Ŝin)Sign(WT
FP )]⊙ Ŝout (5)
We empirically find that using four scaling experts each for the input and output dimensions provides
the optimal compromise between increasing model size and improving accuracy. Consequently, the
proposed BinaryMoS utilizes four scaling experts for each dimension to enhance accuracy while
maintaining efficiency.
3.3 Impact of BinaryMos on LLM Compression
The proposed BinaryMoS introduces additional memory overhead due to multiple scaling experts
and the weights of the router. However, this overhead is relatively minor. For instance, in the
LLaMA-1/2-7B model [16] with a hidden dimension h of 4096, the weight matrix for the linear
layers is typically 4096×4096. If BinaryMoS adopts 4 scaling experts, this translates to four α’s, each
of dimension 4096×1, for both input and output dimensions. Additionally, the weights of the router
would be 4096×4. Compared to the previous OneBit method, which requires a single α for both
input and output dimensions, the additional components in BinaryMoS total 4096×10 parameters.
The number of these extra parameters constitutes only 0.2% of the original weight parameters.
For a comprehensive examination of the impact of various binarization techniques, including Binary-
MoS, on LLM compression, we evaluate the memory requirements of LLaMA models with Float16
parameters and after applying different binarization methods, as detailed in Table 1. Following stan-
dard practice, all binarization techniques exclude the embedding layer and lm-head from binarization.
Our analysis reveals that BinaryMoS significantly reduces the memory footprint of models, achieving
compression ratios ranging from 9.65× to 11.24×. As model size increases, the relative impact of
additional parameters diminishes and the proportion of the unbinarized part decreases. Hence, we
can achieve higher compression ratios for larger models. For instance, the original LLaMA-1/2-13B
model, requiring 26.20 GB for deployment, is impractical for edge devices due to its size. However,
BinaryMoS reduces this model to just 2.33 GB, representing an 11.24-fold decrease in memory
requirements. This significant reduction facilitates deployment on edge devices with typically limited
memory capacities of 4 GB.
In contrast, PB-LLM and BiLLM methods achieve relatively lower compression ratios of around 5×
and 6×, respectively. This is primarily due to two reasons: first, PB-LLM and BiLLM methods must
retain salient weight information, increasing the average bitwidth of weight parameters. Second, the
handling of sparse weight matrices in these methods introduces overhead in indexing sparse weight
matrices, limiting the achievable compression ratio. OneBit achieves the highest compression ratio
by only introducing dual-dimension scaling factors. Remarkably, BinaryMoS achieves a comparable
compression ratio to OneBit, despite incorporating additional components for scaling experts. While
the memory requirement of binarized models with BinaryMoS increases by only 2% compared
to OneBit, the inclusion of scaling experts offers much greater potential to significantly improve
perplexity.
This analysis demonstrates that although BinaryMoS introduces additional parameters, the relative
increase in memory requirement is modest. This makes BinaryMoS a viable option for enhancing
accurcy of binarized models without imposing a significant memory burden.
5
3.4 Quantization-Aware Knowledge Distillation
Following training strategies adopted for network compression [13, 14], we adopt the knowledge
distillation (KD) to transfer the knowledge of a full-precision teacher model to a binarized student
model. We employ the cross entropy (CE) loss to distill the logit knowledge. This is calculated using
the following equation:
LCE = − 1
n
∑
c
n∑
i=1
pTc (Xi) log
(
pSc (Xi)
)
(6)
Here, S and T represent the student and teacher models respectively. n denotes batch size, and c is
the number of classes. Additionally, to minimize the distributional discrepancies in layer outputs, we
incorporate a mean-squared error (MSE) based layer-to-layer (L2L) loss as follows:
LL2L =
L∑
l=1
MSE
(
HT
l ,H
S
l
)
(7)
In this loss, HT
l and HS
l are the output logits from the l-th layer of the teacher and student models,
respectively. The total loss function, integrating both CE and L2L distillation losses, is defined as:
L = LCE + α · LL2L (8)
where α is a hyperparameter that balances the contributions of the CE and L2L losses. For the
training of BinaryMoS, we empirically set α = 10.
4 Experiments
4.1 Experimental Settings
Models and Evaluation Datasets. In our study, we evaluate BinaryMoS on various models, including
those from the LLaMA-1 [16], LLaMA-2 [17], and OPT [15] families. Specifically, we utilize the
OPT models with 125M and 1.3B parameters, and the LLaMA-1 and LLaMA-2 models with 7B and
13B parameters for our evaluations. We measure language modeling capabilities of these models
by evaluating their perplexity on the WikiText2 [24] and C4 [25] datasets. Additionally, we assess
zero-shot accuracy on various Common Sense Reasoning Tasks such as BoolQ [19], PIQA [20],
HellaSwag [21], WinoGrande [22], ARC-e, ARC-c [23]), utilizing the open-source LLM evaluation
framework, LM-Evaluation-Harness [35].
Training Details. We initialize the parameters of binarized models using those from pre-trained
models, which serve as teacher models for KD. For the training dataset, a mixed dataset composed of
the WikiText2 training dataset and a selected partition from the C4 training dataset, with a sequence
length of 2048. The training is conducted over three epochs using the AdamW [18] optimizer, with
hyperparameters set to β1 = 0.9, β2 = 0.999, and zero weight decay. We implement a cosine decay
learning rate scheduler, preceded by a warm-up phase constituting 0.03 of the total training duration.
All training sessions are conducted on NVIDIA A100 GPUs.
Baselines. We compare BinaryMoS against previous LLM binarization methods, including PB-
LLM [5], BiLLM [6], and OneBit [7], ensuring that all implementations adhere to the details
provided in their respective papers. PB-LLM and BiLLM utilize the Post-Training Quantization
(PTQ) approach for model calibration through the Optimal Brain Quantizer (OBQ) based method of
GPTQ [2]. For PB-LLM, which allows variable ratios of salient weights to enhance accuracy, we have
set the ratio of salient weights to 10% to ensure the average bit width of weight parameters remains
below 2 bits. OneBit employs a Quantization-Aware Training (QAT) approach, and for fairness,
its training setup is aligned with that of BinaryMoS. Given the significant accuracy improvements
demonstrated by BinaryMoS over traditional binarization techniques, we also include a comparison
with 2-bit quantization methods with PTQ approach, such as GPTQ [2] and OmniQuant [28], to
broaden the evaluation scope.
4.2 Analysis on the Number of Scaling Experts
To determine the optimal number of scaling experts for BinaryMoS, which effectively maintains
the accuracy of binarized LLMs while minimizing memory usage, we conduct evaluations with
6
Table 2: The impact of the numbers of scaling experts on the proposed BinaryMoS. Quick assessment
conducted using the LLaMA-1-7B model trained on one-third of the training data.
# of Experts Perplexity ↓ Zero-shot Accuracy ↑
Wiki2 C4 BoolQ PIQA Hella. WinoG. ARC-e ARC-c Average
1 9.33 12.54 60.27 67.84 46.77 52.09 38.38 27.98 48.89
2 9.19 12.18 62.69 68.55 48.36 55.09 40.23 28.92 50.64
4 8.92 11.85 60.51 67.46 49.95 55.24 41.16 29.35 50.61
8 9.17 12.28 58.68 67.46 47.51 53.67 39.52 29.43 49.38
Figure 3: (a) Gating scores of 4 scaling experts in 18th layer of LLaMA-1-7B model for each token in
the input sequence. (b) Distribution of values of token-adaptive scaling factors. The boxplot visually
presents the distribution of token-adaptive scaling factors among processed tokens. The box spans
the interquartile range, indicating the middle 50% of the scaling factors. Extending from the box are
whiskers that reach the furthest data points within 1.5 times the interquartile range, providing insight
into the overall range of the data.
LLaMA-1-7B using varying numbers of scaling experts. This evaluation is conducted using only
one-third of the training data for quick assessment. As shown in Table 2, performance metrics,
including perplexity and accuracy, generally improve as the number of experts increases from 1 to 4.
However, a further increase to 8 experts leads to a decline in model performance. This decline arises
from the challenge of training routers to appropriately assign scales to tokens as the number of scales
increases. Based on these observations, we choose to employ 4 experts in the BinaryMoS approach.
4.3 Analysis on the Token-Adaptive Scaling Factors
In this section, we explore the effectiveness of the proposed mixture of scale approach in generating
token-adaptive scaling factors. To accomplish this, we analyze the gating scores for scaling experts
and the scaling factors derived from these scores. For this analysis, we utilize the LLaMA-1-7B
model and input sequences sampled from the C4 dataset.
Figure 3 showcases the gating scores and resulting token-adaptive scaling factors for out projection
of the 18th layer across tokens of the input sequence. The experimental results reveal substantial
variation in the gating scores for each expert across tokens. As depicted in Figure 3(b), while
conventional binarization methods with static scaling factors, akin to having a single expert, offer a
fixed scaling factor, the scaling experts of BinaryMoS successfully generate a diverse range of scaling
factors. This highlights the efficacy of the mixture of scale approach, which adaptively determines the
7
Table 3: Perplexity and zero-shot accuracy results of Float16 and binarized LLMs.
Model Method Wbits Perplexity ↓ Zero-shot Accuracy ↑
Wiki2 C4 BoolQ PIQA Hella. WinoG. ARC-e ARC-c Average
OPT-125M
Float16 16 27.65 24.60 55.47 62.02 31.33 50.19 39.98 22.86 43.64
PB-LLM 1 3233.63 1509.33 37.83 50.60 26.67 50.43 27.02 23.63 36.02
BiLLM 1 2989.53 1769.26 37.82 50.59 25.75 51.30 27.65 23.63 36.12
OneBit 1 39.45 35.58 61.92 60.01 27.01 50.43 35.81 21.84 42.84
BinaryMoS 1 36.46 33.13 61.83 60.17 27.16 51.38 36.74 22.95 43.37
OPT-1.3B
Float16 16 14.62 14.72 57.82 72.42 53.70 59.51 50.97 29.52 53.99
PB-LLM 1 272.83 175.42 62.17 54.24 27.25 50.27 27.98 23.72 40.94
BiLLM 1 69.45 63.92 61.92 59.52 33.81 49.32 34.38 22.35 43.55
OneBit 1 20.36 20.76 57.85 66.53 39.21 54.61 42.80 23.97 47.50
BinaryMoS 1 18.45 18.83 60.34 68.66 41.99 53.99 44.87 26.19 49.34
LLaMA-1-7B
Float16 16 5.68 7.08 73.21 77.42 72.99 66.85 52.53 41.38 64.06
PB-LLM 1 198.37 157.35 60.51 53.53 27.23 49.17 27.48 26.02 40.66
BiLLM 1 41.66 48.15 62.23 58.65 34.64 51.14 33.08 25.68 44.24
OneBit 1 8.48 10.49 62.50 70.40 54.03 55.32 41.07 30.88 52.36
BinaryMoS 1 7.97 9.72 64.59 71.82 58.18 58.88 42.09 31.31 54.48
LLaMA-1-13B
Float16 16 5.09 6.61 68.47 79.05 76.24 70.17 59.85 44.54 66.39
PB-LLM 1 35.83 39.79 62.17 58.70 33.97 52.17 31.86 23.63 43.75
BiLLM 1 14.56 16.67 62.53 68.17 52.24 59.43 41.91 29.94 52.37
OneBit 1 7.65 9.56 63.30 71.98 60.61 59.43 42.85 32.42 55.10
BinaryMoS 1 7.16 8.81 63.82 73.88 64.05 60.93 44.28 33.11 56.68
LLaMA-2-7B
Float16 16 5.47 6.97 71.07 76.87 72.95 67.16 53.45 40.78 63.71
PB-LLM 1 76.75 85.92 62.17 52.82 26.87 50.11 26.89 24.31 40.53
BiLLM 1 27.72 36.34 62.14 59.19 35.18 53.11 34.22 26.54 45.06
OneBit 1 8.60 10.74 63.06 70.40 54.24 56.67 40.82 29.35 52.42
BinaryMoS 1 7.88 9.75 65.02 71.55 59.41 56.18 41.84 30.03 54.01
LLaMA-2-13B
Float16 16 4.88 6.47 68.99 79.05 76.62 69.77 57.95 44.20 66.10
PB-LLM 1 155.25 151.15 37.82 53.26 28.89 49.48 28.28 23.72 36.91
BiLLM 1 20.71 27.19 62.20 62.51 38.05 56.35 40.69 27.73 47.92
OneBit 1 7.56 9.67 65.66 71.60 60.07 56.91 45.76 31.74 55.29
BinaryMoS 1 7.08 8.91 66.12 73.72 63.80 58.98 45.71 33.19 57.09
scaling factor for each token, leading to a wider representation range. Consequently, we can expect
that BinaryMoS effectively enhances the capacity of binarized models and improves model accuracy.
4.4 Perplexity and Accuracy Results of Binarized Models
The perplexity and zero-shot accuracy results of previous binarization methods and the proposed
BinaryMoS are presented in Table 3. BinaryMoS consistently outperforms earlier binarization
techniques across all metrics, effectively narrowing the performance disparity with their Float16
counterparts.
In particular, smaller LLMs such as OPT-125M and OPT-1.3B typically face challenges in maintaining
linguistic capabilities under model compression. Previous methods like PB-LLM and BiLLM result
in significant increases in perplexity, often exceeding 1000 for the OPT-125M model. While OneBit
made substantial improvements, perplexity increases remained above 10. BinaryMoS, however,
significantly enhances these outcomes by keeping the increase in perplexity below 10. Moreover,
it boosts the accuracy of binarized models and diminishes the zero-shot accuracy gap to within
0.3% compared to Float16 models. The distinct advantage of BinaryMoS over previous approaches,
especially OneBit, lies in its use of scaling experts. This evaluation underlines the efficacy of the
BinaryMoS with mixture of scales approach.
8
Table 4: Perplexity and zero-shot accuracy results for 2-bit quantization methods and BinaryMoS.
Perplexity ↓ (Wikitext2)
Method Wbits OPT-125M OPT-1.3B LLaMA-1-7B LLaMA-1-13B LLaMA-2-7B LLaMA-2-13B
GPTQ 2 660.52 125.29 45.73 15.20 40.23 32.87
OmniQuant 2 245.47 28.82 9.75 7.84 11.20 8.25
BinaryMoS 1 36.46 18.45 7.97 7.16 7.88 7.08
Perplexity ↓ (C4)
Method Wbits OPT-125M OPT-1.3B LLaMA-1-7B LLaMA-1-13B LLaMA-2-7B LLaMA-2-13B
GPTQ 2 213.60 45.43 27.87 15.15 31.37 26.23
OmniQuant 2 390.30 33.81 13.01 10.43 15.46 11.06
BinaryMoS 1 33.13 18.83 9.72 8.81 9.75 8.91
Average Zero-shot Accuracy ↑
Method Wbits OPT-125M OPT-1.3B LLaMA-1-7B LLaMA-1-13B LLaMA-2-7B LLaMA-2-13B
GPTQ 2 37.59 40.36 43.75 49.65 43.31 45.03
OmniQuant 2 36.54 46.43 51.58 56.42 49.54 54.24
BinaryMoS 1 43.37 49.34 54.48 56.68 54.01 57.09
4.5 Comparison between BinaryMoS and 2-bit Quantization
Since BinaryMoS consistently outperforms other binarization methods, we proceed to compare it
with conventional 2-bit quantization techniques, GPTQ and OmniQuant. While these two approaches
entail lower calibration overhead for quantization due to their use of the PTQ approach, they differ
in their quantization methods. GPTQ and OmniQuant utilize a group-wise quantization approach,
employing groups of 128 weights to finely quantize parameters and minimize quantization errors.
Consequently, the memory demand during inference for these methods is more than double that of
BinaryMoS. The comparison results, presented in Table 4, reveal that BinaryMoS even outperforms
these 2-bit quantization methods, despite its lower memory requirement during inference. This once
again underscores the effectiveness of integrating scaling experts.
5 Discussion and Future Work
BinaryMoS significantly improves the accuracy of binarized LLMs by increasing their representa-
tional capability with mixture of scales. This MoS approach holds promise for extension to multi-bit
quantization, as multi-bit quantization techniques also involve scaling factors for regulating quantiza-
tion step size. However, in this paper, our study does not delve into the effectiveness of the mixture
of scales on multi-bit quanization schemes, leaving this avenue for future exploration.
Though BinaryMoS adopts the concept of MoE, it does not fully leverage advanced training techniques
established in the field of MoE [26, 27, 34]. These advanced methods optimize routing functions
and balance token assignments among experts, thereby enhancing MoE model accuracy. Thus,
investigating these training techniques is another topic for future research.
6 Conclusion
This paper introduces BinaryMoS, a novel binarization technique designed to enhance the representa-
tion capability of binarized LLMs while preserving the fundamental advantage of binarization—low
memory usage. BinaryMoS adopts the mixture of scale approach to dynamically adjust the scaling
factors of binary weight values in a token-adaptive manner. Given that scaling factors play a crucial
role in reducing binarization error and occupy a small portion of binarized models, this approach
effectively mitigates information loss associated with binarization with minimal memory overhead.
Our experimental findings demonstrate that BinaryMoS surpasses existing binarization approaches
and even outperforms 2-bit quantization methods in both perplexity and zero-shot tasks.
9
References
[1] Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao
Dai, Huazhong Yang, Yu Wang, “Evaluating Quantized Large Language Models”, arXiv preprint
arXiv:2402.18158, 2024.
[2] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh, “GPTQ: Accurate Post-Training
Quantization for Generative Pre-trained Transformers”, International Conference on Learning
Representations (ICLR), 2023.
[3] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan
Xiao, Xingyu Dang, Chuang Gan, Song Han, “AWQ: Activation-aware Weight Quantization for
LLM Compression and Acceleration”, arXiv preprint arXiv:2306.00978, 2023.
[4] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, Eunhyeok Park, “OWQ: Outlier-Aware
Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models”, Pro-
ceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2024.
[5] Yuzhang Shang, Zhihang Yuan, Qiang Wu, Zhen Dong, “PB-LLM: Partially Binarized Large
Language Models”, International Conference on Learning Representations (ICLR), 2024.
[6] Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele
Magno, Xiaojuan Qi, “BiLLM: Pushing the Limit of Post-Training Quantization for LLMs”,
International Conference on Machine Learning (ICML), 2024.
[7] Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu,
Wanxiang Che, “OneBit: Towards Extremely Low-bit Large Language Models”, arXiv preprint
arXiv:2402.11295, 2024.
[8] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang, “ A survey on model compression
for large language models”, arXiv preprint arXiv:2308.07633, 2023.
[9] Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu Sebe, “Binary
neural networks: A survey”, arXiv preprint arXiv:2004.03333, 2020.
[10] Zechun Liu, Wenhan Luo, Baoyuan Wu, Xin Yang, Wei Liu, and Kwang-Ting Cheng, “Bi-
Real Net: Binarizing Deep Network Towards Real-Network Performance”, Proceedings of the
European Conference on Computer Vision (ECCV), 2018.
[11] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi, “XNOR-Net: ImageNet
Classification Using Binary Convolutional Neural Networks”, Proceedings of the European
Conference on Computer Vision (ECCV), 2016.
[12] Zechun Liu, Zhiqiang Shen, Marios Savvides, Kwang-Ting Cheng, “ReActNet: Towards Precise
Binary Neural Network with Generalized Activation Functions”, Proceedings of the European
Conference on Computer Vision (ECCV), 2020.
[13] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad,
Yangyang Shi, Raghuraman Krishnamoorthi, Vikas Chandra, “LLM-QAT: Data-Free Quantiza-
tion Aware Training for Large Language Models”, arXiv preprint arXiv:2305.17888, 2023.
[14] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, Qun Liu, “TernaryBERT:
Distillation-aware Ultra-low Bit BERT”, arXiv preprint arXiv:2009.12812, 2020/
[15] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-
pher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt
Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, “OPT:
Open Pre-trained Transformer Language Models”, arXiv preprint arXiv:2205.01068, 2022.
[16] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,
Armand Joulin, Edouard Grave, Guillaume Lample, “LLaMA: Open and Efficient Foundation
Language Models”, arXiv preprint arXiv:2302.13971, 2023.
[17] Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad
and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and
Bhosale, Shruti and others, “Llama 2: Open Foundation and Fine-Tuned Chat Models”, arXiv
preprint arXiv:2307.09288, 2023.
10
[18] Ilya Loshchilov, Frank Hutter, “Decoupled Weight Decay Regularization”, International Con-
ference on Learning Representations (ICLR), 2019.
[19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina
Toutanova, “BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions”, arXiv
preprint arXiv:1905.10044, 2019.
[20] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, Yejin Choi, “PIQA: Reasoning
about Physical Commonsense in Natural Language”, Proceedings of the AAAI Conference on
Artificial Intelligence (AAAI), 2020.
[21] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, “HellaSwag: Can a
Machine Really Finish Your Sentence?”, arXiv preprint arXiv:1905.07830, 2019.
[22] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi, “WinoGrande: An
Adversarial Winograd Schema Challenge at Scale”, arXiv preprint arXiv:1907.10641, 2019.
[23] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,
Oyvind Tafjord, “Think you have Solved Question Answering? Try ARC, the AI2 Reasoning
Challenge”, arXiv preprint arXiv:1803.05457, 2018.
[24] Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, “Pointer Sentinel Mixture
Models”, arXiv preprint arXiv:1609.07843, 2016.
[25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, Peter J. Liu, “Exploring the Limits of Transfer Learning with a Unified
Text-to-Text Transformer”, arXiv preprint arXiv:1910.10683, 2019.
[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
Jeff Dean, “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
Layer”, arXiv preprint arXiv:1701.06538, 2017.
[27] William Fedus, Barret Zoph, Noam Shazeer, “Switch Transformers: Scaling to Trillion Parame-
ter Models with Simple and Efficient Sparsity”, arXiv preprint arXiv:2101.03961, 2021.
[28] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng
Zhang, Peng Gao, Yu Qiao, Ping Luo, “OmniQuant: Omnidirectionally Calibrated Quantization
for Large Language Models”, International Conference on Learning Representations (ICLR),
2024.
[29] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan
Yang, Ruiping Wang, Yi Wu, Furu Wei, “BitNet: Scaling 1-bit Transformers for Large Language
Models”, arXiv preprint arXiv:2310.11453 , 2023.
[30] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, “QLoRA: Efficient Fine-
tuning of Quantized LLMs”, Advances in Neural Information Processing Systems, (NeurIPS),
2023.
[31] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh
Ashkboos, Alexander Borzunov, Torsten Hoefler, Dan Alistarh, “SpQR: ASparse-Quantized
Representation for Near-Lossless LLM Weight Compression”, arXiv preprint arXiv:2306.03078,
2023.
[32] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael
W. Mahoney, Kurt Keutzer, “SqueezeLLM: Dense-and-Sparse Quantization”, International
Conference on Machine Learning (ICML), 2024.
[33] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De Sa, “QuIP: 2-Bit Quantization of
Large Language Models With Guarantees”, Advances in Neural Information Processing Systems,
(NeurIPS), 2023.
[34] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer,
William Fedus, “ST-MoE: Designing Stable and Transferable Sparse Expert Models”, arXiv
preprint arXiv:2202.08906, 2022.
[35] Subhabrata Mukherjee, Xiaodong Liu, Guoqing Zheng, Saghar Hosseini, Hao Cheng, Greg
Yang, Christopher Meek, Ahmed Hassan Awadallah, Jianfeng Gao, “Few-Shot Learning Eval-
uation in Natural Language Understanding”, Thirty-fifth Conference on Neural Information
Processing Systems Datasets and Benchmarks Track (Round 2)., 2021.
11
A Appendix
A.1 Ablation Study on Datasets
To determine the optimal dataset for training binarized models, we conducted a comparative analysis
using various training datasets, as summarized in Table 5. The results indicate that models trained
solely on the WikiText2 dataset, due to its relatively small dataset size, tend to exhibit overfitting
tendencies and struggle to generalize to other datasets. While these models demonstrate considerable
perplexity improvement on the WikiText2 evaluation, their perplexity on the C4 dataset and zero-shot
accuracy is notably poor. Conversely, models trained exclusively on the C4 dataset perform well
across a wide range of tasks, except for the evaluation on WikiText2. Following the approach of
previous research [13], we also experiment with a generated dataset synthesized using the LLaMA-1-
7B model. Although this dataset generally performs satisfactorily across various language modeling
tasks, its performance lags behind that of the C4 dataset. Therefore, to enhance overall model
performance, we opt to train the models on a mixed dataset comprising both C4 and WikiText2.
Moreover, the accessibility of both C4 and WikiText2 as open-source datasets further facilitates their
adoption for training purposes.
Table 5: Evaluation of binarized LLaMA-1-7B model trained with various training datasets. We train
the model on a subset of the dataset with the same training step. †: Generated dataset synthesized by
LLaMA-1-7B model. ‡: Mixed dataset of Wikitext2 and C4.
Datasets Perplexity ↓ Zero-shot Accuracy ↑
Wiki2 C4 BoolQ PIQA HellaSwag WinoGrande ARC-e ARC-c Avg
Generated † 12.54 13.04 60.51 66.10 45.91 54.69 41.41 27.90 49.42
Wiki2 9.65 28.61 57.95 57.67 36.78 54.45 38.46 26.62 45.32
C4 13.76 11.97 60.33 67.79 49.69 53.74 39.26 29.52 50.06
Mixed ‡ 8.92 11.85 60.51 67.46 49.95 55.24 41.16 29.35 50.61
12
第5个文件的内容为：
Densing Law of LLMs
Chaojun Xiao1, Jie Cai2, Weilin Zhao1, Guoyang Zeng2, Biyuan Lin2, Jie Zhou2, Zhi Zheng2
Xu Han1, Zhiyuan Liu1, Maosong Sun1
1Tsinghua University 2ModelBest Inc.
xiaocj20@mails.tsinghua.edu.cn
{han-xu,liuzy,sms}@tsinghua.edu.cn
Highlights
We introduce the concept of “capability density” to evaluate the training quality of large language
models (LLMs) and describe the trend of LLMs that considers both effectiveness and efficiency.
(Relative) Capability Density. For a given LLM M, its capability density is defined as the ratio
of its effective parameter size to its actual parameter size, where the effective parameter size is
the minimum number of parameters required for the reference model to achieve performance
equivalent to M.
We reveal an empirical law for the capability density of open-source base LLMs released since 2023.
Densing Law. The maximum capability density of LLMs exhibits an exponential growth trend
over time.
ln(ρmax) = At+B
Here, ρmax is the maximum capability density of LLMs at time t.
Figure 1 presents the capability density of popular LLMs, measured by their performance on 5
widely-used benchmarks. A trend is fitted between maximum capability density and release date,
revealing that A ≈ 0.007 with R2 ≈ 0.93. This indicates the maximum capability density of
LLMs doubles approximately every 3.3 months1. That means, around three months, it is possible
to achieve performance comparable to current state-of-the-art LLMs using a model with half the
parameter size.
2023-03 2023-05 2023-07 2023-09 2023-11 2024-01 2024-03 2024-05 2024-07 2024-09
Release Date
10 1
100
Ca
pa
bi
lit
y
De
ns
ity
MPT-30BFalcon-40B
TinyLlama-1.1B
StableLM-Zephyr-3B
Llama-1-7B
Llama-1-13B
Llama-1-65B Llama-2-7B
Llama-2-13B
Llama-2-70B
Llama-3-8B
Llama-3-70B Llama-3.1-minitron-4B
Llama-3.1-8B
Llama-3.1-405B
Llama-3.2-1B
Llama-3.2-3B
Mistral-7B
MiniCPM-1-2.4B
MiniCPM-1-1.2B
MiniCPM-3-4B
Phi-1-1.2B
Phi-1.5-1.2B
Phi-2-2B
Gemma2-27B
Gemma2-9B
Gemma2-2B
Gemma-2B
Gemma-7B
Maximum Capability Density
Trend Line of Maximum Capability Density
ln( )=At+B; Doubling Time = ln(2)/A 3.3 months
Figure 1: The estimated capability density of open-source base LLMs.
1The capability density growth rate is affected by specific evaluation benchmarks and reference models.
ar
X
iv
:2
41
2.
04
31
5v
2
[
cs
.A
I]
6
D
ec
2
02
4
Abstract
Large Language Models (LLMs) have emerged as a milestone in artificial intelli-
gence, and their performance can improve as the model size increases. However,
this scaling brings great challenges to training and inference efficiency, particularly
for deploying LLMs in resource-constrained environments, and the scaling trend
is becoming increasingly unsustainable. This paper introduces the concept of
“capability density” as a new metric to evaluate the quality of the LLMs across
different scales and describes the trend of LLMs in terms of both effectiveness
and efficiency. To calculate the capability density of a given target LLM, we
first introduce a set of reference models and develop a Scaling Law to predict the
downstream performance of these reference models based on their parameter sizes.
We then define the effective parameter size of the target LLM as the parameter size
required by a reference model to achieve equivalent performance, and formalize
the capability density as the ratio of the effective parameter size to the actual
parameter size of the target LLM. Capability density provides a unified framework
for assessing both model effectiveness and efficiency. Our further analysis of recent
open-source base LLMs reveals an empirical law (Densing Law) that the capability
density of LLMs grows exponentially over time. More specifically, using some
widely used benchmarks for evaluation, the capability density of LLMs doubles
approximately every three months. The law provides new perspectives to guide
future LLM development, emphasizing the importance of improving capability
density to achieve optimal results with minimal computational overhead.
1 Introduction
In recent years, large language models (LLMs) have garnered significant attention in the field of
artificial intelligence, demonstrating remarkable improvements across various tasks (Bommasani
et al., 2021; Qiu et al., 2020; Han et al., 2021; Touvron et al., 2023a; OpenAI, 2023). The Scaling
Law for LLMs further reveals that model performance continues to improve as model parameters
and training data increase (Kaplan et al., 2020; Henighan et al., 2020; Hoffmann et al., 2022). This
discovery has led to the development of LLMs with hundreds of billions of parameters, such as GPT-3
175B (Brown et al., 2020), PaLM 540B (Chowdhery et al., 2023), and Llama-3.1-405B (Dubey et al.,
2024), which have demonstrated exceptional capabilities in a wider range of applications.
Besides, with the advancement of LLMs, enhancing inference efficiency has become increasingly
urgent: 1) As LLMs are deployed in an expanding array of scenarios, inference costs have surpassed
training costs, becoming the main bottleneck in practical applications (Sardana et al., 2024; Yun et al.,
2024; OpenAI, 2024a). 2) There is a growing need to deploy LLMs on resource-constrained end
devices like smartphones, serving as personal assistants, which requires models to be more efficient
and compact (Gunter et al., 2024; Xue et al., 2024; Hu et al., 2024). 3) The inference Scaling Law
indicates that allowing LLMs to generate more tokens for "thinking" during the inference stage is
crucial for improving performance in complex reasoning tasks (Brown et al., 2024; OpenAI, 2024b;
Snell et al., 2024), further increasing the demand for efficient inference. To address these challenges,
many efforts have been devoted to developing efficient LLMs with only billions of parameters to
reduce inference overhead, such as OpenAI’s GPT-4o-mini (OpenAI, 2024a) and Apple’s apple
intelligence (Gunter et al., 2024).
Given these two seemingly contradictory paths – scaling up LLMs for effectiveness versus scaling
down LLMs for efficiency – natural questions arise: Can we quantitatively evaluate the quality of
LLMs with different scales? Is there a law that reflects the efficiency trend in LLMs, like the Scaling
Law does for parameter and data scales?
To this end, we introduce the concept of capability density, which serves as a metric for evaluating
and comparing the training quality of LLMs on various scales. Accurately measuring all aspects of an
LLM’s capabilities, or its level of intelligence, is quite challenging. In this article, we design a method
to assess the relative capability density2. Specifically, we use a reference model and then estimate
its scaling function between the performance on downstream tasks and parameter sizes. Based on
2For ease of explanation, in this work, we use “density” to refer to “(relative) capability density”.
2
the scaling function, for any given model, we calculate its effective parameter size – the number of
parameters the reference model would need to achieve equivalent performance. The density of an
LLM relative to the reference model is then defined as the ratio of its effective parameter size to
its actual parameter size. By introducing the concept of model density, we aim to more accurately
measure model quality and enable comparisons between models of different scales. This evaluation
method has the potential to provide new insights into the future direction of LLM development,
helping researchers find the optimal balance between effectiveness and efficiency.
1.1 Key Findings
After defining LLM density, we analyze 29 widely-used open-source pre-trained base models from
recent years. Our key finding for model density is:
Densing Law. The maximum capability density of LLMs exhibits an exponential growth trend
over time.
ln(ρmax) = A · t+B
Here, ρmax is the maximum capability density of LLMs at time t.
Based on our evaluation on 5 widely-used benchmarks, MMLU (Hendrycks et al., 2020), BBH (Suz-
gun et al., 2023), MATH (Hendrycks et al., 2021), HumanEval (Chen et al., 2021), and MBPP (Austin
et al., 2021), A ≈ 0.007, which means the maximum density of LLMs doubles approximately every
three months. For example, MiniCPM-1-2.4B released on February 1st, 2024, can achieve compara-
ble or even superior performance with Mistral-7B released on September 27th, 2023. We can use an
LLM with only 35% parameters to obtain roughly equivalent performance after 4 months. It is worth
noting that using different evaluation benchmarks may result in slight variations in the estimation
and growth rate of model density. We encourage the community to develop more comprehensive
evaluation benchmarks for LLMs to ensure more accurate measurements of density.
Based on the conclusion that the density of LLMs is continuously increasing in an exponential trend,
we can further deduce the following implications:
Corollary 1. Inference Costs Decrease Exponentially: The inference costs are going down
exponentially for LLMs with equivalent downstream performance.
Densing Law indicates that the ratio of effective parameter size to the real parameter size doubles
approximately every three months. Intuitively speaking, in three months, we can achieve perfor-
mance comparable to the current state-of-the-art model using a model with only half the number
of parameters. Thus, the inference costs are going down exponentially for equivalent downstream
performance. We find that from January 2023 to the present, the inference cost of GPT-3.5-level
models has decreased by 266.7 times.
Corollary 2. Densing Law × Moore’s Law: The effective parameter size of LLMs that can run
on chips of the same area increases exponentially.
Moore’s Law (Moore, 1965) states that the number of circuits integrated on a chip of the same area
increases exponentially. This implies an exponential increase in computing power. Densing Law
indicates that the density of LLMs doubles every 3.3 months. Combining these two factors, we
can conclude that the effective parameter size of LLMs that can be run on a chip of the same price
increases faster than both LLMs’ density and computation power of chips.
Corollary 3. Density Growth Accelerated after ChatGPT’s Release: With the release of Chat-
GPT, the growth rate of LLM density increased by 50%.
We compare the increasing trends in LLMs’ density before and after the release of ChatGPT. The
results show that following the release of the ChatGPT model, the growth rate of maximum density
has noticeably accelerated. Specifically, after the release of ChatGPT, the growth rate of LLM density
increased by 50%.
3
Corollary 4. Efficient Compression ̸= Density Improvement: Existing pruning and distillation
methods usually cannot lead to efficient LLMs with higher density.
To enhance model inference efficiency, many researchers have devoted efforts to a series of model
compression algorithms, such as pruning and distillation (Ma et al., 2023; Sun et al., 2024; Yang
et al., 2024; Xu et al., 2024). These algorithms are often believed to improve the performance of
the resulting compressed models. However, by comparing some models with their compressed
counterparts, we can observe that the widely used pruning and distillation methods usually result in
smaller models with lower density than the original models. We encourage the community to further
explore more effective model compression algorithms, with a greater emphasis on improving the
density of smaller models.
Corollary 5. Towards Density-Optimal Training - Green Scaling Law: The development of
LLMs should shift from being performance-centric to being density-centric.
Density is a metric that reflects the trade-off between effectiveness and efficiency. Therefore, blindly
increasing model parameters to pursue performance improvements can lead to lower model density,
resulting in unnecessary energy consumption. For example, while Llama-3.1-405B (Dubey et al.,
2024) achieves state-of-the-art performance among open-source models, it requires computational
resources that are hundreds of times greater than other models. Consequently, model developers need
to shift their focus from merely optimizing performance to optimizing density. This approach aims to
achieve the best results with minimal computational costs, thereby realizing a more sustainable and
environmentally friendly Scaling Law.
In this work, we propose a new evaluation metric, capability density, for LLMs, which can offer a
new, unified perspective on the two current trends – enhancing effectiveness and increasing efficiency.
Based on our proposed metric, we evaluate 29 open-source models and find an empirical experience
law, named Densing Law: the density of LLMs exhibits an exponentially increasing trend. Based
on this empirical relationship, we discuss several deductions and provide observational evidence.
Through this novel evaluation perspective, we hope to provide valuable insights and guidance for the
future development of LLMs.
2 Density for Large Language Models
In this section, we formally define the density for LLMs, which is calculated as the ratio of the
effective parameter size to the actual parameter size. In the following sections, we will first describe
the overall framework and formal definition of LLM density. Then we introduce how to utilize the
Scaling Law to estimate the effective parameter size.
2.1 Overall Framework and Definition
The core of LLM density lies in the effective parameter size, which refers to the number of parameters
required for a reference model to achieve the same performance as a given model. To achieve this,
we need to fit a function that relates the parameter sizes of the reference model to its performance.
Specifically, for a given model M with NM parameters, assume its performance score on the
downstream tasks is SM. This score can be calculated using various metrics depending on the
downstream task, such as accuracy, F1 score, etc. To compute the effective parameter size, we train
a series of reference models with varying scales of parameters and training data. Based on these
models, we fit a function between the parameter size and performance: S = f(N), where S denotes
the downstream performance, and N represents the parameter sizes of the reference model. Then we
can calculate the effective parameter size as N̂(S) = f−1(S) and the density for M is defined as:
ρ(M) =
N̂(SM)
NM
=
f−1(SM)
NM
. (1)
4
It is important to note that Scaling Laws are typically used to fit the relationship between language
modeling loss and parameter sizes (Kaplan et al., 2020), and it is non-trivial to predict downstream
task performance directly. Inspired by Llama-3 (Dubey et al., 2024), we adopt a two-step estimation
approach: (1) Loss Estimation: In the first step, we use a series of reference models to fit the
relationship between the parameter size and language modeling loss on the test set, expressed
as L = f1(N). (2) Performance Estimation: Due to the presence of emergent abilities (Wei
et al., 2022a), it is challenging to accurately estimate the relationship between parameter sizes and
performance using reference models with limited training computes. Therefore, we incorporate
open-source models to compute their loss and performance on the test set and fit the relationship
s = f2(L). This two-step estimation process allows us to derive s = f2(f1(N)). In the following
sections, we will provide a detailed description of the fitting processes for f1(·) and f2(·).
2.2 Loss Estimation
To predict the performance of downstream tasks, the first step involves fitting a function between the
parameter size and language model loss using the Scaling Law widely adopted for LLM pre-training.
Previous Scaling Laws primarily focus on language modeling loss on the whole sequences, which
reflects the model’s ability to estimate the probability of a given corpus. However, instances in the
downstream tasks usually encompass both input instructions and output answers and we are primarily
concerned with the probability of the output answers. Therefore, in this work, we focus on fitting the
conditional loss L = −log(P (answer | instruction)). Concretely, we estimate a power-law function
between the conditional loss L, and parameter size N , as well as the number of training tokens D:
L = aN−α + bD−β , (2)
where a, α, b, and β are parameters need to be fitted.
In previous research on Scaling Laws (Kaplan et al., 2020), the loss typically needs to be specified on
a validation corpus, and the average loss is calculated over all tokens in this corpus. In this work,
our goal is to fit the model’s performance on downstream tasks, which require models to output
the answers based on the input instructions. Therefore, we directly calculate the conditional loss
on downstream tasks, meaning the loss incurred by the model when generating answers given the
task inputs. (1) For multiple-choice problems, calculating the loss solely based on the content of
the correct option can lead to inaccurate estimates, as it ignores the content of incorrect options.
Besides, if we only calculate the loss on the final option labels, the loss for single token is also
unstable. Therefore, we concatenate the problem and its multiple options as inputs, and the output is
the analysis for the input problem as well as the final answer label. (2) For most complex problems,
such as mathematical questions, we often require the model to generate a sequence of reasoning steps
before providing the final answer. For these tasks, when calculating the loss, we include both the
reasoning steps and the correct answer as the output to compute the model’s loss. It is important to
note that most datasets do not provide reasoning steps for each instance. For both two types of tasks,
we use GPT-4o (OpenAI, 2023) to generate reasoning steps for all test instances. These approaches
allow us to better estimate the model’s performance by considering the specific requirements and
formats of different tasks.
2.3 Performance Estimation
In the second step, we need to predict downstream task performance based on the loss on test sets.
In the loss estimation step, the Scaling Law models trained with limited training computes usually
cannot achieve meaningful scores on downstream tasks, with most Scaling Law models performing
only at the level of random guessing. Thus, it is impossible to predict the downstream performance
with only these models. To address this issue, we incorporate well-trained open-source models
for function fitting and calculate their loss and performance on the test set. Considering that the
performance for most downstream tasks is bounded, we use a sigmoid function for fitting. The
sigmoid function naturally maps all input values to the range of 0 to 1. Additionally, when the loss
is particularly large, the model’s performance should approximate that of random guessing, and
when the loss is particularly small, the model’s performance should approach the upper bound. This
characteristic aligns with the properties of the sigmoid function, which is very flat at both extremes
5
of the curve. Specifically, we estimate the downstream performance with the following function:
S =
c
1 + e−γ(L−l)
+ d, (3)
where c, γ, l, and d are parameters need to be estimated.
2.4 Density
After fitting Equation 2 and 3, given the performance SM of a model M, we can infer the effective
parameter size by utilizing the inverse functions of these equations. It is important to note that in
Equation 2, the loss L is a bivariate function of both the parameter count N and the training data size
D. Therefore, when calculating the effective parameter size, it is necessary to specify a particular
training data size D. Here, to calculate the effective parameter size, we defaultly use D = D0 = 1T
tokens. Then the effective parameter size can be explained as the parameter size the reference model
trained with D0 tokens needs to achieve equivalent performance. Concretely, we can compute the
effective parameter size as:
L̂(SM) = l − 1
γ
ln
(
c
SM − d
− 1
)
; N̂(SM) =
(
L̂(SM)− bD−β
0
a
)− 1
α
. (4)
Now, we have established the relationship between the downstream performance and effective
parameter size. The density of the given model M is ρ(M) = N̂(SM)
NM
. Intuitively, if one model can
achieve better performance with the same scale of parameters, then the model’s density is higher.
Therefore, in the future, considering the limited computation resources of deployment devices, we
should devote great effort to improving the model’s density instead of merely increasing the model
parameter scales for better performance.
3 Density Evolution
3.1 Evaluation Settings
Dataset In this work, we adopt the following widely-used datasets for evaluation: MMLU (Hendrycks
et al., 2020) for English knowledge-intensive tasks, BBH (Suzgun et al., 2023) for challenging
logic reasoning tasks, MATH (Hendrycks et al., 2021) for mathematical reasoning tasks, and Hu-
manEval (Chen et al., 2021), MBPP (Austin et al., 2021) for coding tasks. We apply the open-source
tools (OpenCompass, 2023; Liu et al., 2024) for evaluation. Here, we evaluate all models in a few-shot
in-context learning manner and these models are required to generate the final answer label based
on the given demonstrations and inputs of test instances. Following widely-used settings, MMLU,
BBH, MATH, HumanEval, and MBPP are evaluated under the 5-shot, 3-shot, 4-shot, 0-shot, and
3-shot settings, respectively. Besides, for BBH, MATH, and MBPP, we adopt the chain-of-thought
prompting technique (Wei et al., 2022b).
Loss Estimation Models In the loss estimation step, we need to run a series of models with different
scales of parameters and training data. These models will be used as the reference models for further
density computation. In this work, we adopt the training corpus of MiniCPM-3-4B (Hu et al., 2024),
Table 1: The detailed hyper-parameters of small models trained for loss estimation.
Name # Para BS nlayer d dffn dhead nhead nkv
0.005B 5,247,232 32 8 256 640 64 4 1
0.03B 31,470,080 32 12 512 1,280 64 8 2
0.1B 106,196,736 64 18 768 1,920 64 12 3
0.2B 245,416,960 128 24 1,024 2,560 64 16 2
0.4B 476,852,480 256 30 1,280 3,200 64 20 2
0.8B 828,225,024 512 36 1,536 3,840 64 24 3
6
1016 1017 1018 1019 1020
Compute (6ND)
0.3
0.4
0.5
0.6
0.7
0.8
Lo
ss
MMLU
1016 1017 1018 1019 1020
Compute (6ND)
0.2
0.3
0.4
0.5
0.6
0.7
BBH
1016 1017 1018 1019 1020
Compute (6ND)
0.3
0.4
0.5
0.6
0.7
0.8
0.9
MATH
1016 1017 1018 1019 1020
Compute (6ND)
0.4
0.6
0.8
1.0
1.2
Lo
ss
MBPP
1016 1017 1018 1019 1020
Compute (6ND)
0.2
0.3
0.4
0.5
0.6
0.7
HUMANEVAL
1016 1017 1018 1019 1020
Compute (6ND)
0.3
0.4
0.5
0.6
0.7
0.8
0.9
AVERAGE
0.005B
0.03B
0.1B
0.2B
0.5B
0.8B
(a) Loss Estimation
0.150.200.250.300.35
Loss
20
40
60
80
Pe
rfo
rm
an
ce
MMLU
0.100.150.200.25
Loss
20
40
60
80
BBH
0.150.200.250.300.350.400.45
Loss
0
20
40
60
80
MATH
0.080.100.120.140.160.180.200.22
Loss
0
20
40
60
80
Pe
rfo
rm
an
ce
HUMANEVAL
0.100.150.200.250.300.350.400.45
Loss
0
20
40
60
80
100
MBPP
0.150.200.250.300.35
Loss
20
40
60
80
AVERAGE
(b) Performance Estimation
Figure 2: The results for loss estimation and performance estimation. Here, the lines are fitted
curves. X-axis in (a) refers to the pre-training compute, which is approximated by Compute = 6ND.
Triangles in (b) are larger models for prediction.
a widely-used edge-size model, to train the small models. As for the model architecture, we use
grouped query attention (Ainslie et al., 2023), gated feedforward layers with SiLU as the activation
function. We train the models using Warmup-Stable-Decay learning rate scheduler. To estimate the
scaling curve, we train the models with {10, 15, 20, 30, 40, 60} ×N tokens, where N refers to the
parameter size. We list the hyper-parameters for small scaling models in Table 1.
Performance Estimation Models In the performance estimation step, we introduce additional
well-trained models to fit the loss-performance curve. Specifically, we use a series well-trained
MiniCPM-3 models and their intermediate training checkpoints. Their parameter scales range from
0.5 billion to tens of billion. These models use the same vocabulary as our scaling models with
different parameter sizes and training datasets.
Evaluated Models Furthermore, to illustrate the change in density over time, we select widely used
LLMs for evaluation since the release of Llama-1 (Touvron et al., 2023a), as most open-source
models released before Llama-1 cannot achieve meaningful performance on our selected datasets.
Specifically, we evaluate the density of the following models: Llama series of models (Touvron
et al., 2023a,b; Dubey et al., 2024), Falcon (Almazrouei et al., 2023), MPT (Team, 2023), Phi
7
series of models (Gunasekar et al., 2023; Li et al., 2023; Abdin et al., 2024), Mistral (Jiang et al.,
2023), StableLM (Bellagente et al., 2024), TinyLlama (Zhang et al., 2024), and MiniCPM series of
models (Hu et al., 2024). We prioritize using the results reported in each model’s technical reports
for density calculations. Besides, we only evaluate the density of base pre-trained models without
instruction tuning as the instruct-tuning datasets may contain human-annotated data similar to our
selected test data leading to inaccurate density estimation. Notably, many pre-trained models also
introduce supervised finetuning datasets in the pre-training phase, leading to the test set contamination
issue (Wei et al., 2023; Dominguez-Olmedo et al., 2024). Thus, the inaccurate density estimation
remains to be solved, which we leave for future work.
Notably, we only evaluate the density of pre-trained base models without further supervised fine-
tuning and preference learning, due to the following reasons: (1) Pre-trained base models serve as the
foundation for model performance. Considering the impact of further alignments, such as the quality
of human annotations and the choice of alignment algorithms, introduces excessive confounding
factors unrelated to the capabilities of the base model itself. (2) The Scaling Law for the performance
of LLMs with alignment remains an open question that requires further exploration. Nowadays, there
are numerous methods to improve the performance during inference time, such as retrieval-augmented
generation (Lewis et al., 2020), and thinking more for inference Scaling Law (OpenAI, 2024b). Here,
we only consider the basic prompting technique for base LLM evaluation, as this technique cannot
consistently improve the performance of this base model. And we leave the density calculation for
different inference FLOPs for future work, which may lead to inference Densing Law.
3.2 Loss and Performance Estimation Results
We present the estimation results of the two-step process in Figure 2. From the results, we can observe
that the two-step estimation process can effectively fit the performance of different-sized models
on three downstream tasks. With the decrease in the loss on the test instances, the performance
significantly improves as a sigmoidal curve, and the loss has a power-law relationship with the number
of parameters and training tokens.
To evaluate the effectiveness of our estimated method, we use models with parameters of less than
4 billion to fit the loss-performance curve and preserve larger models for prediction. Triangles in
Figure 2(b) are two models with tens of billions of parameters. From the results, we can observe that
we effectively predict the downstream performance based on the loss values.
3.3 Densing Law
After fitting the loss scaling curve and the performance scaling curve, we further measured the density
of widely used open-source models since the release of Llama-1 (Touvron et al., 2023a). We present
the density of each model along with their release dates in Figure 1. From the figure, we can observe
that: (1) The density of LLMs has rapidly increased over time. Notably, the density of Llama-1,
released in February 2023, is below 0.1, whereas more recently released models like Gemma-2-9B
and MiniCPM-3-4B have densities reach 3. This increase in density is largely attributed to the growth
in the scale of pre-training data and improvements in the quality of that data. For example, Llama-1
is pre-trained on 1.4 trillion tokens, whereas Llama-3 utilizes 15 trillion tokens with careful data
cleaning. (2) Better performance does not always lead to better density. Llama-3.1-405B is currently
one of the state-of-the-art open-source models due to its large-scale parameters. However, it is not
the model with the highest density. This is because constrained by computational resources and the
scale of pre-training data, we usually cannot fully optimize the training settings for extremely large
models, making them sub-optimal in terms of cost-effectiveness.
To further illustrate the growth trend of the LLMs’ density, we perform a linear fit on the envelope
line in Figure 1. Specifically, we assume that the logarithmic value of the maximum density increases
linearly over time. Formally, we fit the following linear function:
ln(ρmax) = A · t+B, (5)
where t is the time interval (unit: days) since the release date of Llama-1, ρ is the maximum
density value at time t, and A,B are the parameters to be fitted. Through the fitting process, we
8
obtained A ≈ 0.0073, which implies that the density of the large model doubles approximately every
ln(2)
A ≈ 95 days. Here, the R2 for the linear regression function is 0.912.
The growth trend in model density reveals an important pattern in the development of current LLMs.
While Scaling Laws indicate that model performance improves with an increase in parameter size, the
parameter scale growth is constrained by the limited computation resources available in deployment
scenarios and the demand for fast response. As a result, large models are not simply evolving
towards larger parameter sizes. Instead, developers of LLMs are striving for higher cost-effectiveness,
aiming to achieve optimal performance with minimal inference costs. This discovery aligns with
the principles discovered by Moore’s Law in the development of integrated circuit chips (Moore,
1965), which emphasize increasing transistor density on a limited chip area. Therefore, we name our
discovery on the growth trend in model density as Densing Law.
3.4 Corollaries of Densing Law
Based on Densing Law and our evaluation results, in this section, we discuss several corollaries and
hope our discovery can promote the development of LLMs.
Inference Costs Decrease Exponentially The density of LLMs shows an exponential growth trend,
doubling approximately every three months. Here, density is defined as the ratio of the effective
parameter size to the actual parameter size. This implies that in three months, we can achieve
performance comparable to current models using only half the actual parameter size. Consequently,
under the condition of achieving the same performance, the actual parameter size of LLMs will also
decrease exponentially. This reduction in actual parameter count translates to decreased computational
costs during inference. Therefore, the exponential increase in LLMs’ density will directly result in an
exponential decrease in inference costs for models achieving the same level of performance.
2023-01
2023-04
2023-07
2023-10
2024-01
2024-04
2024-07
2024-10
Release Date
10 1
100
101
Pr
ice
(D
ol
la
r/M
illi
on
To
ke
ns
)
GPT-3.5
GPT-4
GPT-4-Turbo
GPT-4o-mini
GPT-4o
Claude-2
Claude-3-haiku
Claude-3.5-haiku
Gemini-1.5-Flash
Llama-3.1-405B
Figure 3: Prices of LLMs that can outperform
GPT-3.5. The line connects the cheapest models.
To better illustrate the decreasing trend in in-
ference costs for LLMs, we present the API
pricing of LLMs that have achieved superior
performance to GPT-3.5 since its release in Fig-
ure 3. From the figure, we can observe that the
prices of LLMs exhibit an exponential decline.
Specifically, in December 2022, GPT-3.5 cost
$20 for one million tokens, whereas by August
2024, Gemini-1.5-Flash costs only $0.075 for
the same number of tokens, a reduction of 266.7
times. Roughly speaking, the inference costs for
LLMs halve approximately every 2.6 months.
The exponentially decreasing trend of LLM API
pricing is also observed in Appenzeller (2024).
In addition, we can observe that the rate of de-
cline in inference costs is faster than the growth
rate of LLMs’ density. This is because infer-
ence costs depend not only on the actual param-
eter size but also heavily on the inference infras-
tructure. In recent years, inference systems for
LLMs have garnered significant attention from
researchers, including optimizations in mem-
ory access speed for self-attention layers (Kwon
et al., 2023; Dao et al., 2022; Dao, 2023) and sparse computation optimizations for feed-forward
networks (Song et al., 2023; Liu et al., 2023). These advancements have greatly contributed to the
reduction in inference costs for LLMs.
Densing Law Meets Moore’s Law Densing Law describes the exponential trend of increasing model
density over time, focusing on improvements at the algorithmic level of LLMs. On the other hand,
Moore’s Law, which states that computing power increases exponentially, highlights advancements
in hardware technology (Moore, 1965). The combination of these two principles suggests a rapidly
approaching future where high-quality LLMs can run efficiently on consumer-grade devices, such
9
2020-07 2021-01 2021-07 2022-01 2022-07 2023-01 2023-07 2024-01 2024-07
Release Date
10 2
10 1
100
Ca
pa
bi
lit
y
De
ns
ity
GPT-NeoX-20BPaLM-8B
PaLM-62B
PaLM-540B
Chinchilla 70B
GPT-3-175B
Gopher-280B
OPT-66B
MPT-30B
Falcon-40B
TinyLlama-1.1B
StableLM-Zephyr-3B
Llama-1-7B
Llama-1-13B
Llama-1-65B
Llama-2-7B
Llama-2-13B
Llama-2-70B
Llama-3-8B
Llama-3-70B
Llama-3.1-minitron-4B
Llama-3.1-8B
Llama-3.1-405B
Llama-3.2-1B
Llama-3.2-3B
Mistral-7B
MiniCPM-1-2.4B
MiniCPM-1-1.2B
MiniCPM-3-4B
Phi-1-1.2B
Phi-1.5-1.2B
Phi-2-2B
Gemma2-27B
Gemma2-9B
Gemma2-2B
Gemma-2B
Gemma-7B
Release of ChatGPT
Trend Line (Before ChatGPT); A 0.0048
Trend Line (After ChatGPT); A 0.0073
Figure 4: Density evaluated using MMLU. Two trend lines represent the growth of LLMs’ density
before and after the release of ChatGPT.
as smartphones and PCs, with low power consumption. This convergence of algorithmic efficiency
and hardware capability is paving the way for more accessible and widespread use of advanced AI
technologies in everyday devices.
Specifically, recent observations (Hobbhahn et al., 2023) found that the computation power of
chips with the same price doubles approximately every 2.1 years. Densing Law indicates that
the ratio between the effective parameter size and the actual parameter size doubles every three
months. Therefore, given a fixed chip price, the effective parameter size of the largest LLM that
can run on it grows exponentially. This growth rate is the product of the growth rate of model
density and the growth rate of transistor density on the chip. Based on current estimates, this
implies that the maximum effective parameter size approximately doubles every 88 days. This rapid
growth highlights the combined impact of advancements in both algorithmic efficiency and hardware
technology, suggesting a future where increasingly powerful models can be deployed on existing
hardware much more quickly than previously anticipated.
Density Growth Accelerated after ChatGPT’s Release In 2022, ChatGPT achieved great perfor-
mance improvements across various tasks and its zero-shot generalization ability spurred significant
efforts from both industry and academia to advance the development of LLMs. To illustrate the
change in the trend of model density growth before and after the release of ChatGPT, we evaluate the
densities of typical LLMs since the release of GPT-3. We use the MMLU benchmark to capture the
changes in density. The results are presented in Figure 4.
From the figure, we can observe that the rate of increase in model density significantly accelerated
following the release of ChatGPT. Before ChatGPT, the slope of the trend line was approximately
A ≈ 0.0048, whereas after its release, it increased to A ≈ 0.0073, indicating a 50% faster growth
rate in model density. Several factors contribute to this accelerated growth: (1) Increased investment:
The success of ChatGPT highlighted the potential of LLMs, leading to a significant increase in
investment directed towards LLM development. (2) More high-quality open-source models: The rise
in high-quality open-source models has lowered the barriers to research and development in LLMs.
After ChatGPT’s release, there was a notable increase in high-quality small LLMs with only billions
of parameters, whose accessibility allows many researchers to conduct LLM research using relatively
small GPU clusters. Therefore, we encourage the community to open-source their cutting-edge
algorithms and models, which can significantly contribute to density improvement.
Efficient Compression ̸= Density Improvement LLMs are often constrained by high inference
costs, making it challenging to run them on consumer devices. To address this issue, many developers
employ pruning and distillation techniques to compress LLMs. In Figure 5, we also present the
densities of several compressed models. For instance, Llama-3.2-3B/1B and Llama-3.1-minitron-
4B (Muralidharan et al., 2024) are derived from pruning and distilling Llama-3.1-8B (Dubey et al.,
2024), while Gemma-2-9B/2B is distilled from Gemma-2-27B (Team et al., 2024).
10
Figure 5: Comparison between compressed mod-
els and their larger counterparts.
The results show that only the Gemma-2-9B
model has a higher density than the original
model, whereas all other compressed models
have lower densities compared to their original
counterparts. Intuitively, pruning involves re-
moving unimportant neurons from LLMs, which
suggests that these neurons might store less
knowledge than other neurons. This would im-
ply that compressed models should intuitively
achieve higher density. However, the results are
quite the opposite. This discrepancy might be
due to the insufficient training of smaller mod-
els during the compression process, preventing
them from reaching optimal density. Therefore,
we encourage the community to address this
challenge by ensuring that compressed models
are adequately trained during future efforts.
Towards Density-Optimal Training - Green
Scaling Law Since the release of GPT-3 (Brown et al., 2020) and the introduction of the Scaling
Law (Kaplan et al., 2020), many researchers focus on training language models with extremely
large parameter sizes to continuously enhance model performance. Guided by this trend, PaLM-
540B (Chowdhery et al., 2023) and Gopher-280B (Rae et al., 2021) achieve great improvements
on various natural language processing tasks. Given the constraints of pre-training computational
resources, maximizing the use of pre-training clusters to develop training compute-optimal LLMs has
become a key focus (Hoffmann et al., 2022). Furthermore, inference compute costs have surpassed
training compute costs as a major concern, leading to a shift towards pre-training smaller models
using increasingly large-scale training data (Hu et al., 2024; Gunter et al., 2024).
In light of the discovery of the Densing Law, we now encourage a shift towards density-optimal LLM
pre-training. With the continuous efforts in LLM development worldwide, model density is rapidly
increasing, resulting in shorter lifecycles for each model. Simply increasing the scale of pre-training
corpora for LLMs can lead to longer development cycles and higher training costs. However, shortly
after a model is released, it is expected that a new model with comparable performance and lower
inference costs will be available in three months. In this context, LLM developers must consider
the growth trend of model density and adopt more efficient and generalized training techniques
to enhance model density. This approach helps avoid excessive cost investments and the losses
associated with short profit recovery cycles.
4 Discussion
Accurate Capability Measurement Capability density reflects the abilities of an LLM per unit of
parameters. However, with current technology, we cannot accurately assess the absolute capability
level of LLMs, meaning that quantifying intelligence remains a great challenge. Therefore, in this
work, we design a method to measure the relative density value of LLMs. Besides, we use widely-
used benchmarks to evaluate the performance of LLMs. However, the limited number of benchmarks
and potential data contamination issues introduce bias in performance evaluation. Thus, advancing
accurate measurement of LLMs’ capabilities or intelligence levels in the future will enable better
calculation of their density.
Connection between Densing Law and Scaling Law The Scaling Law of LLMs reveals the
relationship between an LLM’s performance and its parameter and data sizes, reflecting the intrinsic
characteristics of complex systems composed of vast numbers of neurons. The Densing Law further
highlights the trend in the development of LLMs’ efficiency and effectiveness over time, marking
a technological advancement trend as humanity pursues high-level AI models. Formally, under
conditions of sufficient training data, the Scaling Law explains the relationship between model loss
and parameter size as: L = AN−α, which is appropriate for the training of all Transformer-based
models. Furthermore, the Densing Law indicates that developers of LLMs can increase α through
continuous improvements in data, algorithms, and architecture, thereby reducing the model loss for a
given parameter size.
11
Period of Validity of Densing Law Densing Law reveals the rapid development of LLM algorithms.
In this paragraph, we discuss the question: how long this exponential growth in model density will
continue?. We believe that the rapid increase in model density is driven by significant investments in
personnel and resources. The improvement of general intelligence capabilities in LLMs can bring
substantial benefits to various industries, further encouraging investment in model research and
development. Given the great potential of LLMs, we believe that Densing Law will remain effective
for a considerable period. However, it is essential to continually update the evaluation datasets used to
evaluate model density, as LLMs will soon achieve satisfactory performance on existing datasets. In
the event of achieving artificial general intelligence, LLMs themselves may be capable of conducting
scientific research autonomously, exploring new pathways to further increase density. At that point,
the growth in LLM density could accelerate even more, driven by the models’ ability to innovate and
optimize their own development processes.
5 Limitations and Future Directions
In this section, we discuss the limitations and future directions of our proposed method to evaluate
the capability density of LLMs.
Fair and Comprehensive Evaluation The capability density measurement of LLMs relies on existing
benchmarks to evaluate model performance. Therefore, the benchmark quality greatly impacts the
density measurement results. In this work, we use those benchmarks widely adopted by researchers
to evaluate various LLMs. However, several challenges remain: (1) Comprehensive evaluation: With
the development of LLMs, the capabilities of LLMs significantly expand, such as the ability to
handle complex reasoning tasks (OpenAI, 2024b). Consequently, the capability density measurement
needs to be continually updated by incorporating more comprehensive evaluation datasets that reflect
evolving capabilities. (2) Fair evaluation: With the increasing scale of pre-training data and the
construction of synthetic data, some LLMs are overoptimized towards benchmarks, leading to inflated
scores. To address this, we plan to use newly constructed datasets to evaluate model performance,
thereby mitigating the overfitting risk and ensuring accurate density estimation.
Multi-modal Density In this work, we focus on measuring the capability density of language models.
However, measuring the density and trends in large multimodal models is also crucial as multimodal
applications increase. In the future, designing reasonable density evaluation methods for multimodal
models will be an important research direction.
Inference Densing Law Recent research has highlighted that more inference computational costs
allow LLMs to engage in deeper reasoning, effectively enhancing their performance on complex
tasks (OpenAI, 2024b). In this work, we use the parameter size as the basis to evaluate model
capability density. However, as the importance of chain-of-thought reasoning continues to grow,
density evaluation should shift towards being based on inference FLOPs. Specifically, capability
density could be formalized as the ratio of effective inference FLOPs to actual inference FLOPs. In
this way, we hope that LLMs achieve optimal results with the minimum number of reasoning steps.
6 Conclusion
To illustrate the recent trend towards efficient LLMs and to quantitatively measure the training quality
of LLMs, this paper introduces a method for evaluating the capability density of LLMs. By measuring
the capability density of open-source base LLMs released since 2023, we show an empirical law:
the capability density of LLMs increases exponentially over time. The evaluation results on some
widely-used LLM benchmarks indicate that the density of LLMs doubles every three months. This
implies that, within three months, a model with only half the parameters can achieve performance
comparable to the current state-of-the-art models. This finding highlights the rapid development and
increasing efficiency of LLMs. We discuss several corollaries based on the law, and hope that the
law and its corollaries will encourage the LLM community to continue enhancing model capability
density and achieving optimal performance with minimal computational costs.
12
References
Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach,
Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: A highly
capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024.
Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit
Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints.
In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,
pp. 4895–4901, 2023.
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojo-
caru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al.
The falcon series of open language models. arXiv preprint arXiv:2311.16867, 2023.
Guido Appenzeller. Welcome to llmflation – llm inference cost is going down fast. Blog, 2024. URL
https://a16z.com/llmflation-llm-inference-cost/.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language
models. arXiv preprint arXiv:2108.07732, 2021.
Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth
Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable lm 2 1.6 b
technical report. arXiv preprint arXiv:2402.17834, 2024.
Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,
Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,
Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen
Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus,
Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale,
Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori
Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang,
Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling,
Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi,
and et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258, 2021.
Bradley C. A. Brown, Jordan Juravsky, Ryan Saul Ehrlich, Ronald Clark, Quoc V. Le, Christopher
Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated
sampling. CoRR, abs/2407.21787, 2024. doi: 10.48550/ARXIV.2407.21787. URL https:
//doi.org/10.48550/arXiv.2407.21787.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. Language models are few-shot learners. In Proceedings of NeurIPS,
2020.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1–113,
2023.
Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv
preprint arXiv:2307.08691, 2023.
13
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-
efficient exact attention with io-awareness. Advances in Neural Information Processing Systems,
35:16344–16359, 2022.
Ricardo Dominguez-Olmedo, Florian E Dorner, and Moritz Hardt. Training on the test task confounds
evaluation and emergence. arXiv preprint arXiv:2407.07890, 2024.
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha
Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.
arXiv preprint arXiv:2407.21783, 2024.
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth
Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all
you need. arXiv preprint arXiv:2306.11644, 2023.
Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen
Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, et al. Apple intelligence foundation language
models. CoRR, abs/2407.21075, 2024. doi: 10.48550/ARXIV.2407.21075. URL https://doi.
org/10.48550/arXiv.2407.21075.
Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao,
Ao Zhang, Liang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Yang Liu, Zhiyuan
Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan, Wayne Xin Zhao,
and Jun Zhu. Pre-trained models: Past, present and future. AI Open, 2:225–250, 2021.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt. Measuring massive multitask language understanding. In International Conference on
Learning Representations, 2020.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn
Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In
Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track
(Round 2), 2021.
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo
Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford,
Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCan-
dlish. Scaling laws for autoregressive generative modeling. CoRR, abs/2010.14701, 2020. URL
https://arxiv.org/abs/2010.14701.
Marius Hobbhahn, Lennart Heim, and Gökçe Aydos. Trends in machine learning hardware,
2023. URL https://epoch.ai/blog/trends-in-machine-learning-hardware. Ac-
cessed: 2024-12-05.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom
Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia
Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent
Sifre. Training compute-optimal large language models. CoRR, abs/2203.15556, 2022. doi:
10.48550/ARXIV.2203.15556. URL https://doi.org/10.48550/arXiv.2203.15556.
Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang,
Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models
with scalable training strategies. CoRR, abs/2404.06395, 2024. doi: 10.48550/ARXIV.2404.06395.
URL https://doi.org/10.48550/arXiv.2404.06395.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,
Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.
Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.
14
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph
Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model
serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems
Principles, pp. 611–626, 2023.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented genera-
tion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:
9459–9474, 2020.
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.
Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.
Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by
chatgpt really correct? rigorous evaluation of large language models for code generation. Advances
in Neural Information Processing Systems, 36, 2024.
Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava,
Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms
at inference time. In International Conference on Machine Learning, pp. 22137–22176. PMLR,
2023.
Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large
language models. Advances in neural information processing systems, 36:21702–21720, 2023.
Gordon E Moore. Cramming more components onto integrated circuits. Electronics, 1965.
Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa
Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Compact
language models via pruning and knowledge distillation. arXiv preprint arXiv:2407.14679, 2024.
OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.
OpenAI. Learning to reason with llms. Technical Report, 2024a. URL https://openai.com/
index/gpt-4o-mini-advancing-cost-efficient-intelligence/.
OpenAI. Gpt-4o mini: advancing cost-efficient intelligence. Technical Report, 2024b. URL
https://openai.com/index/learning-to-reason-with-llms/.
OpenCompass. Opencompass: A universal evaluation platform for foundation models. https:
//github.com/open-compass/opencompass, 2023.
Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. Pre-trained
models for natural language processing: A survey. CoRR, abs/2003.08271, 2020.
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:
Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.
Nikhil Sardana, Jacob Portes, Sasha Doubov, and Jonathan Frankle. Beyond chinchilla-optimal:
Accounting for inference in language model scaling laws. In Forty-first International Conference
on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024.
URL https://openreview.net/forum?id=0bmXrtTDUu.
Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally
can be more effective than scaling model parameters. CoRR, abs/2408.03314, 2024. doi: 10.48550/
ARXIV.2408.03314. URL https://doi.org/10.48550/arXiv.2408.03314.
Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. Powerinfer: Fast large language model serving
with a consumer-grade gpu. arXiv preprint arXiv:2312.12456, 2023.
Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach
for large language models. In The Twelfth International Conference on Learning Representations,
2024.
15
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and
whether chain-of-thought can solve them. In Findings of the Association for Computational
Linguistics: ACL 2023, pp. 13003–13051, 2023.
Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya
Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al.
Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118,
2024.
MosaicML NLP Team. Introducing mpt-30b: Raising the bar for open-source foundation models,
2023. URL www.mosaicml.com/blog/mpt-30b. Accessed: 2023-06-22.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand
Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language
models. CoRR, abs/2302.13971, 2023a.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.
Transactions on Machine Learning Research, 2022a.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in
neural information processing systems, 35:24824–24837, 2022b.
Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng,
Weiwei Lü, Rui Hu, et al. Skywork: A more open bilingual foundation model. arXiv preprint
arXiv:2310.19341, 2023.
Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao,
and Tianyi Zhou. A survey on knowledge distillation of large language models. arXiv preprint
arXiv:2402.13116, 2024.
Zhenliang Xue, Yixin Song, Zeyu Mi, Le Chen, Yubin Xia, and Haibo Chen. Powerinfer-2: Fast
large language model inference on a smartphone. CoRR, abs/2406.06282, 2024. doi: 10.48550/
ARXIV.2406.06282. URL https://doi.org/10.48550/arXiv.2406.06282.
Chuanpeng Yang, Yao Zhu, Wang Lu, Yidong Wang, Qian Chen, Chenlong Gao, Bingjie Yan, and
Yiqiang Chen. Survey on knowledge distillation for large language models: Methods, evaluation,
and application. ACM Transactions on Intelligent Systems and Technology, 2024.
Longfei Yun, Yonghao Zhuang, Yao Fu, Eric P. Xing, and Hao Zhang. Toward inference-optimal
mixture-of-expert large language models. CoRR, abs/2404.02852, 2024. doi: 10.48550/ARXIV.
2404.02852. URL https://doi.org/10.48550/arXiv.2404.02852.
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small
language model. arXiv preprint arXiv:2401.02385, 2024.
16
第6个文件的内容为：
OneBit: Towards Extremely Low-bit
Large Language Models
Yuzhuang Xu1 Xu Han1 Zonghan Yang1 Shuo Wang1
Qingfu Zhu2 Zhiyuan Liu1 Weidong Liu1 Wanxiang Che2,B
1Department of Computer Science & Technology, Tsinghua University, Beijing, China
2Research Center for Social Computing and Information Retrieval,
Harbin Institute of Technology, Harbin, China
xyz21thu@gmail.com, car@ir.hit.edu.cn
Abstract
Model quantification uses low bit-width values to represent the weight matrices
of existing models to be quantized, which is a promising approach to reduce
both storage and computational overheads of deploying highly anticipated LLMs.
However, current quantization methods suffer severe performance degradation
when the bit-width is extremely reduced, and thus focus on utilizing 4-bit or 8-bit
values to quantize models. This paper boldly quantizes the weight matrices of
LLMs to 1-bit, paving the way for the extremely low bit-width deployment of
LLMs. For this target, we introduce a 1-bit model compressing framework named
OneBit, including a novel 1-bit parameter representation method to better quantize
LLMs as well as an effective parameter initialization method based on matrix
decomposition to improve the convergence speed of the quantization framework.
Sufficient experimental results indicate that OneBit achieves good performance
(at least 81% of the non-quantized performance on LLaMA models) with robust
training processes when only using 1-bit weight matrices. Code and checkpoints
are available at https://github.com/xuyuzhuang11/OneBit
1 Introduction
Transformer [36] has emerged as the pivotal architecture in large language models (LLMs), fun-
damentally reshaping the approach to natural language processing in deep learning era [6, 34, 4].
Despite their popularity, deploying transformer-based LLMs presents significant challenges due to
their computational intensity and considerable memory requirements as the parameters of LLMs
become more and more. For instance, even moderately-sized LLMs like LLaMA-13B [34] require
around 26GB of memory to load its all parameters in FP16 format. Such overheads make deploying
LLMs difficult beyond mid-to-high-end GPUs like the A100, let alone on mobile devices. The high
demand for resources not only drives up usage costs, but also restricts their wider application.
Numerous efforts [10, 14, 13] have been devoted to reducing the computational and memory over-
heads of LLMs, while still preserving most of their original model capabilities. Among these efforts,
quantization has gained widespread attention, particularly Post-Training Quantization (PTQ), benefit-
ted from its lower transferring costs. Seminal studies such as GPTQ [14], SpQR [12], and AWQ [20]
successfully compress the weight matrices of LLMs to 4-bit values while maintaining the main
abilities of LLMs. Efficient quantization represents significant advances in LLM optimization, by
achieving a balance between time and space efficiency as well as model performance.
Unfortunately, the efficacy of PTQ rapidly diminishes when the quantization bit-width is extremely
low, as shown in Figure 1. Existing PTQ methods managed to compress weight matrices down
Preprint. Under review.
ar
X
iv
:2
40
2.
11
29
5v
3
[
cs
.C
L
]
2
2
M
ay
2
02
4
to at least 3-bit [9]. Recent researches hope to leverage Quantization-Aware Training (QAT) to
overcome the bottlenecks faced by PTQ. LLM-QAT [21] introduces a few learnable parameters into
16 8 4 2 1
# weight bits
6
8
10
12
14
16
Pe
rp
le
xi
ty
Ours
GPTQ
LLM-QAT
OmniQuant
Figure 1: The perplexity (lower scores mean better
performance) of existing widely-used low-bit quan-
tization methods on LLaMA-7B, reported on Wiki-
text2 [23]. All the examined previous approaches suf-
fer from significant performance degradation when
quantizing models to 2-bit values. Our 1-bit quanti-
zation method can outperform these 2-bit baselines.
the quantization process, achieving notable re-
sults. OmniQuant [30], integrating learnable
equivalent transformation, presents promis-
ing results in 2-bit quantization. However,
existing methods decline when compressing
model weights to 1 bit, struggling to main-
tain effectiveness. This mainly stems from
the drastic precision loss at extremely low bit-
width representation in weight matrix W, sig-
nificantly increasing loss in linear projection
WX, which is the core operator within LLMs.
In this paper, we propose a novel Linear
layer and Sign-Value-Independent Decompo-
sition (SVID) for weight matrices to repre-
sent LLMs using approximately 1-bit values.
In our novel layer architecture, each original
high-bit weight matrix is represented as one
sign matrix (±1) and two value vectors. The
value vectors provide necessary floating-point
precision in linear projection at little cost and
help the model to be trained easily. The sign
matrix maintains the high rank of the original weight matrix with a small space cost, thereby pre-
serving high information capacity. SVID offers a better parameter initialization for 1-bit models
from the non-quantized model and we employ quantization-aware knowledge distillation to transfer
the capabilities of the original model to the proposed 1-bit counterpart. Experiments demonstrate
that our method performs well at the W1A16 (1-bit weight and 16-bit activation) quantization level.
Furthermore, our 1-bit model is more amenable to training and knowledge transfer than previous
works. In summary, our contributions are 3-fold:
• We propose a novel and efficient 1-bit model architecture for LLMs, which can improve
both the time and space efficiency during model inference. Moreover, our architecture is
more stable during quantizing LLMs.
• We propose SVID to decompose high-bit matrices into low-bit ones, which is essential for
the initialization of our 1-bit architecture. Experiments demonstrate that the SVID-based
initialization can improve the model performance and convergence speed.
• Extensive experiments demonstrate that our method works well in model sizes from 1.3B to
13B in OPT, LLaMA, and LLaMA2, showcasing its generalizability.
2 Related Work
2.1 Large Language Model Compression
Quantization, pruning, and knowledge distillation (KD) are the mainstream methods for model
compression. Quantization compresses model weights into low-bit values [14, 20, 11]. For data type
alignment in computation and reducing memory, it also involves quantizing activation [10, 39] and
key-value cache [30]. Pruning simplifies model complexity by removing unimportant weights or
modules, thereby sparsifying the original larger models [13, 31, 22]. KD trains a smaller student
model under the guidance of a larger teacher model [16, 1, 16], achieving the purpose of compressing
the larger one. Beyond these methods, low-rank factorization approximates the original weight
matrix W with the product of two lower-rank matrices [40] and also achieves promising results. Our
work belongs to quantization, using KD for knowledge transfer from the original LLM and uniquely
focusing on extremely low bit-width quantization. More details about model compression can refer
to existing survies [37, 43].
2
2.2 Large Language Model Quantization
Since this paper aims to obtain extremely low-bit LLMs, here we thus introduce more details about
LLM quantization. Quantization stands as a popular and crucial method for model compression,
capable of achieving a significant compression ratio with a relatively small loss. It can be classified
into Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) according to when
quantization is applied.
PTQ directly converts trained models into lower-bit counterparts using accurate solvers and limited
calibration data without additional training. Typically, GPTQ [14] row-wisely quantizes weight
matrices and adjusts remaining weights to compensate for the precision loss caused by quantization,
achieving nearly lossless 4-bit weight quantization. Moreover, numerous studies observed the effect
of “outliers” in quantization [10, 18, 20]. LLM.int8() [10] suggests mixed-precision decomposition
to ensure the accuracy of a few outliers in activations. SmoothQuant [39] reduces the difficulty of
quantization by smoothing the outliers of activation. SpQR [12] identifies sensitive weights to ensure
their precision, while quantizing other weights to lower bit-width.
QAT integrates quantization steps within the model, applying them during training or fine-tuning. It
allows the model to better adapt to the reduced precision induced by quantization, leading to improved
performance compared to PTQ. LLM-QAT [21] introduces a small number of learnable parameters
into quantization and employs KD using data generated by the original model itself. OmniQuant (30;
we classify it as QAT) further introduces learnable equivalent transformation, achieving acceptable
results in 2-bit weight quantization. Contemporary work QuIP# [35] combines randomized Hadamard
transform, vector quantization techniques, and fine-tuning to achieve better performance in 2-bit level.
PEQA [17] and QLoRA [11] focus on fine-tuning a limited number of extra parameters to mitigate
the precision loss caused by sub-4bit weight quantization. Our work is closely related to QAT, but due
to the unique challenges posed by 1-bit quantization, our representation and initialization methods of
quantized weights are distinct from any existing work.
3 Methodology
This section demonstrates our 1-bit architecture of the Linear layer to be quantized and discuss how
to initialize the quantized model to achieve better performance in knowledge distillation. We start
with a short review of classical weight quantization methods in Section 3.1 and then formulate our
OneBit from Section 3.2 to Section 3.4 in detail.
3.1 Background
The main idea of model quantization is to compress each weight matrix W within models in FP32 or
FP16 format to a low-bit counterpart. Specifically, we often quantize the weight matrices of Linear
layers in transformer to 8, 4, and even 2 bits.
The majority of quantization studies primarily employ the round-to-nearest (RTN) method, by which
the weight w is rounded to the nearest value in the quantization grid. It can be formulated as
ŵ = Clip
(⌊w
s
⌉
+ z, 0, 2N − 1
)
, (1)
where s denotes the quantization scale parameter, z denotes the zero point parameter, and N is the
quantization bit-width. Clip(·) truncates the result in the range of 0 to 2N − 1. With the bit-width
being lower and lower, the quantization grid also becomes sparser. When we quantize a LLM to
1-bit values, there are only 2 available numbers to be chosen in the quantized model. Existing study
[9] points out that quantization based on the RTN method may get their best performance at the
4-bit level. Further quantizing to 2-bit values following this paradigm would result in a substantial
degradation [30] as shown in Figure 1.
Furthermore, when N equals 1, quantization based on RTN method is essentially equivalent to
setting a threshold, with weight w on either side of it being converted to corresponding integer
value ŵ. In such a scenario, the parameters s and z in Eq. (1) effectively lose their practical
significance. Consequently, when quantizing weights to 1 bit, the element-wise RTN operation
drastically undermines the precision of the weight matrix W, leading to poor performance of the
quantized model.
3
LayerN
orm
(a) FP16 Linear Layer (b) Our Binary Quantized Linear Layer
Figure 2: The main idea of our method OneBit. The left is the original FP16 Linear Layer, in
which both the activation X and the weight matrix W are in FP16 format. The right is our proposed
architecture. Only value vectors g and h are in FP16 format, and the weight matrix consists of ±1
instead, which can be represented in INT1.
3.2 1-bit Linear Layer Architecture
Due to the severe precision loss of 1-bit weight quantization, converting weight matrices in Linear
layers directly from FP32/16 to 1-bit format based on RTN is challenging. Wang et al. [38] explore
this possibility by studying the capabilities of purely 1-bit weight matrices, training the 1-bit model
from scratch. In the W1A16 setting, their Linear layers are designed as
W±1 = Sign
[
W −Mean
(
W
)]
,
η = Mean
[
Abs
(
W −Mean
(
W
))]
,
Y = η · LayerNorm
(
X
)
WT
±1,
(2)
where W denotes the quantized weight matrix with the shape m × n and W±1 denotes the 1-bit
quantized matrix. X is the input of Linear layer and Y is the output. Sign(·), Mean(·) and Abs(·)
functions return the sign matrix, average and absolute value matrix. Unfortunately, this approach
reduces computational demands but also leads to a marked decrease in performance [38]. Moreover,
due to training difficulties, experiments show that this method is challenging to use for quantizing
existing models and can only be applied to training models from scratch.
Inspired by Wang et al. [38], we also quantize the weight matrix using the function Sign(·), and the
element of the quantized matrix is set to +1 or -1 as well. Moreover, we also notice that although
W±1 maintains a high rank of W, the missed floating-point precision still destroys the model
performance. Therefore, different from previous work, we introduce 2 value vectors with an FP16
format to compromise the precision loss in the quantization process. During training, our proposed
Linear layers are designed as
W±1 = Sign
(
W
)
,
Y =
[(
X⊙ g
)
WT
±1
]
⊙ h,
Z = LayerNorm
(
Y
)
,
(3)
where g and h are the two FP16 value vectors. During inference, W±1 is packed with an INT1
format, and Sign(·) will not be used, as shown in Figure 2. Note that we specify the calculation order
using brackets in Eq. (3) for minimizing the time and space cost. The main difference between Wang
et al. [38] and OneBit is the extra parameter g and h. Even if additional parameters are brought in,
the benefits far outweigh its small cost. For instance, when we quantize one weight matrix with the
shape 4096× 4096, the average bit-width of the quantized result is 1.0073. See A.7 for the details.
3.3 Sign-Value-Independent Decomposition
In our proposed 1-bit architecture, the weight matrix W is mathematically divided into two compo-
nents: one sign matrix W±1 in INT1 format and two value vector g/h in FP16 format. To initialize
the 1-bit model with the help of the fully trained weight, we introduce the Sign-Value-Independent
Decomposition (SVID) of the weight matrix W, which can be formulated as W = Wsign ⊙Wvalue.
Here we have Wvalue = |W| and Wsign = Sign(W). For Wvalue, we further approximately decom-
pose it into the outer product of two vectors a and b, which is also known as rank-1 approximation.
4
Hence, our proposed matrix decomposition method can be represented as
W ≈ Wsign ⊙
(
abT
)
. (4)
We can employ some widely used matrix decomposition methods to perform the rank-1 approximation,
such as SVD [2] and NMF [25].
Proposition 1 Given the weight matrix W and input X, the Linear layer can be reformulated as
the following according to SVID:
XWT ≈
[ (
X⊙ bT
)
WT
sign
]
⊙ aT. (5)
We prove this approximation in Appendix A.1. This bridges the gap between the architecture of
the quantized model and its original weights. It indicates that if we assign Wsign to W±1, aT to h
and bT to g, the quantized model is an approximate initialization of the original model. Moreover,
compared to restoring the original matrix W first (such as in Eq. (4)), the computational order in
Eq. (5) saves approximately one matrix W in FP16 format in memory as there is no need to restore
W in FP16 format.
The main objective of SVID is to involve the sign matrix Wsign in approximating matrix W, rather
than solely relying on value vectors in FP16 format. To substantiate the role of the sign matrix Wsign
in matrix approximation, we present the following proposition.
Proposition 2 Given matrices W and |W|, W = Wsign ⊙ |W|. We decompose these matrices in
the way W = abT +E1 and |W| = ãb̃T +E2, where Ei denotes the error matrices. In terms of
the Frobenius-norm, the SVID is closer to the original matrix W:∥∥∥W −Wsign ⊙ ãb̃T
∥∥∥2
F
≤
∥∥∥W − abT
∥∥∥2
F
. (6)
We also prove this proposition in Appendix A.1. It clearly demonstrates the practical role of the sign
matrix Wsign in matrix approximation.
Note that, given the predominantly low precision of most parameters, it is quite challenging to
approximate the weight matrix W accurately. SVID is not aimed to precisely replicate the original
model’s parameters, but to provide an effective starting point for further training, leveraging the
extensive training of the original model. Details on transferring knowledge from the original model
to the quantized counterpart are in Section 3.4.
3.4 Knowledge Transfer
We employ quantization-aware knowledge distillation to transfer knowledge from the original model
(i.e. teacher model) to the quantized one (i.e. student model). In the student model, the element
in matrix W and vectors g/h in Eq. (3) will be trained. We use cross-entropy based logits and
mean-square-error based hidden state of the full-precision teacher model to direct the quantized
student model [32]. Language modeling loss is not used. The cross-entropy is defined as
LCE = − 1
ns
ns∑
i=1
∑
c
P T
c (oi) logP
S
c (oi) , (7)
where c denotes the number of classes and ns denotes the number of training samples in the current
batch. T and S are the teacher model and student model, respectively. The error of hidden states is
defined as
LMSE =
ns∑
i=1
nl∑
j=1
∥∥∥∥∥ qT
i,j∥∥qT
i,j
∥∥
2
−
qS
i,j∥∥qS
i,j
∥∥
2
∥∥∥∥∥
2
2
, (8)
where nl denotes the number of layers and q denotes the hidden state. Hence the final objective
function can be formulated as
LKD = LCE + αLMSE, (9)
where α is the hyper-parameter that balances the importance of the cross-entropy loss and the features
in the intermediate layers. Please refer to A.6 for further discussions of this part.
5
4 Experiments
We experiment with 1-bit weight-only quantizaton and maintain 16-bit activation (W1A16) in this
work. We evaluate our approach by performing experiments on OPT-1.3B/2.7B models, LLaMA-
7B/13B models and LLaMA2-7B/13B models, and present results on various tasks.
4.1 Settings
Data For the training data of our quantization-aware knowledge distillation, we follow Liu et al.
[21] to synthesize corpus using next token generation from the original teacher model. It randomizes
the first token from vocabulary and generates the next token iteratively until reaching either the
<EOS> token or the maximum length. Specially, the top-1 predictions are selected deterministically
for the first 3 to 5 tokens, followed by stochastic sampling for the remaining tokens. We utilized
LLaMA-7B to generate a total of 132k data entries, each with a maximum length of 2,048.
Training Details Every KD experiment learns the training data over 50 epochs, from which 2048-
token segments are selected. We employ NMF in scikit-learn 1 to decompose the weight matrices in
SVID. The quantized student models are optimized by Adam [19] with β1 = 0.9, β2 = 0.98. The
learning rate for all experiments is scheduled by cosine strategy. We use NVIDIA A100 GPUs and
maintain FP16 precision while training quantized models. For additional details such as learning rate,
please refer to Table 1.
Table 1: Training details of knowledge distillation.
Models learning rate α # GPUs
OPT-1.3B 4e-4 1.0 1 × 8
OPT-2.7B 2e-4 1.0 1 × 8
LLaMA-7B 4e-4 1.0 1 × 8
LLaMA-13B 2e-4 1.0 2 × 8
LLaMA2-7B 1e-4 1.0 1 × 8
LLaMA2-13B 2e-4 1.0 2 × 8
Baselines To our knowledge, there is no previous work exploring the 1-bit quantization of LLMs
from a knowledge transfer perspective. To this end, we relax the quantization bit-width of baselines
to 2 bits (W2A16) while maintaining the W1A16 setting in our method. We compare our method with
GPTQ [14], LLM-QAT [21] and OmniQuant [30]. To ensure a fair comparison in terms of space
usage, baselines do not employ grouped quantization. Additionally, we included the results of vanilla
transformers with FP16 precision as a reference. While the recent work BitNet [38] also introduced
one 1-bit model architecture, it only worked for training models from scratch. We also analyze its
capability to transfer knowledge from the original models in Appendix A.5.
Evaluation Metrics Basically, we evaluate quantized models by testing the perplexity on the
validation set, specifically on WikiText2 [23] and C4 [28]. Lower perplexity indicates that the
compressed model is better at preserving the output distribution of the original model. Furthermore,
accuracies of zero-shot tasks including Winograde [29], HellaSwag [41], PIQA [4], BoolQ [7], and
ARC [8] are also reported. They evaluate if the capabilities of the original model on downstream
tasks are retained. We utilize the open-sourced toolkit “LM-Evaluation-Harness”2 to perform the
perplexity test and all zero-shot tasks.
4.2 Main Results
Table 2 compares our method with other typical strong baselines on different models. Due to space
limitations, results of LLaMA2-7B/13B are listed in Appendix A.3. In various model sizes, our 1-bit
weight quantization method obviously outperforms others under the W2A16 setting. Moreover, the
effectiveness of QAT based methods consistently improves as the model size increases, whereas the
result of the PTQ method, GPTQ, may degrade when model size increases (e.g., from 7B to 13B on
1https://scikit-learn.org/
2https://github.com/EleutherAI/lm-evaluation-harness
6
Table 2: Main results of evaluation experiment. We report the perplexity and zero-shot accuracy.
“FP16” is the transformer with FP16 parameters and we refer to it as the upper-bound of all the
methods. The best score is bolded.
Models Methods Perplexity(↓) Zero-shot Accuracy(↑)
Wiki2 C4 Wino. Hella. PIQA BoolQ ARC-e ARC-c Avg.
OPT-1.3B
FP16 14.63 14.72 59.67 53.73 72.42 57.68 50.80 29.69 54.00
GPTQ 9.5e3 3.8e3 49.33 25.57 52.07 39.60 26.68 23.63 36.15
LLM-QAT 4.9e3 2.1e3 49.72 25.72 50.05 37.83 25.76 25.09 35.70
OmniQuant 42.43 55.64 51.85 33.39 60.94 56.45 38.76 23.38 44.13
OneBit 25.42 22.95 51.14 34.26 62.57 59.45 41.25 24.06 45.46
OPT-2.7B
FP16 12.47 13.17 60.93 60.59 74.81 60.28 54.34 31.31 57.04
GPTQ 8.7e3 3.9e3 49.88 26.47 49.84 39.88 25.76 26.02 36.31
LLM-QAT 3.7e3 1.4e3 52.09 25.47 49.29 37.83 24.92 25.60 35.87
OmniQuant 30.25 41.31 51.62 38.21 62.19 54.25 40.82 24.74 45.31
OneBit 21.86 20.76 51.67 38.18 63.87 54.28 43.39 24.40 45.97
LLaMA-7B
FP16 5.68 7.08 66.85 72.99 77.37 73.21 52.53 41.38 64.06
GPTQ 1.9e3 7.8e2 49.41 25.63 49.95 43.79 25.84 27.47 37.02
LLM-QAT 7.1e2 3.0e2 51.78 24.76 50.87 37.83 26.26 25.51 36.17
OmniQuant 15.34 26.21 52.96 43.68 62.79 58.69 41.54 29.35 48.17
OneBit 10.19 11.40 58.48 51.54 68.01 57.28 42.47 30.20 51.33
LLaMA-13B
FP16 5.09 6.61 70.17 76.24 79.05 68.47 59.85 44.54 66.39
GPTQ 3.2e3 9.9e2 50.67 25.27 50.00 42.39 26.14 27.39 36.98
LLM-QAT 1.8e3 1.2e3 51.62 25.40 50.33 37.83 27.02 26.87 36.51
OmniQuant 13.43 19.33 53.83 54.16 68.99 62.20 45.50 30.38 52.51
OneBit 9.18 10.25 62.90 56.78 70.67 64.16 44.53 32.00 55.17
LLaMA). This demonstrates that QAT-based method can achieve stable results in extremely low-bit
quantization. Specifically, our method approaches the performance of FP16 more closely as the
model size increases. For instance, when scaling from LLaMA-7B to LLaMA-13B, the perplexity
(on C4) of the FP16 model decreases by only 0.47, whereas our method sees a reduction of 1.15.
For perplexity, only our method achieves comparable results to the strongest FP16 baseline. For
instance, our method achieves 9.18 in the Wiki2 dataset on LLaMA-13B model and the FP16
baseline is 5.09. The performance loss of other methods is significant, even though they use 2-bit
quantization, which is more than our 1 bit. For GPTQ and LLM-QAT, the performance degradation
after quantization is pretty severe. As for OmniQuant, even though it is the strongest baseline under
the W2A16 setting, it still suffers greater performance loss compared to our W1A16 setting.
For zero-shot accuracy, although all methods inevitably have some degradation, our method achieves
the closest performance to the FP16 baseline among most models. On the OPT-1.3B/2.7B model, our
method shows smaller performance loss on most tasks such as PIQA and ARC-e. Additionally, the
loss of other tasks is negligible compared with the second-best baseline, OmniQuant. On the LLaMA-
7B model, our method also notably outperforms OmniQuant in most tasks except BoolQ/ARC-e,
averaging about a 4% improvement overall.
4.3 Problem Solving Ability
We have demonstrated the superior performance of our method under the W1A16 setting, compared
to other representative baselines. Although all methods inevitably face performance degradation in
1-bit weight quantization, it remains of interest how our method fares in solving practical problems
among the various approaches to reducing model size. For instance, directly training smaller models
[42] or employing low-rank decomposition to reduce the number of parameters.
To this end, we consider two crucial abilities of LLMs: commonsense reasoning and world knowledge.
For commonsense reasoning, we use the 6 tasks (Hellaswag, etc.) and settings described in Section 4.2.
For world knowledge, we examine it using the Massive Multi-task Language Understanding (MMLU;
15), a benchmark that covers wide domains and knowledge. We compare the following 4 models:
7
(a) Common sense reasoning tasks (b) General world knowledge (MMLU)
Total Memory Average Bit-width1.0
1.2
1.4
1.6
1.8
2.0
2.2
M
em
or
y
(G
B)
2.0
2.2
1.3 1.3
Pythia-1.0B
TinyLLaMA-1.1B
LowRank Llama
OneBit-7B
0
2
4
6
8
10
12
14
16
18
Bi
t-w
id
th
(b
it)
16.0 16.0 16.0
2.88
(c) Memory footprint and bit-width
Figure 3: Comparison of model capabilities and compressive degree.
Pythia-1.0B [3] A well-trained model released by EleutherAI whose memory footprint is 1.54x that
of our OneBit-7B model.
TinyLLaMA-1.1B [42] A model with the same structure as the LLaMA models, which undergoes
continued training. To compare fairly, we use the checkpoint at 10k training steps, which is 2x that of
our OneBit-7B model.
LowRank LLaMA [24] Decompose every weight matrix in Linear layers to two low-rank matrices
and learn from the original LLaMA-7B model by KD in the same setting of OneBit-7B.
OneBit-7B The model that we use in Section 4.2, which is built with OneBit.
Figure 3a and 3b demonstrate common sense reasoning ability and general world knowledge of
different models. We can observe that, although other models have more parameters and are more
thoroughly trained than ours, our model still has advantages in common sense reasoning. This reflects
the benefits inherited from the larger 7B model. In terms of world knowledge, despite a significant
loss in social sciences, our model outperforms the fully trained Pythia-1B in other domains. These
results demonstrate the practical usability of OneBit.
5 Analysis and Discussion
5.1 Efficiency
It is evident that extremely low-bit quantization of weights can significantly reduce the memory
footprint of models. As shown in Table 3, the actual compression ratio increases as the model size
increases. This is particularly meaningful for larger models, making it possible to fit the model into
one GPU. While there is a performance loss, Figure 4 illustrates that our method achieves a good
8
0 6 12 18 24
Model Size (GB)
5
10
15
20
25
Pe
rp
le
xi
ty
o
n
W
ik
i2
OPT-1.3B
OPT-2.7B
LLaMA-7B LLaMA-13B
OPT-1.3B
OPT-2.7B
LLaMA-7B
LLaMA-13B
same size, 0.67 better PPL
same PPL, 0.22x size
Baseline (FP16)
OneBit (W1A16)
Figure 4: Tradeoff between size and PPL.
0 1000 2000 3000 4000 5000 6000
Training Steps
0
20
40
60
80
100
120
Tr
ai
ni
ng
L
os
s
Singular Value Decomposition (SVD)
Non-negative Matrix Factorization (NMF)
Only Copy from Original Weight
Figure 5: Training process of OneBit-7B.
Table 3: Compression ratio of LLaMA models.
Models FP16 (GB) OneBit (GB) Ratio (%)
LLaMA-7B 13.5 1.3 90.4
LLaMA-13B 26.0 2.2 91.5
LLaMA-30B 65.1 4.9 92.5
LLaMA-65B 130.6 9.2 93.4
trade-off between space occupancy and model performance. For example, we can achieve comparable
performance to FP16 with only 0.2x the model space. Furthermore, quantizing to ±1 also aids in
accelerating matrix multiplication on CPUs. It is because the floating-point multiplication of elements
in two matrices can be converted into much faster bit operations on these chips. Thus the substantial
reduction in memory overhead makes these low-bit LLMs meet the requirements for deployment on
PCs and smartphones.
5.2 Robustness
Existing work [38] has already noted the instability within QAT. Extremely low-bit quantization
makes the training process highly sensitive to the learning rate, making it difficult for the model
to converge when the rate is too small or too large. This is primarily due to the large magnitude
of gradients generated as the weight elements fluctuate between +1 and -1, leading to substantial
fluctuations in the output of Linear layers. Experiments demonstrate that OneBit shows more stable
training process and is not sensitive to learning rates. Please refer to Appendix A.5 for more details.
5.3 Effect of Different Components
The variable components in our method primarily include Post-LayerNorm, value vectors, and
parameter initialization.
Post-LayerNorm We discover that there might be floating-point overflow during the QAT process.
As depth increases, the activation can become progressively larger. We tackle it using Post-LayerNorm
instead of Pre-LayerNorm. In contrast, Pre-LayerNorm may occasionally be ineffective.
Value Vectors The main structural difference between OneBit and BitNet [38] is the two value
vectors, which are demonstrated to be effective in Section 4.2. They facilitate stable training and the
knowledge transfer process. Please refer to Appendix A.5 for more details of comparison.
Parameter Initialization In our proposed SVID, both NMF and SVD can be used to decompose
|W| and we recommend using the former. This is because we find that NMF may make the training
more faster to converge. Figure 5 shows that initializing by NMF facilitates better performance.
9
6 Conclusion
We propose a novel model structure for 1-bit weight quantization and a corresponding parameter
initialization method to address the difficulty in 1-bit quantization. Extensive experiments on LLMs
of various sizes and series demonstrate that OneBit has clear advantages over representative strong
baselines and achieves a good tradeoff between model size and performance. We further analyze the
capabilities of such extremely low-bit quantized models and provide guidance for future research.
Limitations
Although our proposed method significantly reduces the memory footprint of LLMs, bringing hope
for efficient deployment of them, there are still some limitations. Firstly, compared to the original
model, our extremely low-bit quantization inevitably incurs a performance loss. Additionally, we are
yet to understand the mathematical principles behind the optimal parameters of the 1-bit quantized
model, thus capability transfer can only be achieved through the relatively costly process of KD.
Fortunately, this cost is a one-time expense. Moreover, due to the unique nature of 1-bit quantization,
our method can not be naturally extended to higher bit-width. Lastly, we have not considered the
activation quantization and leave it as future work.
Ethics Statement
In this study, we employ models that are publicly available and open source. We affirm that the
use of these models aligns with their original intended purposes. These models have been utilized
strictly within the scope of academic and research-based activities, adhering to ethical guidelines and
ensuring compliance with open-source licenses.
References
[1] R. Agarwal, N. Vieillard, P. Stanczyk, S. Ramos, M. Geist, and O. Bachem. GKD: Generalized
knowledge distillation for auto-regressive sequence models. arXiv preprint arXiv:2306.13649,
2023.
[2] E. Beltrami. Sulle funzioni bilineari, giomale di mathematiche ad uso studenti delle uninersita.
11, 98–106.(an english translation by d boley is available as university of minnesota, department
of computer science). Technical report, Technical Report 90–37, 1990.
[3] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O’Brien, E. Hallahan, M. A. Khan,
S. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models
across training and scaling. In ICML, pages 2397–2430, 2023.
[4] Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al. PIQA: Reasoning about physical commonsense in
natural language. In Proceedings of the AAAI, volume 34, pages 7432–7439, 2020.
[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in NeurIPS, 33:
1877–1901, 2020.
[6] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee,
Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4.
arXiv preprint arXiv:2303.12712, 2023.
[7] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. BoolQ:
Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044,
2019.
[8] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think
you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint
arXiv:1803.05457, 2018.
10
[9] T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. In
ICML, pages 7750–7774, 2023.
[10] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. LLM.int8(): 8-bit matrix multiplication
for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.
[11] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. QLoRA: Efficient finetuning of
quantized LLMs. In Advances in NeurIPS, 2023.
[12] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar, S. Ashkboos,
A. Borzunov, T. Hoefler, and D. Alistarh. SpQR: A sparse-quantized representation for near-
lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023.
[13] E. Frantar and D. Alistarh. SparseGPT: Massive language models can be accurately pruned in
one-shot. In ICML, pages 10323–10337, 2023.
[14] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. GPTQ: Accurate post-training quantization
for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.
[15] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring
massive multitask language understanding. In ICLR, 2021.
[16] C.-Y. Hsieh, C.-L. Li, C.-k. Yeh, H. Nakhost, Y. Fujii, A. Ratner, R. Krishna, C.-Y. Lee, and
T. Pfister. Distilling step-by-step! outperforming larger language models with less training data
and smaller model sizes. In Findings of the ACL, pages 8003–8017, 2023.
[17] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, and D. Lee. Memory-efficient
fine-tuning of compressed large language models via sub-4-bit integer quantization. arXiv
preprint arXiv:2305.14152, 2023.
[18] S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W. Mahoney, and K. Keutzer.
SqueezeLLM: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.
[19] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[20] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han. AWQ: Activation-aware weight
quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.
[21] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Krishnamoorthi, and
V. Chandra. LLM-QAT: Data-free quantization aware training for large language models. arXiv
preprint arXiv:2305.17888, 2023.
[22] X. Ma, G. Fang, and X. Wang. LLM-Pruner: On the structural pruning of large language models.
In Advances in NeurIPS, 2023.
[23] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. arXiv preprint
arXiv:1609.07843, 2016.
[24] M. B. Noach and Y. Goldberg. Compressing pre-trained language models by matrix decomposi-
tion. In Proceedings of the AACL-IJCNLP, pages 884–889, 2020.
[25] P. Paatero and U. Tapper. Positive matrix factorization: A non-negative factor model with
optimal utilization of error estimates of data values. Environmetrics, 5(2):111–126, 1994.
[26] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint
arXiv:2304.03277, 2023.
[27] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are
unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[28] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.
Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of
Machine Learning Research, 21(1):5485–5551, 2020.
11
[29] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd
schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.
[30] W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Zhang, P. Gao, Y. Qiao, and P. Luo.
OmniQuant: Omnidirectionally calibrated quantization for large language models. arXiv
preprint arXiv:2308.13137, 2023.
[31] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter. A simple and effective pruning approach for large
language models. arXiv preprint arXiv:2306.11695, 2023.
[32] S. Sun, Y. Cheng, Z. Gan, and J. Liu. Patient knowledge distillation for BERT model compres-
sion. In Proceedings of the EMNLP-IJCNLP, pages 4323–4332, 2019.
[33] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto.
Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/
stanford_alpaca, 2023.
[34] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,
E. Hambro, F. Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971, 2023.
[35] A. Tseng, J. Chee, Q. Sun, V. Kuleshov, and C. De Sa. Quip#: Even better llm quantization
with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024.
[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I. Polosukhin. Attention is all you need. Advances in NeurIPS, 30, 2017.
[37] Z. Wan, X. Wang, C. Liu, S. Alam, Y. Zheng, Z. Qu, S. Yan, Y. Zhu, Q. Zhang, M. Chowdhury,
et al. Efficient large language models: A survey. arXiv preprint arXiv:2312.03863, 2023.
[38] H. Wang, S. Ma, L. Dong, S. Huang, H. Wang, L. Ma, F. Yang, R. Wang, Y. Wu, and F. Wei.
BitNet: Scaling 1-bit transformers for large language models. arXiv preprint arXiv:2310.11453,
2023.
[39] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. SmoothQuant: Accurate and
efficient post-training quantization for large language models. In ICML, pages 38087–38099,
2023.
[40] M. Xu, Y. L. Xu, and D. P. Mandic. TensorGPT: Efficient compression of the embedding layer
in llms based on the tensor-train decomposition. arXiv preprint arXiv:2307.00526, 2023.
[41] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine really
finish your sentence? arXiv preprint arXiv:1905.07830, 2019.
[42] P. Zhang, G. Zeng, T. Wang, and W. Lu. TinyLlama: An open-source small language model.
arXiv preprint arXiv:2401.02385, 2024.
[43] X. Zhu, J. Li, Y. Liu, C. Ma, and W. Wang. A survey on model compression for large language
models. arXiv preprint arXiv:2308.07633, 2023.
A Appendix
A.1 Proofs of Propositions
In this section, we provide the necessary and detailed proofs for the propositions presented in this
paper. All symbols have the same definition as in the main text.
Proposition 1 Given the weight matrix W and input X, the Linear layer can be reformulated as
the following according to SVID:
XWT ≈
[ (
X⊙ bT
)
WT
sign
]
⊙ aT.
12
Proof From Eq. (4), we have wij ≈ sij · aibj , where sij is the element of Wsign. Hence we have(
XWT
)
ij
≈
∑
k
xikw
T
kj =
∑
k
xikwjk
=
∑
k
xiksjkajbk
=
∑
k
xikbksjkaj
=
∑
k
(
X⊙ bT
)
ik
sTkjaj
=
[ (
X⊙ bT
)
WT
sign
]
ij
aj
=
{[ (
X⊙ bT
)
WT
sign
]
⊙ aT
}
ij
.
This proposition is proved.
Lemma 1 Let σi (W) denote the i-th biggest singular value of matrix W. The following inequality
holds:
σ1 (|W|) ≥ σ1 (W) .
Proof According to the definition of induced norm, there are
σ1 (W) = ∥W∥2 = max
x,∥x∥2=1
∥Wx∥2,
σ1 (|W|) = ∥|W|∥2 = max
y,∥y∥2=1
∥|W|y∥2.
Note that for ∀x, ∥x∥2 = 1 and we have
∥|W||x|∥22 =
∑
i
(∑
j
|wij ||xj |
)2
≥
∑
i
(
|
∑
j
wijxj |
)2
=
∑
i
(∑
j
wijxj
)2
= ∥Wx∥22.
Therefore
max
y,∥y∥2=1
∥|W|y∥2 ≥ max
x,∥x∥2=1
∥Wx∥2.
This lemma is proved.
Proposition 2 Given matrices W and |W|, W = Wsign ⊙ |W|. We decompose these matrices in
the way W = abT +E1 and |W| = ãb̃T +E2, where Ei denotes the error matrices. In terms of
the Frobenius-norm, the SVID is closer to the original matrix W:∥∥∥W −Wsign ⊙ ãb̃T
∥∥∥2
F
≤
∥∥∥W − abT
∥∥∥2
F
.
Proof Here we consider SVD to prove it. For SVD, the norm of the error matrix E in the rank-1
approximation is the sum of the squares of all singular values except for the largest one. We have
∥E1∥2F =
n∑
i=2
σ2
i (W) ,
∥E2∥2F =
n∑
i=2
σ2
i (|W|) .
13
Based on ∥W∥2F = ∥|W|∥2F , we have
n∑
i=1
σ2
i (W) =
n∑
i=1
σ2
i (|W|) .
According to Lemma 1, we can conclude
∥E2∥2F ≤ ∥E1∥2F .
From the equation in this proposition, we can formulate
Wsign ⊙ |W| = Wsign ⊙ ãb̃T +Wsign ⊙E2.
Hence we have
W −Wsign ⊙ ãb̃T = Wsign ⊙E2.
Therefore
∥Wsign ⊙E2∥2F =
∑
i,j
s2ije
2
ij =
∑
i,j
e2ij
= ∥E2∥2F ≤ ∥E1∥2F ,
where sij = ±1 is the element of Wsign. Hence the inequation in this proposition is proved.
A.2 Details on Baselines
In this subsection, we provide the essential details of the baselines in this work:
• GPTQ [14]: We employ the open-source code released by the author. Both OPT models
and LLaMA models take 128 2048-token samples from the C4 dataset to calibrate the
quantized model. For LLaMA models, we apply the activation order heuristic according to
the recommendation from the code.
• LLM-QAT [21]: We reimplement this method to adapt the W2A16 setting, as LLM-QAT is
not designed for 2-bit weight quantization. We also do not quantize the KV Cache. When
quantizing the weight matrix in Linear layer, we use symmetric MinMax quantization in
which the zero-point is set to 0. The training hyper-parameters are the same as ours. Please
refer to the training details in Section 4.1.
• OmniQuant [30]: We employ the open-source code released by the author. Both OPT
models and LLaMA models take 128 2048-token samples from the WikiText2 dataset to
calibrate the quantized model. The learning rate for learnable weight clipping and equivalent
transformation is set to 5e-3 and 1e-2, respectively. We use a batch size of 1 and train 40
epochs for each model. For OPT models, both learnable weight clipping and equivalent
transformation are leveraged. For LLaMA models, only learnable weight clipping is used.
A.3 Results of LLaMA2
Table 4 compares the results on LLaMA2-7B/13B. Obviously, our method has advantages in both
perplexity and zero-shot accuracy. It also reflects that the advantages of our method are more
pronounced in larger models. For instance, when scaling from LLaMA2-7B to LLaMA2-13B, the
perplexity of the FP16 model decreases by around only 0.5, whereas our method reduces it by around
1.0 on both Wiki2 and C4 datasets.
A.4 Instrution Following Ability
Instruction following is an important ability of LLMs [27, 5, 26]. Beyond the discussion on model
abilities and efficiency before, we also focus on the instruction following ability of extremely low-bit
models, which is closely related to their practical usability. In this subsection, we empirically study
this capability of our quantized model. We fine-tune the model for 3 epochs using the alpaca_en_52k
dataset and alpaca templates [33], then observe the generation in both zero-shot and few-shot settings
14
Table 4: Results of LLaMA2. We bold the best scores.
Models Methods Perplexity(↓) Zero-shot Accuracy(↑)
Wiki2 C4 Wino. Hella. PIQA BoolQ ARC-e ARC-c Avg.
LLaMA2-7B
FP16 5.47 6.97 67.09 72.94 76.88 71.10 53.58 40.61 63.70
GPTQ 7.7e3 NAN 50.28 26.19 49.46 42.97 26.77 28.58 37.38
LLM-QAT 1.1e3 6.6e2 49.08 25.10 50.12 37.83 26.26 26.96 35.89
OmniQuant 31.21 64.34 51.22 33.87 56.53 59.14 33.63 24.32 43.12
OneBit 9.73 11.11 58.41 52.58 68.12 63.06 41.58 29.61 52.23
LLaMA2-13B
FP16 4.88 6.47 69.77 76.62 79.05 68.99 57.95 44.20 66.10
GPTQ 2.1e3 3.2e2 51.85 25.67 51.74 40.61 25.46 27.30 37.11
LLM-QAT 5.1e2 1.1e3 51.38 24.37 49.08 39.85 27.15 24.32 36.03
OmniQuant 16.88 27.02 53.20 50.34 62.24 62.05 40.66 29.61 49.68
OneBit 8.76 10.15 61.72 56.43 70.13 65.20 43.10 33.62 55.03
before and after fine-tuning. During training, the learning rate is set to 1e-7 and the batch size to 32.
Other parameters are consistent with Section 4.1.
Table 5 demonstrates the content generation and instruction following abilities of our 7B model.
Under the zero-shot setting, the model without SFT produced verbose, repetitive, and low-quality
text. However, once experienced to SFT, our model is able to smoothly output high-quality content,
exhibiting excellent instruction following ability. For the few-shot setting, our model exhibits
instruction following ability both before and after SFT.
A.5 Comparison with BitNet
Recently, BitNet [38] introduces a 1-bit model architecture and applies the architecture to train
models from scratch, demonstrating the feasibility and application value of the 1-bit model structure.
In this paper, we attempt to combine 1-bit quantization with knowledge distillation to quantize the
LLaMA-7B model. Unfortunately, despite following the suggestion to use larger learning rates, the
behavior remains unstable during training.
Figure 6 shows that the training process of BitNet may suffer from instability during knowledge
distillation. We conjecture that it is because the gradient is pretty large when the weight elements
fluctuate between +1 and -1, further aggravating the output of the Linear layer.
As a more effective measure, the value vectors we propose for quantization not only supplement the
necessary floating-point numerical precision but also limit the fluctuation range of the matrix multi-
plication results after quantization. This can be understood from forward and backward computation,
respectively.
Forward stability. Quantized matrix multiplication is more prone to overflow than FP16 coun-
terparts in response to minor perturbations of input activations. This is because the magnitude of
elements in quantized matrices, particularly the value ±1, is far greater than the parameters of most
FP16 matrices. By multiplying by value vectors of a magnitude similar to that of the FP16 model, the
range of variation in model output activations can be restored to the level of FP16. Furthermore, we
also avoid the increasingly large “drift phenomenon” of activations through Post-LayerNorm.
Backward stability. Since Sign(·) function is not differentiable, when the elements of the matrix
change, their gradient may become infinite. Similar to forward stability, by multiplying two numer-
ically smaller value vectors, we avoid layer-by-layer accumulation and explosion during gradient
back-propagation. Moreover, we implement the derivative function of Sign(·) using the derivative of
the hyperbolic tangent function, thereby avoiding the problem of gradient explosion at the zero point
of every weight.
15
Table 5: Instruction following cases in zero-shot and few-shot settings before and after Supervised
Fine-Tuning (SFT). The few-shot examples are from MMLU benchmark.
Zero-shot w/o SFT
Prompt:
List several Windows operating systems developed by Microsoft corporation.
Generation:
Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal
computers and tablets. It was first released to the public on 29 April 2014.
Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal
computers and tablets. It was first released to the public on 29 April 2014. Windows 10 is the
successor to Windows 8.1.
Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal
computers and tablets. It was first released to the public on 29 April 2014. Windows 10 is the
successor to Windows 8.1. Windows 10 is the successor to Windows 8.1.
Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal
computers and tablets.
Zero-shot w/ SFT
Prompt:
Below is an instruction that describes a task. Write a response that appropriately completes
the request.
### Instruction:
List several Windows operating systems developed by Microsoft corporation.
### Response:
Generation:
Microsoft has developed several Windows operating systems, including Windows 10, Windows
8, Windows 7, Windows XP, Windows 2000, Windows NT, Windows ME, Windows 95, and
Windows 1. Windows 10 is the latest version of the Windows operating system. It was released
in 2019. Windows 8 was released in 2011. Windows 7 was released in 2009. Windows XP
was released in 2001. Windows NT was released in 1997. Windows ME was released in 1999.
Windows 95 was released in 1995. Windows 1 was released in...
Few-shot w/o SFT
Prompt:
The following are multiple choice questions (with answers) about college biology.
Which of the following contain DNA sequences required for the segregation of chromosomes
in mitosis and meiosis? A. Telomeres B. Centromeres C. Nucleosomes D. Spliceosomes
Answer: B. The Barr body found within female mammalian cells’ nuclei is an example of: A.
euchromatin B. heterochromatin C. a cistron D. pseudogenes Answer: B. Cleavage symmetry
arising from early cell divisions that occur at an oblique angle to the polar axis is known
as: A. meroblastic cleavage B. radial cleavage C. spiral cleavage D. superficial cleavage
Answer: C. Which of the following is NOT a source of atmospheric carbon? A. Respiration
B. Photosynthesis C. Bacterial decomposition D. Combustion of fossil fuels Answer: B. The
concept of punctuated equilibrium refers to: A. oscillating ecological successional stages
B. ecological succession arrested by sudden environmental changes, e.g., fire C. persistent
predator-prey relationships in relatively stable environments D. bursts of speciation followed
by relatively unchanging lineages Answer: D. Which of the following is a second messenger
that stimulates release of calcium ions into the cytoplasm? A. Prostaglandins B. Calmodulin
C. Cyclic AMP D. Inositol triphosphate Answer:
Generation:
B
Few-shot w/ SFT
Prompt:
Below is an instruction that describes a task. Write a response that appropriately completes
the request.
### Instruction:
The following are multiple choice questions (with answers) about college biology.
Which of the following contain DNA sequences required for the segregation of chromosomes
in mitosis and meiosis? A. Telomeres B. Centromeres C. Nucleosomes D. Spliceosomes
Answer: B. The Barr body found within female mammalian cells’ nuclei is an example of: A.
euchromatin B. heterochromatin C. a cistron D. pseudogenes Answer: B. Cleavage symmetry
arising from early cell divisions that occur at an oblique angle to the polar axis is known
as: A. meroblastic cleavage B. radial cleavage C. spiral cleavage D. superficial cleavage
Answer: C. Which of the following is NOT a source of atmospheric carbon? A. Respiration
B. Photosynthesis C. Bacterial decomposition D. Combustion of fossil fuels Answer: B. The
concept of punctuated equilibrium refers to: A. oscillating ecological successional stages
B. ecological succession arrested by sudden environmental changes, e.g., fire C. persistent
predator-prey relationships in relatively stable environments D. bursts of speciation followed
by relatively unchanging lineages Answer: D. Which of the following is a second messenger
that stimulates release of calcium ions into the cytoplasm? A. Prostaglandins B. Calmodulin
C. Cyclic AMP D. Inositol triphosphate Answer:
### Response:
Generation:
D
16
0 50 100 150 200 250 300 350
Training Steps
0
50
100
150
200
250
300
350
Tr
ai
ni
ng
L
os
s
(a) learning rate 4e-4
0 100 200 300 400 500 600
Training Steps
0
50
100
150
200
250
300
350
Tr
ai
ni
ng
L
os
s
(b) learning rate 6e-4
0 200 400 600 800
Training Steps
0
50
100
150
200
250
300
350
Tr
ai
ni
ng
L
os
s
(c) learning rate 8e-4
0 250 500 750 1000 1250 1500 1750
Training Steps
0
50
100
150
200
250
300
350
Tr
ai
ni
ng
L
os
s
(d) learning rate 10e-4
0 200 400 600 800 1000 1200
Training Steps
0
50
100
150
200
250
300
350
Tr
ai
ni
ng
L
os
s
(e) learning rate 12e-4
0 100 200 300 400 500 600 700 800
Training Steps
0
50
100
150
200
250
300
350
Tr
ai
ni
ng
L
os
s
(f) learning rate 15e-4
Figure 6: Training comparisons among different learning rates when BitNet performs knowledge
distillation from LLaMA-7B. Here we choose the same W1A16 setting as ours. The weight matrices
in BitNet are directly copied from the original LLaMA-7B model.
A.6 Discussion on Knowledge Distillation
Although knowledge distillation is not the main contribution of this paper, we nevertheless provide
the rationale behind certain settings used in our experiments to explain the necessity of these
configurations.
We firstly explain the role of different loss functions in guiding the process of knowledge transfer.
Fundamentally, distillation loss alone can achieve a satisfactory transfer process (comparing to other
baselines). Additionally, as shown in the Table 6, aligning the hidden states between layers can
result in a quantized model with better perplexity. However, further incorporating attention score
alignment on this basis leads to the model failing to converge. LLM-QAT [21] has conducted similar
experiments on quantization-aware knowledge distillation loss and concluded that using only the
distillation loss yields the best results. The difference in conclusions may stem from two factors. On
one hand, due to our adoption of a novel model architecture, which differs from theirs, the optimal
17
Table 6: Ablation study of different loss on LLaMA-7B. “ATTN” means attention score alignment.
Loss Setting Perplexity(↓) Zero-shot Accuracy(↑)
Wiki2 C4 Wino. Hella. PIQA BoolQ ARC-e ARC-c Avg.
LKD 13.48 14.57 50.83 35.14 62.89 60.46 37.33 26.37 45.50
LKD + LMSE (α = 1) 10.19 11.40 58.48 51.54 68.01 57.28 42.47 30.20 51.33
LKD + LMSE (α = 10) 10.38 11.56 60.30 50.73 67.46 62.51 41.71 29.61 52.05
LKD + LMSE + LATTN NAN NAN - - - - - - -
usage of loss functions may be different as well. On the other hand, as we focus on extremely low
bit-width compression, each layer of the model suffers significant information loss compared to the
teacher model. The regularization of hidden states between layers may help reduce the variance in
the learning process, thus demonstrating stronger generalization.
Furthermore, we also discuss the cost of our quantization method. Using LLaMA-7B as an example,
quantizing the model with our method requires approximately 7 days on 8 A100-80GB GPUs. In
comparison, training the LLaMA-7B model from scratch consumes 82,432 GPU hours [34]. The
quantization time, being less than 2% of the pretraining time, is still an acceptable cost.
A.7 Average Bit-width of Linear Layer
This subsection formulates the calculation of the average bit-width of Linear layers. Assume there is
a weight matrix with a shape of 4096× 4096 in such a layer, the number of bits in every component
is
1× 4096× 4096,
16× 1× 4096× 2,
where the first is for the 1-bit quantized weight matrix and the second is for the two FP16 value
vectors. Hence the overall number of bits is 16, 908, 288. Moreover, the number of parameters is
4096× 4096 + 2× 4096× 1 = 16, 785, 408. Therefore, the average bit-width of this Linear layer
is 16, 908, 288÷ 16, 785, 408 ≈ 1.0073.
18
第7个文件的内容为：
PB-LLM: PARTIALLY BINARIZED LARGE LANGUAGE
MODELS
Yuzhang Shang∗
Illinois Institute of Technology
Zhihang Yuan∗
Huomo AI
Qiang Wu
Huomo AI
Zhen Dong
UC Berkeley
ABSTRACT
This paper explores network binarization, a radical form of quantization, compress-
ing model weights to a single bit, specifically for Large Language Models (LLMs)
compression. Due to previous binarization methods collapsing LLMs, we propose
a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme
low-bit quantization while maintaining the linguistic reasoning capacity of quan-
tized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naı̈ve
applications of existing binarization algorithms and highlights the imperative role
of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small
ratio of salient weights during binarization, allocating them to higher-bit storage,
i.e., partially-binarization. PB-LLM is extended to recover the capacities of quan-
tized LMMs, by analyzing from the perspective of post-training quantization (PTQ)
and quantization-aware training (QAT). Under PTQ, combining the concepts from
GPTQ, we reconstruct the binarized weight matrix guided by the Hessian matrix
and successfully recover the reasoning capacity of PB-LLM in low-bit. Under QAT,
we freeze the salient weights during training, explore the derivation of optimal
scaling factors crucial for minimizing the quantization error, and propose a scaling
mechanism based on this derived scaling strategy for residual binarized weights.
Those explorations and the developed methodologies significantly contribute to
rejuvenating the performance of low-bit quantized LLMs and present substantial
advancements in the field of network binarization for LLMs. The code is available
at PB-LLM.
1 INTRODUCTION
Recently, large language models (LLMs) have gained significant traction in artificial intelligence. It
can be attributed to the success of models such as ChatGPT [Brown et al., 2020, Ouyang et al., 2022].
Following its lead, other LLMs such as OPT [Zhang et al., 2022], BLOOM [Scao et al., 2022], and
LLaMA [Touvron et al., 2023] have emerged, proving that an increase in model size typically results
in enhanced capabilities. As a result, models with tens to hundreds of billions of parameters have
become the norm. However, their vast size poses considerable deployment challenges on memory-
constrained devices. A model such as the LLAMA-65B (with 65 billion parameters) requires at least
130GB of memory for inference - a number that often exceeds the capacity of a single GPU or server.
Many methods have been proposed to reduce the memory consumption of LLMs [Zhu et al., 2023].
Those methods can be categorized into weight quantization [Dettmers et al., 2022], network prun-
ing [Frantar and Alistarh, 2023], and low-rank factorization [Zhang et al., 2023]. Among these
compression paradigms, weight quantization is particularly prominent and widely adopted for LLMs.
Since it preserves the original model architecture and leverages well-trained LLMs’ full-precision
checkpoints, the compression process is greatly simplified [Zhu et al., 2023]. However, state-of-the-art
LLM quantization methods show a marked decline in quality beyond 4 bits [Liu et al., 2023a].
More aggressive compression methods are required to push the LLM quantization into the lower bit
range. The network binarization technique stands out, reducing the bit-width of weights to just one
bit [Helwegen et al., 2019, Rusci et al., 2020, Qin et al., 2020a; 2023]. The binarized models take
little storage and memory, and accelerate the inference by efficient bitwise operations. Compared
∗Equal contribution.
1
ar
X
iv
:2
31
0.
00
03
4v
2
[
cs
.L
G
]
7
N
ov
2
02
3
Activation
Partially-
Binarized
Weight FC 𝑸𝑸
Partially-
Binarized
Weight FC 𝑲𝑲
Partially-
Binarized
Weight FC 𝑽𝑽
KV cache KV cache
Activation
Partially-
Binarized
Weight FC
Activation
Partially-
Binarized
Weight FC-1
Activation
Partially-
Binarized
Weight FC-1
Activation
Multi-Head
Self-Attention
Feed-
Forward
Network
+0.7 -0.3 +0.1
-0.3 -0.3 +0.9
-3.7 -0.4 +0.6
+0.1
+2.9
+0.6
+0.1
+0.2
+0.6
−
1
2 +
1
3
−
1
2
-3.7
+2.9−
1
3
−
1
3
+
1
3
+
1
3
+
1
3
+
1
2
+
1
2
+
1
3
+
1
3
+
1
3
Partially Binarize
Column Scaling
-1 +1 +1
-1 -1 +1
-3.7 -1 +1
+1
+2.9
+1
+1
-1
-1
Partially-Binarized
Weight Matrix
(a) One basic block of the Partially-Binarized LLM.
FP
OPT1.3B
Random
Guess
(b) Performance on BoolQ.
Figure 1: (a) We introduce Partially-Binarized Large Language Model (PB-LLM), where a small subset of
the weights of the LLM are frozen and preserved with higher bit precision, while the remaining weights are
binarized utilizing an optimal scaling factor strategy; (b) By using PB-LLM, an extremely low-bit LLM can
be acquired efficiently (i.e., quantization-aware training converges quickly) while maintaining its language
reasoning capabilities.
to other aggressive compression technologies like high-sparsity pruning, network binarization has
potent topological generics, as it only applies to parameters. Binarization is widely studied in
academic research as a standalone compression technique, rather than simply a 1-bit specialization of
quantization. Some SoTA binarization algorithms have even achieved full-precision performance on
large-scale tasks, e.g., ReActNet [Liu et al., 2020a] for ImageNet classification [Deng et al., 2009].
It is theoretically possible to significantly lower the LLM quantization if we generalize the idea of
binarizing the weights of LLMs.
In this paper, we explore network binarization specifically for LLM quantization and propose Partially-
binarized LLMs (abbreviated as PB-LLM). This methodology aims to achieve extreme quantization
to the lowest possible bit, while maintaining the language reasoning capacity inherent in LLMs. The
explorations indicate that simple adaptations of existing binarization algorithms do not work well
for LLM quantization. As a result of this realization, attention is directed towards the salient-weight
property of LLM quantization. In order to achieve the desired extreme low-bit quantization, salient
weights must be fully exploited. We investigate the salient weights in aspects of their detection criteria
and granularity, as well as the storage costs. Then, we propose the partially binarized matrix, storing
the salient weights in higher bits. After establishing the foundation of PB-LLM, the exploration
extends to regain the lost reasoning capacity of the quantized LLMs, under the frameworks of post-
training quantization (PTQ) and quantization-aware training (QAT). In the view of PTQ, inspired
by the concepts of GPTQ [Frantar et al., 2022], we reconstruct the PB-LLM matrix guided by the
Hessian matrix and successfully recover the reasoning capacity of PB-LLM in low-bit. In the view of
QAT, salient weights are frozen throughout the binarization process for efficient training. In addition,
from the perspective of quantization error minimization, we explore how binarized LLMs should be
scaled based on the ideal scaling factor. We scale the binarized weight based on the derived scaling
strategy shown in Fig. 1a. Low-bit quantized LLMs can significantly improve their performance with
such explorations. Benefited from explorations of PTQ and QAT, PB-LLM can efficiently obtain
an extremely low-bit LLM with comparable reasoning capacity (see Fig. 1b). The methodologies
applied and the insights gained within this study stand to contribute substantially to the advancement
of knowledge and development in the field of network binarization for LLMs.
2 RELATED WORK
2.1 NETWORK BINARIZATION.
Binarization uses the sign function to binarize weights and activations to ±1. To eliminate the
vanishing gradient issue caused by the sign function in the binarization, the straight-through estimator
2
(STE) [Bengio et al., 2013] is utilized for the network backpropagation. Based on this archetype,
copious studies contribute to improving the performance of BNNs. Binarization techniques can
be broadly classified into three categories: the enhancement of training objectives, the reduction
of gradient mismatch, and the minimization of quantization errors [Qin et al., 2020b; 2023, Yuan
and Agaian, 2023]. To illustrate: Gradient Mismatch: Liu et al. [2020b] introduce double residual
connections paired with full-precision downsampling layers. This approach addresses the gradient
vanishing problem that arises due to binarization. Training Objectives: Martinez et al. [2020]
focus on optimizing the loss function during training. They suggest aligning the spatial attention
maps derived from both binary and real-valued convolutions. Quantization Error Minimization:
Rastegari et al. [2016] identify that the disparity in quantization between full-precision and binarized
weights can impede the representational abilities of BNNs. As a solution, they introduce a scaling
factor—determined by the L1 norm—for both weights and activation functions.
While binarization has proven successful in computer vision, its exploration in natural language
processing remains limited. Existing methods [Bai et al., 2020, Qin et al., 2022, Liu et al., 2022;
2023b] primarily target smaller language models (e.g., BERT-base [Devlin et al., 2018] with 110M
parameters) potentially hindering their generalization to larger ones (e.g., LLAMA-7B [Touvron
et al., 2023] with 7B parameters). We investigate binarization for LLMs comprehensively in this
paper and propose PB-LLM, which is an attempt to compress LLMs using binarization.
2.2 LARGE LANGUAGE MODEL QUANTIZATION.
Quantization, a prominent method in model compression, addresses the storage and computational
overhead of deep learning models. Recent research efforts successfully apply quantization to compress
Large Language Models (LLMs), including Quantization-Aware Training (QAT) and Post-Training
Quantization (PTQ).
In the domain of QAT, innovative strategies like LLM-QAT [Liu et al., 2023a] address challenges in
acquiring training data for LLMs by leveraging pre-trained models for data-free distillation. Addi-
tionally, techniques such as QLORA [Dettmers et al., 2023a] focus on parameter-efficient fine-tuning
(PEFT), expediting model compression and inference acceleration. In PTQ, approaches range from
quantizing only the weights of LLMs to jointly quantizing both weights and activations. Methods like
GPTQ [Frantar et al., 2022] and QuIP [Chee et al., 2023] optimize matrix multiplications and propose
novel layer-wise quantization techniques achieving high compression rates. SqueezeLLM [Kim et al.,
2023] and SpQR [Dettmers et al., 2023b] identify weights that lead to particularly large quantization
errors and subsequently storing them with higher precision to mitigate the accuracy degradation
caused by weight quantization. AWQ [Lin et al., 2023] and OWQ [Lee et al., 2023] contend that when
quantizing weights, it is crucial to account for the impact of activation outliers on weights. Norm
Tweaking [Li et al., 2023] addresses the issue of activation value deviation by training LayerNorm.
For activation quantization, ZeroQuant [Yao et al., 2022] proposes a fine-grained quantization method
that can be applied to both weights and activations. Methods like SmoothQuant [Xiao et al., 2022]
and Outlier Suppression [Wei et al., 2022; 2023] shift the quantization challenge from activations
to weights by proposing a mathematically equivalent per-channel scaling transformation. Omni-
Quant [Shao et al., 2023] further enhances performance by training the quantization parameters.
RPTQ [Yuan et al., 2023] proposed proposes performance improvement through grouped quantization
after clustering similar channels. In this paper, our primary focus lies in the binarization of weights
exclusively, employing both PTQ and QAT methodologies.
3 PARTIALLY BINARIZING LARGE LANGUAGE MODELS (PB-LLM)
In this section, we elaborate on the methodology of Partially Binarizing Large Language Models,
named PB-LLM. To begin, a review of the foundational framework of binarized neural networks is
presented, showcasing its applicability and limitation to LLM quantization. Subsequently, a novel
format for the quantized matrix is formulated, specifically tailored for the binarization of LLMs.
Taking advantage of the proposed partially-binarized weight matrix, we delve into its potential in
the realms of post-training quantization and training-aware training for LLMs, to break the trade-off
between bit-width and performance. It is crucial to note that, due to constraints in computational
resources, the methodology exploration predominantly utilizes OPT-1.3B [Zhang et al., 2022] to
perform the majority of experiments. Given the space constraints, this section primarily focuses on key
3
aspects of the methodology. For detailed discussions, exact result values, and specific implementation
details in codes, readers are referred to the supplemental materials.
3.1 PRELIMINARY: NETWORK BINARIZATION
To begin with, we briefly review the general concept of network binarization and binarized neural
networks (BNNs) in [Courbariaux et al., 2016, Hubara et al., 2016]. As most optimizable quantized
structures of LLMs are linear layers (see Fig. 1a) in LLMs, we use a one-layer Perceptron to
show the training and inference processes of the BNN. The one-layer neural network is defined as
f(x) = (W)(a), where a ∈ Rdi is the input activation and W : Rdi 7−→ Rdo stands for the weight
matrix, with di and do representing the sizes of the input and output of the layer, respectively.
The goal of network binarization is to represent floating-point (FP) weights, denoted as WF , and/or
FP activations aF as 1-bit (i.e., ., ±1) values [Qin et al., 2020b]. Networks utilizing this representation
are referred to as BNNs. BNNs diverge from FP neural networks in their forward operations and in
the approximation of backward gradients. In the forward propagation, the sign function is used for
binarizing FP values of weights:
Forward: sign(x) =
{
+1 x ≥ 0
−1 x < 0.
(1)
Specifically, in the training process of binarized network, the BNN maintains FP latent weights WF
for gradient updates, and the updated weight matrix WF is binarized into the binary weight matrix
WB via the binarize function sign(·), i.e. WB = sign(WF ). Then the intermediate activation
map (full-precision) of this layer is produced by AF,o = WBAF,i. For inference efficiency, BNNs
with 1-bit weights significantly reduce the memory cost of inference. Theoretically, BNNs can
binarize both weights and activations to 1-bit, providing a 32x compression in memory cost and a
64x acceleration in inference speed, by replacing FP multiplications in conventional floating-point
networks with Xnor-Bitcount operations. However, recent studies highlight that the weights of LLMs
as the main contributor to memory overhead [Kim et al., 2023], and thus we primarily aim to curtail
memory costs. Therefore, in this pivotal exploration of binarized LLMs, our attention is specifically
centered on weight binarization, foregoing the simultaneous binarization of weights and activations.
0 . 1
0 . 2
0 . 3
0 . 4
0 . 5
0 . 6
0 . 7
A R C - E a s y
W i n o G r a n d e
H e l l a S w a g
P I Q A
B o o l Q R a n d o m
F P
B N N
X N O R
B i - R e a l
R e C U
F D A
A R C - C h a l l e n g e
O B Q A
M e a n
Figure 2: We implement five renowned bi-
narization methods on LLMs and assess the
resultant binarized LLMs across seven zero-
shot common sense reasoning tasks. Ran-
dom represents the hypothetical worst base-
line, indicating random guesses, while FP
stands as the optimal baseline, representing
full-precision OPT-1.3B. The exact values
corresponding to this radar graph are detailed
in the Appendix.
In the backward propagation, the main challenge is
that the pervasive sign functions are theoretically non-
differentiable, and thus extremely destroy the gradient
chain in the backward propagation. To address this prob-
lem, researchers widely exploit the straight-through esti-
mator (STE) [Bengio et al., 2013] to numerically approx-
imate the derivative of the whole BNN [Qin et al., 2020b],
i.e.,
Backward:
∂L
∂x
=
{
∂L
∂sign(x) |x| ≤ 1
0 |x| > 1,
(2)
which makes the optimization of BNN accessible.
We first investigate the possibility of implementing bina-
rization to LLM quantization. Specifically, following
the binarization benchmark in BiBench [Qin et al., 2023],
we generalize some representative binarization methods
into LLM quantization scenarios. BNN [Hubara et al.,
2016], XNOR [Rastegari et al., 2016], Bi-Real [Liu et al.,
2020b], ReCU [Xu et al., 2021a] and FDA [Xu et al.,
2021b] are re-implemented to quantize LLMs, particularly
to OPT [Zhang et al., 2022]. Training details are illustrated
in the Sec. 4. The results evaluated on seven zero-shot
common sense reasoning tasks are shown in Fig. 2. We can see that the LLMs binarized via the exist-
ing popular binarization algorithms perform worse than random guesses, showing that the existing
binarization methods are not suitable for LLM binarization.
4
3.2 PARTIALLY BINARIZED WEIGHT MATRIX
In the low-bit quantization of Transformers, a significant challenge is managing the salient weights,
as they can unnecessarily extend the quantization range [Kovaleva et al., 2021]. Several outlier-aware
quantization methods have been explored to tackle this issue [Dettmers et al., 2022, Wei et al., 2022,
Kim et al., 2023, Lin et al., 2023]. Notably, SqueezeLLM [Kim et al., 2023] provides a generalized
methodology for handling outliers in weight values during 4-bit LLM post-training quantization.
Concurrently, AWQ [Lin et al., 2023] demonstrates that preserving only 1% of significant weights
can benefit 4-bit LLM quantization. Motivated by existing research, this study also seeks to optimize
the treatment of salient weights while binarizing most of weights. We present Partially-Binarized
LLMs (PB-LLM), a method involving the selective binarization of the LLMs’ weight matrix, wherein
a minor fraction of weights is kept in high bits for enhanced language capacity.
3.2.1 SALIENT WEIGHT: CRITERIA, GRANULARITY, AND COST
Beyond the most straightforward method of choosing salient weights—selecting based on magnitude
element-wise—we conduct a thorough investigation into salient weight detection from two perspec-
tives: criteria and granularity. For criteria, we compare Magnitude- and Hessian-based methods, and
for granularity, we explore both element-wise and column-wise approaches. In addition, we discuss
the cost of storing matrix weights in a mixed-precision manner.
Criteria: Magnitude vs. Hessian. Beyond the identification of salient weights through magnitude,
alternative criteria have also been examined. The Hessian metric emerges as a crucial factor in LLM
quantization, as elucidated in [Dong et al., 2019, Frantar et al., 2022, Frantar and Alistarh, 2023],
particularly in relation to post-training quantization for LLMs (details regarding the Hessian criteria
for PTQ can be found in Sec. 3.3). However, we observe that the selection of salient weights, whether
by magnitude or Hessian, does not significantly impact the efficacy of PTQ. Consequently, magnitude
is elected as the preferred criterion for the identification of salient weights in both PTQ and QAT,
primarily due to its simplicity and efficacy in distinguishing critical weight components.
0 50 100 150 200 250
0
50
100
150
200
250
Outliers Matrix Visualization
0.0
0.2
0.4
0.6
0.8
1.0
Figure 3: Distribution
of 5% salient weight.
Granularity: Element-wise vs. Column-wise. Our investigations reveal
that adopting a column-wise approach for selecting salient weights has the
potential to impair the performance of binarization. Visualization of the salient
weights’ distribution within the matrix, as depicted in Fig. 3 (where the white
dots represent the filtered salient weights), disclosed a random and uniform
scattering of these weights. Given the absence of any discernable column-wise
pattern in the distribution of salient weights, a column-wise filtration method
is deemed unsuitable. This scattered and uniform distribution necessitates an
element-wise approach for effective filtration in the binarization process.
Salient Weight Storing Cost. The additional overhead for storing the salient weights is acceptable.
The overall bit number, Nbit must adhere to the following condition:
Nbit ≤
for binary weights︷ ︸︸ ︷
1 ∗ rbinary +
for salient weights︷ ︸︸ ︷
Nsalient−bit ∗ (1− rbinary)+
for index storing, could be optimized︷ ︸︸ ︷
1 , (3)
0.0 0.2 0.4 0.6 0.8 1.0
Ratio of Binarized Weights (rbinary)
2
3
4
5
6
7
8
9
Ov
er
al
l B
it
Nu
m
be
r (
N
bi
t)
Noutlier bit = 8
Nbit 9
Nbit 2
Figure 4: Variation in overall bit
number Nbit with the ratio of
the salient weights rbinary , where
salient weights are stored in 8-bit.
Here, rbinary denotes the ratio of the binarized weights, Nsalient−bit
represents the number of bits allocated for storing salient weights
(e.g., 8 bits), and the additional 1 bit is allocated for using the
bitmap mechanism [Chan and Ioannidis, 1998] for index saving. It’s
important to note that employing bitmap for index storage is not
the most efficient method and can be optimized further using sparse
matrix storage methods such as Compressed Sparse Row (CSR) or
Compressed Sparse Column (CSC) [Borštnik et al., 2014]; hence the
use of ≤ instead of = in Eq. 3. Given this research’s emphasis on
the theoretical aspects of binarization for LLM quantization, we do
not delve into saving the cost of storing the index. The relationship
between the ratio of salient weights and the overall bit number is
illustrated in Fig. 4, depicting that a lower ratio corresponds to a
reduced overall bit number. For example, retaining 10% of weights in 8 bits and binarizing the
remaining 90% equates to, at most, a 2.7-bit quantization.
5
Table 1: Perplexity of C4 on OPT-1.3B quantized with RTN (without GPTQ) and PB-GPTQ. Magnitude criteria
or Hessian criteria is used for detecting salient weights.
Salient Fraction 50% 20% 10% 5%
RTN Magnitude 24.5675 5892.0898 4889.0385 8023.1132
RTN Hessian 20.2512 2109.8522 7508.7788 6173.1611
PB-GPTQ Magnitude 18.3674 46.4093 895.0322 2880.6157
PB-GPTQ Hessian 17.7567 42.1157 165.6767 528.4877
PB-GPTQ Magnitude g=128 18.0293 57.2164 1230.8537 2662.7114
PB-GPTQ Hessian g=128 17.6000 45.9811 157.8825 646.3616
3.3 POST-TRAINING QUANTIZATION FOR PB-LLMS
After defining the partially-binarized matrix format, the next step is to recover the performance
(i.e., the reasoning capacity in the literature of LLMs) of the quantized PB-LLM. In this section,
we explore the weight binarization with post-training quantization (PTQ) methods. PTQ methods
hold a prominent position in the realm of quantization techniques for LLMs due to their ease of
implementation. They enable direct quantization of pre-trained LLMs without the need for a training
dataset and additional training overhead. Therefore, we first explore the weight binarization within
the PTQ framework.
GPTQ [Frantar et al., 2022] is the most efficient and effective method for weight quantization [Zhu
et al., 2023], capable of quantizing LLMs to 4-bit or even 2-bit. Therefore, we generalize the idea
of GPTQ to the partial-binarization setting. Specifically, GPTQ quantizes the weights in LLM
layer-by-layer to minimize the layer-wise quantization error:
argmin
Ŵ
||WX− ŴX||22 (4)
GPTQ quantizes a weight wq to ŵq, calculates the compensation δ−q for remaining weights w−q,
and then applies the compensation factor to the remaining weights:
δ−q =
wq − ŵq
[H−1]qq
· (H−1):,q, w−q := w−q + δ−q, (5)
where the H is the Hessian matrix of the layer-wise quantization error with respect to the weights
and wq is the q-th value in flattened weight matrix W. In GPTQ, weights are quantized iteratively
and the remaining weights are updated until all weights have been quantized.
We propose to use GPTQ to iteratively bianrize the un-salient weights and quantize the salient weights
to higher bit, and then apply the compensation to the remaining weights. Specifically, we first detect
the salient weights Wsal and un-salient (to-be-binarized) weights Wunsal in the weight matrix
W = Wsal + Wunsal. Drawing inspiration from SparseGPT [Frantar and Alistarh, 2023], we
calculate the saliency metric, represented as vi = w2
i /[H
−1]2ii, for the purpose of detecting salient
weights using Hessian criterion. The un-salient weights will be binarized to Ŵunsal, and the salient
weights will be quantized to higher bit Ŵsal. We use asymmetric per-channel quantization for both
salient and un-salient weights. For un-salient weight, we use the per-channel mean as zero point and
calculate the optimal scaling factor α for the un-salient weights using the method in Sec. 3.4.2. We
use MinMax metric to calibrate the scaling factor and zero point for salient weights.
In the quantization process, we iteratively quantize the columns in the weight matrix W. For each
column, we binarize the un-salient weights and quantize the salient weights, and then calculate
the compensation for remaining weights, and then apply the compensation factor to the remaining
columns of weights. This process is repeated until all the weights are quantized. The proposed
method is denoted as PB-GPTQ. We also explore the fine-grained PB-GPTQ, which quantizes the
weights in a group-wise manner. Specifically, the weight matrix is split into several groups, each
group contains g columns. In each group, we detect the salient weights and un-salient weights, and
then calibrate to set the scaling factor and zero point using the weights in this group.
The results are listed in Tab. 1. PB-GPTQ is significantly better than RTN. We note that the Hessian-
based PB-GPTQ exhibits a superior performance compared to the Magnitude criterion PB-GPTQ.
The group-wise PB-GPTQ performs better or worse than the non-group-wise PB-GPTQ, but the
difference is not significant. Our analysis suggests that the disparity in scaling factors is not the
primary determinant of binarization performance; hence, the introduction of group-wise methodology
does not yield an enhancement in binarization performance. Subsequently, our next endeavor will
involve the application of QAT to reduce the error introduced by weight binarization.
6
3.4 QUANTIZATION-AWARE TRAINING FOR PB-LLMS
In order to further enhance the reasoning capacity of the Partially-Binarized Large Language Models
(PB-LLM), we extend our exploration by employing Quantization-aware Training (QAT) to meticu-
lously train the quantized models. Because LLM training is difficult, we desire that PB-LLM training
could be as efficient as possible. To realize efficient training for PB-LLM, we propose the Salient
Weights Frozen and Optimal Scaling Factor for Binary Weights, targeting the salient weights and
binarized weights, respectively.
3.4.1 SALIENT WEIGHTS FROZEN
0 2000 4000 6000 8000 10000
Step
3
4
5
6
7
8
9
Lo
ss
Loss (w.o. Outlier Frozen)
Loss (w. 2% Outlier Frozen)
Figure 5: Training Loss Curves:
When only 2% of weights are re-
tained in their un-binarized state,
the training loss converges more
swiftly.
To leverage the value of pre-trained weights, we propose freezing
the salient weights, determined by weight magnitude, prior to the
weight binarization process. As illustrated in Fig. 1a, we initially
filter out a number of weights from a pre-trained weight matrix—e.g.,
2% by magnitude—at the beginning of quantization-aware training,
maintaining their fixed state throughout the training process. Exami-
nation of training efficiency (refer to Fig.5) suggests that these salient
weights play a crucial role in LLM capacity. Maintaining the high
bit representation of certain weights, thereby freezing them, aids
in the training of quantized LLMs and reduces their optimization
difficulty.
3.4.2 OPTIMAL SCALING FACTOR FOR BINARY WEIGHTS.
AWQ [Lin et al., 2023] enhances the weight-only quantization method for LLMs by optimizing scaling
factors to mitigate the quantization error of quantized weights. Specifically, AWQ demonstrates
that searching for empirically optimal scaling factors proves to be an effective strategy for reducing
quantization errors and recovering the performance of the quantized models. Fortunately, in the
context of LLM binarization, we have a better choice for scaling the binarized weights. There’s no
need to search for optimal scaling factors as they can be analytically derived. Specifically, we apply
a column-wise scaling factor to binarized weights to reduce the binarization error, i.e., enforcing
wF = αw̄B . The optimal values of scaling factor α for the w̄B ∈ {−1, 1} can be calculated by
minimizing the L2 error:
α⋆ = arg min
α∈R+
J (α), in which J (α) = ∥wF − αw̄B∥22 (6)
Following XNOR-Net [Rastegari et al., 2016], by expanding the below equation, we have
J (α) = α2w̄T
Bw̄B − 2αwT
F w̄B +wT
FwF (7)
Method0
50
100
150
200
250
300
350
PP
L
on
C
4
367.3791
322.6059
141.7981
109.5769
47.8226
16.0707
Method
XNOR
BNN
PB-LLM w.o. Outlier Frozen
PB-LLM w.o. Optimal Scaling
PB-LLM
FP-OPT1.3B
Figure 6: Perplexity (PPL) on C4:
When 50% of the weights are main-
tained in their un-binarized state
(equivalent to around 5-bit quantiza-
tion), the untrained PB-LLM does not
experience a total loss of reasoning
capabilities.
For the vector with wF ∈ Rn we follow the traditional methods
of binarizing weights [Hubara et al., 2016] by taking the sign of
real-valued weights:
w̄i
B = sign(wi
F ) =
{
+1, wi
F ≥ 0;
−1, wi
F < 0.
(8)
In that case, w̄T
Bw̄B = nwF
, where nwF
is number of elements
in wF , and α∗ can be solved as:
α∗ =
wT
F w̄B
nwF
=
∥wF ∥1
nwF
. (9)
A counterintuitive outcome emerges from the incorporation of
salient-frozen and optimal-scaling mechanisms: directly deploy-
ing those two mechanisms to pre-trained LLM even without any
retraining or fine-tuning, still results in commendable perfor-
mance. For instance, applying these techniques to OPT-1.3B with 50% salient weights (see Fig. 6)
reveals that the partially-binarized OPT-1.3B retains a small amount of language capacity, corrob-
orating the importance of a small number of salient weights in LLM quantization. Consequently,
7
100 101 102 103 104
Epoch
0.40
0.45
0.50
0.55
0.60
A
cc
ur
ac
y
30% Salient PB­LLM
4­bit LLM­QAT
(a) BoolQ [Clark et al., 2019]
100 101 102 103 104
Epoch
0.52
0.54
0.56
0.58
0.60
0.62
0.64
0.66
0.68
A
cc
ur
ac
y
30% Salient PB­LLM
4­bit LLM­QAT
(b) PIQA [Bisk et al., 2020]
100 101 102 103 104
Epoch
0.225
0.250
0.275
0.300
0.325
0.350
0.375
0.400
A
cc
ur
ac
y
30% Salient PB­LLM
4­bit LLM­QAT
(c) HellaSwag [Zellers et al., 2019]
100 101 102 103 104
Epoch
0.50
0.51
0.52
0.53
0.54
0.55
0.56
0.57
A
cc
ur
ac
y
30% Salient PB­LLM
4­bit LLM­QAT
(d) WinoGrande [Sakaguchi et al., 2021]
100 101 102 103 104
Epoch
0.20
0.25
0.30
0.35
0.40
0.45
A
cc
ur
ac
y
30% Salient PB­LLM
4­bit LLM­QAT
(e) ARC-E [Clark et al., 2018]
100 101 102 103 104
Epoch
0.16
0.18
0.20
0.22
0.24
0.26
A
cc
ur
ac
y
30% Salient PB­LLM
4­bit LLM­QAT
(f) ARC-C [Clark et al., 2018]
100 101 102 103 104
Epoch
0.16
0.17
0.18
0.19
0.20
0.21
A
cc
ur
ac
y
30% Salient PB­LLM
4­bit LLM­QAT
(g) OBQA [Mihaylov et al., 2018]
100 101 102 103 104
Epoch
0.32
0.34
0.36
0.38
0.40
0.42
0.44
A
cc
ur
ac
y
30% Salient PB­LLM
4­bit LLM­QAT
(h) Average on Seven Tasks
100 101 102 103 104
Epoch
0.40
0.45
0.50
0.55
0.60
A
cc
ur
ac
y
5% Salient PB­LLM
3­bit LLM­QAT
(i) BoolQ [Clark et al., 2019]
100 101 102 103 104
Epoch
0.500
0.525
0.550
0.575
0.600
0.625
0.650
0.675
A
cc
ur
ac
y
5% Salient PB­LLM
3­bit LLM­QAT
(j) PIQA [Bisk et al., 2020]
100 101 102 103 104
Epoch
0.22
0.24
0.26
0.28
0.30
0.32
0.34
0.36
A
cc
ur
ac
y
5% Salient PB­LLM
3­bit LLM­QAT
(k) HellaSwag [Zellers et al., 2019]
100 101 102 103 104
Epoch
0.40
0.42
0.44
0.46
0.48
0.50
0.52
0.54
A
cc
ur
ac
y
5% Salient PB­LLM
3­bit LLM­QAT
(l) WinoGrande [Sakaguchi et al., 2021]
100 101 102 103 104
Epoch
0.20
0.25
0.30
0.35
0.40
0.45
A
cc
ur
ac
y
5% Salient PB­LLM
3­bit LLM­QAT
(m) ARC-E [Clark et al., 2018]
100 101 102 103 104
Epoch
0.16
0.17
0.18
0.19
0.20
0.21
0.22
0.23
A
cc
ur
ac
y
5% Salient PB­LLM
3­bit LLM­QAT
(n) ARC-C [Clark et al., 2018]
100 101 102 103 104
Epoch
0.16
0.18
0.20
0.22
0.24
A
cc
ur
ac
y
5% Salient PB­LLM
3­bit LLM­QAT
(o) OBQA [Mihaylov et al., 2018]
100 101 102 103 104
Epoch
0.28
0.30
0.32
0.34
0.36
0.38
0.40
0.42
A
cc
ur
ac
y
5% Salient PB­LLM
3­bit LLM­QAT
(p) Average on Seven Tasks
Figure 7: QAT training results with 30% salient weights PB-LLM (upper two lines): As fine-tuning epochs
increase, quantized models swiftly regain their reasoning capacities, demonstrating the resilience and adaptability
of PB-LLMin sustaining cognitive functionalities within models, despite substantial quantization; QAT training
results with 5% salient weights PB-LLM (bottom two lines): Existing LLM QAT methods exhibit an absolute
failure when subjected to extremely-low bit conditions. In contrast, PB-LLMtriumphs in restoring the reasoning
capacities of low-bit quantized LLMs. This underlines the efficacy of PB-LLM in balancing quantization and
performance, preserving the essential reasoning abilities of LLMs even under rigorous bit reduction.
implementing just these two techniques—Outlier Frozen and Optimal Scaling Factor for Binary
Weights—on pre-trained LLMs serves as an efficient starting point for training PB-LLM.
Both of the above-proposed mechanisms are very effective when used during quantization-aware
training of PB-LLM. The consequential outcomes are delineated in Figs.7a-7p. Observations from the
presented results elucidate that optimizing using the partially-binarized quantization format is notably
more straightforward compared to single-bit quantization. This empirical evidence corroborates the
discussion regarding the rapid convergence property found in Sec.3.4.1, highlighting the efficacy
and adaptability of our proposed methodology in optimizing LLMs within the constraints of partial
binarization. From the perspective of QAT, PB-LLM emerges as more efficient in training compared
to existing LLM QAT methods. For instance, while models like LLM-QAT [Liu et al., 2023a]
necessitate up to 100K iterations for adequate training, PB-LLM remarkably achieves recovery of the
performance of quantized LLMs in merely around 1-10K iterations. This substantial reduction in
required iterations represents a leap in training efficiency, streamlining the path to achieving optimal
performance in quantized LLMs with significantly reduced computational effort.
4 EXPERIMENTS
Besides the exploration with OPT-1.3B in Sec. 3, we assess the effectiveness of PB-LLM by conduct-
ing experiments on LLaMA-7B [Touvron et al., 2023] and presenting results on various tasks.
8
Table 2: Zero-shot performance on Common Sense Reasoning tasks within a 4-bit setting. Reported results of
previous works are documented in their papers. PB-LLM 30% denotes the preservation of 30% salient weights,
and PB-LLM 10% implies the preservation of 10% salient weights.
Method BoolQ PIQA HellaSwag WinoGrande ARC-E ARC-C OBQA Avg
FP LLaMA-7B 76.8 79.3 76.1 70.0 73.0 48.0 57.6 68.7
RTN 71.2 77.3 72.7 66.9 68.8 46.4 52.8 65.2
SmoothQuant 67.7 76.0 69.4 66.7 66.9 43.0 50.6 63.0
LLM-QAT 75.5 78.3 74.0 69.0 70.0 45.0 55.4 66.6
PB-GPTQ 10% 62.3 55.9 27.7 49.3 29.3 20.1 10.6 36.5
PB-GPTQ 30% 73.5 74.9 47.5 64.9 61.3 32.4 25.2 54.2
PB-LLM 10% 68.9 67.8 68.1 67.4 58.7 42.9 50.6 60.6
PB-LLM 30% 75.7 78.0 74.3 69.7 69.0 45.6 55.8 66.9
4.1 EXPERIMENTAL SETUP
Dataset. In this study, the PB-LLM is trained using the RedPajama-simple-1B dataset, as the dataset
for LLaMa training is not openly accessible. This dataset, RedPajama-1T, is structured to closely
resemble the LLaMa paper and serves as a transparent, open-source alternative to LLM training
dataset. It amalgamates data from diverse sources including Commoncrawl, C4, GitHub, Wikipedia,
Gutenberg Books3, ArXiv, and Stackexchange. RedPajama-simple-1B, representing a 0.1% subset
of RedPajama-1T, is substantially smaller than the typical datasets used for training other LLMs,
making it a convenient choice for our experiments.
Training Details. In the training process of our quantized network, we commence with a pre-
trained model for initialization. The optimization of the model is facilitated through the AdamW
optimizer [Loshchilov and Hutter, 2017], applied with zero weight decay. We assign a batch size of 1
to each GPU and implement a learning rate of 2e-5, adhering to a cosine learning rate decay strategy.
We only fine-tune our PB-LLM for 10K iterations.
Evaluated Tasks. To eliminate the variance of evaluated performance, we evaluate the binarized
LLMs on seven zero-shot common sense reasoning tasks, i.e., BoolQ [Clark et al., 2019], PIQA [Bisk
et al., 2020], HellaSwag [Zellers et al., 2019], WinoGrande [Sakaguchi et al., 2021], ARC-Easy,
ARC-Challenge [Clark et al., 2018], OBQA [Mihaylov et al., 2018]. We also along eavulated the
quantized moelds’ perplexity scores on WikiText2 [Merity et al., 2016] and C4 [Raffel et al., 2020].
4.2 RESULTS ON LLAMA
Table 3: Perplexity of C4, wikitext2 and PTB on
LLaMA-7b quantized with PTQ methods.
C4 WIKI PTB
FP 7.3435 5.6770 41.1509
GPTQ 4b 8.6977 8.1368 57.9951
SparseGPT 50% 15.5949 12.829483 505.1396
PB-GPTQ 50% 8.1466 6.3089 54.8674
PB-GPTQ 20% 20.6057 17.1929 280.4353
PB-GPTQ 10% 72.1115 85.7838 708.4120
PB-GPTQ 5% 401.6475 619.1054 1687.1815
Experiments were conducted on LLaMA-7B. The
results of employing PB-GPTQ and PB-LLM are
illustrated in Tabs. 2 and 3. When employing PTQ,
PB-GPTQ exhibited commendable performance,
particularly when the salient weight exceeded 30%.
Nevertheless, a noteworthy decline in the perfor-
mance of the quantized network was observed when
the salient weight was reduced to 10%. On the other
hand, employing QAT resulted in a notable improve-
ment in the performance. A comparison within a 4-bit quantization setting between PB-LLM 30%
and LLM-QAT in Tab. 2 reveals superior performance by our method. It is notable that PB-LLM is
only fine-tuned for 10K iterations, whereas LLM-QAT underwent 100K iterations of training, show-
ing its fast convergence property (refer to Sec. 3.2). The results under PB-LLM 10% represent the
outcomes of PB-LLM where 10% of salient weights are preserved. This demonstrates the potential
for advancing LLM quantization towards a fully 1-bit state.
5 CONCLUSION
In conclusion, this work is the first to implement network binarization for LLM quantification, intro-
ducing the novel Partially-binarized LLM (PB-LLM) methodology. This approach is meticulously
designed to maintain linguistic reasoning capabilities of LLMs, even under extreme low-bit quantiza-
tion. The research unearthed the significant role of salient weights in achieving extreme quantization
and proposed innovative strategies like optimal scaling for effective binarization. This framework
is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of
post-training quantization (PTQ) and quantization-aware training (QAT). The methodology is a
significant stride in the realm of network binarization for LLMs.
9
REFERENCES
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:1877–1901, 2020.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,
Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv
preprint arXiv:2205.01068, 2022.
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné,
Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access
multilingual language model. arXiv preprint arXiv:2211.05100, 2022.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023.
Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A survey on model compression for large language
models. arXiv preprint arXiv:2308.07633, 2023.
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for
transformers at scale. arXiv preprint arXiv:2208.07339, 2022.
Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot.
ICML, 2023.
Mingyang Zhang, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang, et al. Pruning meets low-rank
parameter-efficient fine-tuning. arXiv preprint arXiv:2305.18403, 2023.
Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi,
Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large
language models. arXiv preprint arXiv:2305.17888, 2023a.
Koen Helwegen, James Widdicombe, Lukas Geiger, Zechun Liu, Kwang-Ting Cheng, and Roeland Nusselder.
Latent weights do not exist: Rethinking binarized neural network optimization. Advances in neural information
processing systems, 2019.
Manuele Rusci, Alessandro Capotondi, and Luca Benini. Memory-driven mixed low precision quantization for
enabling deep network inference on microcontrollers. MLSys, 2020.
Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, and Jingkuan Song.
Forward and backward information retention for accurate binary neural networks. In CVPR, 2020a.
Haotong Qin, Mingyuan Zhang, Yifu Ding, Aoyu Li, Zhongang Cai, Ziwei Liu, Fisher Yu, and Xianglong Liu.
Bibench: Benchmarking and analyzing network binarization. ICML, 2023.
Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards precise binary neural
network with generalized activation functions. In ECCV, 2020a.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In CVPR, 2009.
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for
generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.
Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic
neurons for conditional computation. arXiv:1308.3432, 2013.
Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu Sebe. Binary neural networks:
A survey. Pattern Recognition, 105:107281, 2020b.
Chunyu Yuan and Sos S Agaian. A comprehensive review of binary neural network. Artificial Intelligence
Review, pages 1–65, 2023.
10
Zechun Liu, Wenhan Luo, Baoyuan Wu, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real net: Binarizing
deep network towards real-network performance. IJCV, 2020b.
Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos. Training binary neural networks with
real-to-binary convolutions. arXiv preprint arXiv:2003.11535, 2020.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification
using binary convolutional neural networks. In ECCV, 2016.
Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King.
Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701, 2020.
Haotong Qin, Yifu Ding, Mingyuan Zhang, Qinghua Yan, Aishan Liu, Qingqing Dang, Ziwei Liu, and Xianglong
Liu. Bibert: Accurate fully binarized bert. arXiv preprint arXiv:2203.06390, 2022.
Zechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao, Scott Yih, Meng Li, Raghuraman Krishnamoorthi, and Yashar
Mehdad. Bit: Robustly binarized multi-distilled transformer. Advances in neural information processing
systems, 35:14303–14316, 2022.
Zechun Liu, Barlas Oguz, Aasish Pappu, Yangyang Shi, and Raghuraman Krishnamoorthi. Binary and ternary
natural language generation. ACL, 2023b.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized
llms. arXiv preprint arXiv:2305.14314, 2023a.
Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of large language
models with guarantees. arXiv preprint arXiv:2307.13304, 2023.
Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and
Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.
Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos,
Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for
near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023b.
Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight
quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.
Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. Owq: Lessons learned from
activation outliers for weight quantization in large language models. arXiv preprint arXiv:2306.02272, 2023.
Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu. Norm tweaking: High-performance low-bit quantization
of large language models. arXiv preprint arXiv:2309.02784, 2023.
Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Ze-
roquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint
arXiv:2206.01861, 2022.
Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient
post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.
Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and
Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. NeurIPS,
2022.
Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu.
Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and
scaling. arXiv preprint arXiv:2304.09145, 2023.
Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao,
Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models.
CoRR, abs/2308.13137, 2023.
Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu,
Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language models.
arXiv preprint arXiv:2304.01089, 2023.
11
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks
with binary weights during propagations. In NeurIPS, 2016.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural
networks. In NeurIPS, 2016.
Zihan Xu, Mingbao Lin, Jianzhuang Liu, Jie Chen, Ling Shao, Yue Gao, Yonghong Tian, and Rongrong Ji.
Recu: Reviving the dead weights in binary neural networks. In ICCV, 2021a.
Yixing Xu, Kai Han, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe Wang. Learning frequency domain
approximation for binary neural networks. In NeurIPS, 2021b.
Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky. Bert busters: Outlier dimensions
that disrupt transformers. arXiv preprint arXiv:2105.06990, 2021.
Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq: Hessian aware
quantization of neural networks with mixed-precision. In ICCV, 2019.
Chee-Yong Chan and Yannis E Ioannidis. Bitmap index design and evaluation. In SIGMOD, 1998.
Urban Borštnik, Joost VandeVondele, Valéry Weber, and Jürg Hutter. Sparse matrix multiplication: The
distributed block-compressed sparse row library. Parallel Computing, 2014.
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044,
2019.
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in
natural language. In AAAI, 2020.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really
finish your sentence? arXiv preprint arXiv:1905.07830, 2019.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial
winograd schema challenge at scale. Communications of the ACM, 2021.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint
arXiv:1803.05457, 2018.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a
new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,
2017.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv
preprint arXiv:1609.07843, 2016.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei
Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR,
2020.
12
A SUPPLEMENTAL MATERIALS
A.1 EXISITING BINARIZATION METHODS ON LLM QUANTIZATION
Method BoolQ PIQA HellaSwag WinoGrande ARC-Easy ARC-Challenge OBQA Mean
Random Performance 0.5 0.5 0.25 0.5 0.25 0.25 0.25 0.36
FP 0.595 0.63 0.415 0.595 0.54 0.22 0.25 0.46
BNN 0.38 0.545 0.235 0.46 0.195 0.165 0.15 0.30
XNOR 0.37 0.525 0.265 0.49 0.195 0.165 0.16 0.31
Bi-Real 0.395 0.5 0.25 0.505 0.235 0.185 0.165 0.32
ReCU 0.39 0.515 0.24 0.51 0.255 0.185 0.175 0.32
FDA 0.39 0.485 0.265 0.49 0.265 0.19 0.17 0.32
Table 4: Table corresponds to Figure 2 in the main paper: We implement five renowned binarization
methods on LLMs and assess the resultant binarized LLMs across seven zero-shot common sense
reasoning tasks.
We first investigate the possibility of implementing binarization to LLM quantization. Specifically,
following the binarization benchmark in BiBench [Qin et al., 2023], we generalize some representative
binarization methods into LLM quantization scenarios. BNN [Hubara et al., 2016], XNOR [Rastegari
et al., 2016], Bi-Real [Liu et al., 2020b], ReCU [Xu et al., 2021a] and FDA [Xu et al., 2021b] are
re-implemented to quantize LLMs, particularly to OPT [Zhang et al., 2022]. Training details are
illustrated in the Sec. 4. The results evaluated on seven zero-shot common sense reasoning tasks are
shown in the above table. We can see that the LLMs binarized via the existing popular binarization
algorithms perform worse than random guesses, showing that the existing binarization methods are
not suitable for LLM binarization.
13
请分别归纳一下每一篇论文的内容，并说明他们之间的相互引用关系以及相互的评价，特别是所有论文对于第一篇论文的评价